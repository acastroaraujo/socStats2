[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Social Statistics II (Exercises)",
    "section": "",
    "text": "Preface\nSyllabus\nHi everyone, I will be uploading the homework questions to this website.\nFeel free to reach out to me with any questions.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#resources",
    "href": "index.html#resources",
    "title": "Social Statistics II (Exercises)",
    "section": "Resources",
    "text": "Resources\nThese are my personal recommendations for resources to start getting interested in statistics. It’s somewhat incomplete—e.g., there are no dedicated textbooks to causal inference, which is what we’ll cover next semester.\nClass Resources:\n\nThe Effect by NHK (Huntington-Klein 2021)\n\nShort:\n\nResearch questions and estimands (Lundberg et al. 2021)\nIntroduction to DAGs (Rohrer 2018; Cinelli et al. 2022)\nThe Causal Diagrams edX course by Miguel Hernán is pretty good too.\nDon’t interpret “control” or “adjustment” variables (Westreich and Greenland 2013; Keele et al. 2020; Hünermund and Louw 2023).\nThe effects of seemingly immutable characteristics (Sen and Wasow 2016; cf. Holland 1986).\n\nGood to play around with:\n\nCounterfactuals and Causal Inference (Morgan and Winship 2014)\nI’ve been told many times that the first edition is better, so maybe try that one instead.\nCausal Inference: The Mixtape (Cunningham 2021)\nCovers a lot of the same ground as NHK, but has more math. It is also written by an economist.\nRegression and Other Stories (Gelman et al. 2020)\nChapters 18-21\nTheory & Credibility (Ashworth et al. 2021)\n\nAdvanced Resources:\n\nMostly Harmless Econometrics (Angrist and Pischke 2009)\nCausality: Models, Reasoning and Inference (Pearl 2009)\nCausal Inference: What If (Hernan and Robins 2023)\nThis book gets used a lot in epidemiology.\n\n\n\n\n\n\n\nAngrist, Joshua D., and Jörn-Steffen Pischke. 2009. Mostly Harmless Econometrics. Princeton university press.\n\n\nAshworth, Scott, Christopher R. Berry, and Ethan Bueno de Mesquita. 2021. Theory and Credibility: Integrating Theoretical and Empirical Social Science. Princeton University Press.\n\n\nCinelli, Carlos, Andrew Forney, and Judea Pearl. 2022. “A Crash Course in Good and Bad Controls.” Sociological Methods & Research 00491241221099552.\n\n\nCunningham, Scott. 2021. Causal Inference. Yale University Press.\n\n\nGelman, Andrew, Jennifer Hill, and Aki Vehtari. 2020. Regression and Other Stories. Cambridge University Press.\n\n\nHernan, Miquel A., and James M. Robins. 2023. Causal Inference: What If. CRC Press.\n\n\nHolland, Paul W. 1986. “Statistics and Causal Inference.” Journal of the American Statistical Association 81(396): 945–60.\n\n\nHünermund, Paul, and Beyers Louw. 2023. “On the Nuisance of Control Variables in Causal Regression Analysis.” Organizational Research Methods 10944281231219274.\n\n\nHuntington-Klein, Nick. 2021. The Effect: An Introduction to Research Design and Causality. New York: Chapman; Hall/CRC.\n\n\nKeele, Luke, Randolph T. Stevenson, and Felix Elwert. 2020. “The Causal Interpretation of Estimated Associations in Regression Models.” Political Science Research and Methods 8(1): 113.\n\n\nLundberg, Ian, Rebecca Johnson, and Brandon M. Stewart. 2021. “What Is Your Estimand? Defining the Target Quantity Connects Statistical Evidence to Theory.” American Sociological Review 00031224211004187.\n\n\nMorgan, Stephen L., and Christopher Winship. 2014. Counterfactuals and Causal Inference: Methods and Principles for Social Research. 2nd edition. 2nd edition. New York, NY: Cambridge University Press.\n\n\nPearl, Judea. 2009. Causality: Models, Reasoning and Inference. 2nd edition. 2nd edition. Cambridge, U.K. ; New York: Cambridge University Press.\n\n\nRohrer, Julia M. 2018. “Thinking Clearly about Correlations and Causation: Graphical Causal Models for Observational Data.” Advances in Methods and Practices in Psychological Science 1(1): 2742.\n\n\nSen, Maya, and Omar Wasow. 2016. “Race as a Bundle of Sticks: Designs That Estimate Effects of Seemingly Immutable Characteristics.” Annual Review of Political Science 19(1): 499–522.\n\n\nWestreich, Daniel, and Sander Greenland. 2013. “The Table 2 Fallacy: Presenting and Interpreting Confounder and Modifier Coefficients.” American Journal of Epidemiology 177(4): 292–98.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "sec01.html",
    "href": "sec01.html",
    "title": "Part 1",
    "section": "",
    "text": "…",
    "crumbs": [
      "Part 1"
    ]
  },
  {
    "objectID": "week01.html",
    "href": "week01.html",
    "title": "1  Week 1",
    "section": "",
    "text": "1.1 Exercise\nWe will be digesting this article more fully in the coming weeks, so don’t worry if you don’t understand everything yet.",
    "crumbs": [
      "Part 1",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Week 1</span>"
    ]
  },
  {
    "objectID": "week01.html#sec-glossary",
    "href": "week01.html#sec-glossary",
    "title": "1  Week 1",
    "section": "",
    "text": "Start reading What Is Your Estimand? Defining the Target Quantity Connects Statistical Evidence to Theory.\nKeep track of at least 5 terms that are new to you or that perhaps you don’t fully understand yet (e.g., estimand, DAG, potential outcome).\nI will be asking you to build a glossary of terms related to causal inference. I will not grade this yet, but still do your best attempt at defining each of these terms.",
    "crumbs": [
      "Part 1",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Week 1</span>"
    ]
  },
  {
    "objectID": "week01.html#exercise",
    "href": "week01.html#exercise",
    "title": "1  Week 1",
    "section": "1.2 Exercise",
    "text": "1.2 Exercise\n\n\nPackages\nlibrary(tidyverse)\nlibrary(gt)\n\n\nThe following data frame contains the potential outcomes for 8 individuals.\n\n\nCode\nd &lt;- data.frame(\n  T = c(0, 0, 1, 0, 0, 1, 1, 1),\n  Y0 = c(5, 8, 5, 12, 4, 8, 4, 9),\n  Y1 = c(5, 10, 3, 13, 2, 9, 1, 13), \n  id = LETTERS[1:8]\n)\n\ngt(d, rowname_col = \"id\") # gt is used for fancy printing of tables\n\n\n\n\n\n\n\n\n\nT\nY0\nY1\n\n\n\n\nA\n0\n5\n5\n\n\nB\n0\n8\n10\n\n\nC\n1\n5\n3\n\n\nD\n0\n12\n13\n\n\nE\n0\n4\n2\n\n\nF\n1\n8\n9\n\n\nG\n1\n4\n1\n\n\nH\n1\n9\n13\n\n\n\n\n\n\n\nThe variable T depicts whether someone got the “treatment” or not.\n\n\n\n\n\n\n\nCreate a new variable called Y that contains the observed outcomes.\nWhat is the Average Treatment Effect (ATE) for this 8 person experiment?\n\n\n\n\nNote. I’m only asking for a simple difference in means; this is also sometimes called the “naive estimator” (or “naive comparison”), which works just fine in simple experimental settings.",
    "crumbs": [
      "Part 1",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Week 1</span>"
    ]
  },
  {
    "objectID": "week01.html#exercise-1",
    "href": "week01.html#exercise-1",
    "title": "1  Week 1",
    "section": "1.3 Exercise",
    "text": "1.3 Exercise\n\n\n\n\n\n\n\nSimulate a new completely randomized experiment on these 8 people; that is, re sample \\(T\\) at random so that equal numbers get the treatment and the control.\nCreate a new variable called Y that contains the observed outcomes.\nWhat is the Average Treatment Effect (ATE) for this 8 person experiment?\nDo this a couple of times (at least 3) and note the differences.\n\n\n\n\n\n\n\n\n\n\nHint: You can re-sample \\(T\\) very easily using the sample() function.\nFor example:\n\n\nCode\nsample(d$T)\n\n\n[1] 1 0 1 0 0 1 0 1\n\n\n\n\n\n\n\n\n\n\n\nHow do these estimates compare to the “real” ATE?\nWhen I say “the real ATE” I basically mean this:\n\n\nCode\nmean(d$Y1 - d$Y0)\n\n\n[1] 0.125",
    "crumbs": [
      "Part 1",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Week 1</span>"
    ]
  },
  {
    "objectID": "week01.html#exercise-2",
    "href": "week01.html#exercise-2",
    "title": "1  Week 1",
    "section": "1.4 Exercise",
    "text": "1.4 Exercise\n\n\n\n\n\n\nObviously, an experiment of 8 people will not give you enough “statistical power.”\n\nAssuming the ATE is \\(0.125\\), how many people would you need to enroll in this experiment to have enough statistical power?\n\n\n\n\n\n\n\n\n\n\nHint: There are a few different ways of giving a reasonable answer to this question. The wording of this problem is ambiguous.",
    "crumbs": [
      "Part 1",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Week 1</span>"
    ]
  },
  {
    "objectID": "week02.html",
    "href": "week02.html",
    "title": "2  Week 2",
    "section": "",
    "text": "2.1 Exercise",
    "crumbs": [
      "Part 1",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Week 2</span>"
    ]
  },
  {
    "objectID": "week02.html#exercise",
    "href": "week02.html#exercise",
    "title": "2  Week 2",
    "section": "",
    "text": "Which of the following is the best definition of the term identified as in “this variation has identified the effect we’re interested in”?\n\nWe’ve generated the data by conducting a controlled experiment in which treatment is randomly assigned.\nIn the data generating process, the only reason why we see variation in the outcome variable is because of the treatment variable.\nThe relationship we are looking at in the data actually tests a hypothesis.\nIn the variation we use, there’s no reason we’d see any relationship at all except for the effect we’re interested in.",
    "crumbs": [
      "Part 1",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Week 2</span>"
    ]
  },
  {
    "objectID": "week02.html#exercise-1",
    "href": "week02.html#exercise-1",
    "title": "2  Week 2",
    "section": "2.2 Exercise",
    "text": "2.2 Exercise\n\n\n\n\n\n\nYou read about a new study with the headline “eating caviar linked to longer lifespan.” The study’s research question is “does eating caviar make you live longer?” In the study’s data, they find that people who eat caviar have, on average, longer lifespans than people who don’t.\n\nWhat are some alternate explanations for this relationship?\nWhat sort of variation would identify the answer to the research question?\nGive one suggestion for how the study authors might isolate variation that would identify the answer to the research question",
    "crumbs": [
      "Part 1",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Week 2</span>"
    ]
  },
  {
    "objectID": "week02.html#exercise-2",
    "href": "week02.html#exercise-2",
    "title": "2  Week 2",
    "section": "2.3 Exercise",
    "text": "2.3 Exercise\n\n\n\n\n\n\nFor each of the following news headlines, assume that the underlying data actually only shows a correlation between the two variables mentioned. Give an alternate explanation for the correlation other than the causal relationship implied by the headline.\n\n“As stock market drops, presidential approval ratings decline.”\n“Dates are announced for the downtown summer concert series, driving up sales at downtown restaurants.”\n“Unsanitary? Hospital visits linked to 20% increased risk of disease.”\n“Dress for success! Every CEO follows this office-wear rule.”",
    "crumbs": [
      "Part 1",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Week 2</span>"
    ]
  },
  {
    "objectID": "week02.html#exercise-3",
    "href": "week02.html#exercise-3",
    "title": "2  Week 2",
    "section": "2.4 Exercise",
    "text": "2.4 Exercise\n\n\n\n\n\n\nWhy is a variable that causes both the “treatment” and “outcome” variables especially concerning for identification? You may want to use the phrase “alternate explanation” in your answer.",
    "crumbs": [
      "Part 1",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Week 2</span>"
    ]
  },
  {
    "objectID": "week02.html#exercise-4",
    "href": "week02.html#exercise-4",
    "title": "2  Week 2",
    "section": "2.5 Exercise",
    "text": "2.5 Exercise\n\n\n\n\n\n\nShoe company Crikey claims that people who wear their fancy and expensive professional running-shoe Cool Mistrunner brand run 4 to 5% faster than if they wore an average shoe.\n\nIn a few sentences, describe the data-generating process (you will probably leave some things out, that’s okay).\nWhat are possible alternative explanations for this claim, aside from the shoe making the person run faster?\nIn running their study, the researchers accounted for some alternative explanations, including: gender, enthusiasm for running, and whether runners have participated in marathons and/or half marathons. Think of an alternative explanation not on this list. What is the implication of not accounting for this alternative explanation?",
    "crumbs": [
      "Part 1",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Week 2</span>"
    ]
  },
  {
    "objectID": "week03.html",
    "href": "week03.html",
    "title": "3  Week 3",
    "section": "",
    "text": "3.1 Exercise\nIgnorability\nExperiments work because they make the distribution of potential outcomes the same across levels of the treatment variable. In other words, the potential outcomes and the treatment indicator are independent.\nThis is how this idea was introduced in class:\n\\[\n\\begin{align}\nY^0 \\perp T, && Y^1 \\perp T\n\\end{align}\n\\]\nAssume the following table comes from perfectly executed experiment.\n*30% of the population is in group T = 1",
    "crumbs": [
      "Part 1",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Week 3</span>"
    ]
  },
  {
    "objectID": "week03.html#exercise",
    "href": "week03.html#exercise",
    "title": "3  Week 3",
    "section": "",
    "text": "Table 3.1: Perfect Experiment Example*\n\n\n\n\n\nGroup (\\(T\\))\n\\(E[Y^1]\\)\n\\(E[Y^0]\\)\n\n\n\n\n\\(T = 1\\)\n10,000\n?\n\n\n\\(T = 0\\)\n?\n5,000\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFill in the missing cells.\nWhat is the ATE?",
    "crumbs": [
      "Part 1",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Week 3</span>"
    ]
  },
  {
    "objectID": "week03.html#exercise-1",
    "href": "week03.html#exercise-1",
    "title": "3  Week 3",
    "section": "3.2 Exercise",
    "text": "3.2 Exercise\nGo back to the glossary I asked you to start creating during Week 1.\nMake sure to add the following terms:\n\n\n\n\n\n\n\nDAG.\nPaths.\nDirect effects.\nIndirect effects.\nTotal effects.\nFront door paths.\nBack door paths.\nConfounding.\nCollider.\nOpen Path.\nClosed Path.\n\n\n\n\n\nSee: Section 6.1",
    "crumbs": [
      "Part 1",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Week 3</span>"
    ]
  },
  {
    "objectID": "week03.html#exercise-2",
    "href": "week03.html#exercise-2",
    "title": "3  Week 3",
    "section": "3.3 Exercise",
    "text": "3.3 Exercise\nDrawing DAGs.\nThe following exercises are problems from NHK (Chapter 7).\n\n\n\n\n\n\nDraw a causal diagram for the research question “do long shift hours make doctors give lower-quality care?” that incorporates the following features (and only the following features):\n\nLong shift hours affect both how tired doctors are, and how much experience they have, both of which affect the quality of care\nHow long shifts are is often decided by the hospital the doctor works at. There are plenty of other things about a given hospital that also affect the quality of care, like its funding level, how crowded it is, and so on\nNew policies that reduce shift times may be implemented at the same time (with the timing determined by some unobservable change in policy preferences) as other policies that also attempt to improve the quality of care",
    "crumbs": [
      "Part 1",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Week 3</span>"
    ]
  },
  {
    "objectID": "week03.html#exercise-3",
    "href": "week03.html#exercise-3",
    "title": "3  Week 3",
    "section": "3.4 Exercise",
    "text": "3.4 Exercise\n\n\n\n\n\n\nConsider this research question: Does the funding level of public schools affect student achievement for students in your country?\n\nWhat is the treatment and what is the outcome of interest?\nWrite down a list of relevant variables.\nWhich of the variables in your list in part b are causes of both treatment and outcome?\nWhy might we want to pay extra attention to the variables listed in part c?\nDraw a causal diagram of the variables listed in part b.\nSimplify the diagram from part e.",
    "crumbs": [
      "Part 1",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Week 3</span>"
    ]
  },
  {
    "objectID": "week03.html#exercise-4",
    "href": "week03.html#exercise-4",
    "title": "3  Week 3",
    "section": "3.5 Exercise",
    "text": "3.5 Exercise\n\n\n\n\n\n\nHow can a causal diagram be modified so as to avoid cyclic relationships?\nConsider the diagram below. It depicts a cyclical relationship between student achievement and motivation. If students achieve more (i.e., score well on exams), then their motivation goes up, and if their motivation goes up, they achieve more. Change the diagram so that the relationship is not cyclic anymore.\n\\[\n\\text{Student Achievement} \\longleftrightarrow \\text{Motivation}\n\\]\n\n\n\n\n\n\n\n\n\nHint: We didn’t see this in class, but you should be able to figure it out from the readings.",
    "crumbs": [
      "Part 1",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Week 3</span>"
    ]
  },
  {
    "objectID": "week03.html#exercise-5",
    "href": "week03.html#exercise-5",
    "title": "3  Week 3",
    "section": "3.6 Exercise",
    "text": "3.6 Exercise\nThe following exercises are problems from NHK (Chapter 8).\n\n\n\n\n\n\nAssuming that a path has no colliders on it, what is the difference between a path being Open and Closed?",
    "crumbs": [
      "Part 1",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Week 3</span>"
    ]
  },
  {
    "objectID": "week03.html#exercise-6",
    "href": "week03.html#exercise-6",
    "title": "3  Week 3",
    "section": "3.7 Exercise",
    "text": "3.7 Exercise\n\n\n\n\n\n\nConsider the below generic causal diagram.\n\n\n\n\n\n\nList every path from X to Y.\nWhich of the paths are front-door paths?\nWhich of the paths are open back-door paths?\nWhat variables must be controlled for in order to identify the effect of X on Y? (only list what must be controlled for, not anything that additionally could be controlled for).",
    "crumbs": [
      "Part 1",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Week 3</span>"
    ]
  },
  {
    "objectID": "week03.html#exercise-7",
    "href": "week03.html#exercise-7",
    "title": "3  Week 3",
    "section": "3.8 Exercise",
    "text": "3.8 Exercise\n\n\n\n\n\n\nWhich of the following describes a causal path where all the arrows point away from the treatment?\n\nOpen Path\nClosed Path\nFront Door Path\nBack Door Path",
    "crumbs": [
      "Part 1",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Week 3</span>"
    ]
  },
  {
    "objectID": "week03.html#exercise-8",
    "href": "week03.html#exercise-8",
    "title": "3  Week 3",
    "section": "3.9 Exercise",
    "text": "3.9 Exercise\n\n\n\n\n\n\nConsider the figure below, which depicts the relationship between teaching quality, number of publications (e.g., articles, books), and popularity among scholars and students in a population of professors.\n\n\n\n\n\n\nWhat type of variable is Popularity in one path on this diagram?\nDiscuss what would happen if you controlled for Popularity.",
    "crumbs": [
      "Part 1",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Week 3</span>"
    ]
  },
  {
    "objectID": "week03.html#exercise-9",
    "href": "week03.html#exercise-9",
    "title": "3  Week 3",
    "section": "3.10 Exercise",
    "text": "3.10 Exercise\n\n\n\n\n\n\nGo to the app Steve showed us in class.\nhttps://cbdrh.shinyapps.io/daggle/\nSpend some time noodling around with it and upload screenshots with the right answer for three DAGs with 4, 6, and 8 nodes each. Set the complexity to “difficult.”",
    "crumbs": [
      "Part 1",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Week 3</span>"
    ]
  },
  {
    "objectID": "week03.html#exercise-10",
    "href": "week03.html#exercise-10",
    "title": "3  Week 3",
    "section": "3.11 Exercise",
    "text": "3.11 Exercise\nHouse of DAG Simulation.\nI’ve included a little script with a couple of functions meant to illustrate the connection between DAGs and the estimands we saw in class (ATE, ATT, ATC).\nSave it to your project and load it using the source() function.\nYou should see a function called hod_simulation() which creates a dataset that corresponds to the following DAG:\n\n\n\n\n\n\n\\(Y\\): outcome\n\\(T\\): treatment\n\\(U\\): unobserved confounder\n\\(S\\): affects selection into \\(T\\)\n\\(X\\): affects \\(Y\\) directly\n\n\n\nThe hod_simulation() function has the following arguments:\n\nN: Sample Size\nrho: The correlation between \\(S\\) and \\(X\\), it accepts values between -1 and 1.\nBt: this is the treatment effect.\nBx: this is the direct effect of \\(X\\) on \\(Y\\)\n\nNote. There’s bunch of stuff going on under the hood, but we won’t worry about that this week.\nThis is the dataset it creates:\n\n\nCode\nsource(\"hod_simulation_functions.R\")\n\n\nStandard Error ~  0.322 \nPower ~  0.873\n\n\nJoining with `by = join_by(variable)`\n\n\n# A tibble: 4 × 3\n  variable    sd  mean\n  &lt;chr&gt;    &lt;dbl&gt; &lt;dbl&gt;\n1 y        5.22   1.51\n2 t        0.500  0.51\n3 x        0.995  1.01\n4 s        0.970  1.02\n\n\nCode\nset.seed(12345) ## include this so that grading is easier for me.\nd &lt;- hod_simulation(N = 1e3, Bt = 2, Bx = 4, rho = 0.8)\n\n\nStandard Error ~  0.405 \nPower ~  0.999\n\n\nJoining with `by = join_by(variable)`\n\n\n# A tibble: 4 × 3\n  variable    sd  mean\n  &lt;chr&gt;    &lt;dbl&gt; &lt;dbl&gt;\n1 y        6.75   4.98\n2 t        0.500  0.52\n3 x        1.02   1.00\n4 s        1.01   1.02\n\n\nNote. Ignore the “Standard Error” and “Power” messages.\n\n\nCode\nd\n\n\n# A tibble: 1,000 × 6\n       y0     y1     t      y       x      s\n *  &lt;dbl&gt;  &lt;dbl&gt; &lt;int&gt;  &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;\n 1  3.03   5.03      1  5.03   1.90   1.84  \n 2  5.20   7.20      1  7.20   0.545  0.699 \n 3 -0.351  1.65      1  1.65  -0.355  0.729 \n 4 -2.76  -0.764     1 -0.764  1.03   1.44  \n 5 -3.79  -1.79      0 -3.79   0.0507 0.335 \n 6 12.9   14.9       0 12.9    2.57   1.71  \n 7  6.70   8.70      0  6.70   1.63   1.56  \n 8  3.68   5.68      0  3.68   1.40   0.694 \n 9 -1.65   0.353     1  0.353  0.307  0.0589\n10 10.8   12.8       0 10.8    1.77   2.14  \n# ℹ 990 more rows\n\n\n\n\n\n\n\n\n\nWithout looking at the results just yet… do you think the naive estimate will be larger or smaller than the “real” estimate ( \\(ATE = 2\\) )?\nCheck your answer. What are the results given by the naive estimator?\nRe-do this but set rho to -0.8 (so that \\(S\\) and \\(X\\) are now negatively correlated).\n\n\n\n\n\n\n\n\n\n\nHint: You can use group_by() and then summarize() to create a table just like Table 3.1.",
    "crumbs": [
      "Part 1",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Week 3</span>"
    ]
  },
  {
    "objectID": "week03.html#exercise-11",
    "href": "week03.html#exercise-11",
    "title": "3  Week 3",
    "section": "3.12 Exercise",
    "text": "3.12 Exercise\n\n\n\n\n\n\nTake the dataset d created in the previous question and modify it so that the treatment is now randomized (this will destroy the path between \\(S\\) and \\(T\\)).\n\n\n\n\n\n\n\n\n\nHint: You can achieve this using the sample() function on d$t.\nYou will also want to create a new d$y using the ifelse() function (or something similar to that).\n\n\n\n\n\n\n\n\n\n\nWithout looking at the results just yet… do you think the naive estimate will be larger or smaller than the “real” estimate ( \\(ATE = 2\\) )?\nCheck your answer. What are the results given by the naive estimator?\nUse lm() to predict the newly created y from t. What are the coefficient values?\nUse lm() to predict the newly created y from t and x. What are the coefficient values?",
    "crumbs": [
      "Part 1",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Week 3</span>"
    ]
  },
  {
    "objectID": "week04.html",
    "href": "week04.html",
    "title": "4  Week 4",
    "section": "",
    "text": "4.1 Exercise\nColliders\nFor this exercise I am going to ask you to create the following simulated dataset.\nCode\nN &lt;- 1e4\n\nd &lt;- tibble(\n  x = rnorm(N, 0, 1),\n  y = rnorm(N, 0, 1)\n)\nThe relationship between \\(x\\) and \\(y\\) should look something like this:",
    "crumbs": [
      "Part 1",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Week 4</span>"
    ]
  },
  {
    "objectID": "week04.html#exercise",
    "href": "week04.html#exercise",
    "title": "4  Week 4",
    "section": "",
    "text": "Now I am going to ask you to create an association between \\(x\\) and \\(y\\) via some form of collider bias.\n\\[\nx \\longrightarrow \\underbrace{\\text{z}}_{\\small {\\text{collider}}} \\longleftarrow y\n\\]\nYou are tasked to do this in four different ways, each of them corresponding to one of the plots in Figure 4.1.\n\n\n\n\n\n\n\n\n\n\n\n(1)\n\n\n\n\n\n\n\n(2)\n\n\n\n\n\n\n\n\n\n(3)\n\n\n\n\n\n\n\n(4)\n\n\n\n\n\n\nFigure 4.1: Conditioning on a Collider\n\n\n\n\n\n\n\n\n\nHint: The first three plots represent a process in which “conditioning on a collider” means that some observations are removed from the d dataset. You can then calculate the slopes simply by doing something like this:\nlm(y ~ x, data = d_filtered)\nThe last plot represents a process in which the association between \\(x\\) and \\(y\\) is created by literally “conditioning on a collider.” Something like this:\nlm(y ~ x + z, data = d)",
    "crumbs": [
      "Part 1",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Week 4</span>"
    ]
  },
  {
    "objectID": "week04.html#exercise-1",
    "href": "week04.html#exercise-1",
    "title": "4  Week 4",
    "section": "4.2 Exercise",
    "text": "4.2 Exercise\nNote. The following exercises are problems from NHK (Chapter 10). If you have issues with some of the terminology used, you should be able to figure it out from reading the book.\nDefine in your own words (i.e., don’t just copy down what’s written in the glossary) each of the following terms:\n\nConditional average treatment effect\nAverage treatment on the treated\nAverage treatment on the untreated",
    "crumbs": [
      "Part 1",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Week 4</span>"
    ]
  },
  {
    "objectID": "week04.html#exercise-2",
    "href": "week04.html#exercise-2",
    "title": "4  Week 4",
    "section": "4.3 Exercise",
    "text": "4.3 Exercise\nProvide an example of a treatment effect that you would expect to be highly heterogeneous, and explain why you think it is likely to be heterogeneous.",
    "crumbs": [
      "Part 1",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Week 4</span>"
    ]
  },
  {
    "objectID": "week04.html#exercise-3",
    "href": "week04.html#exercise-3",
    "title": "4  Week 4",
    "section": "4.4 Exercise",
    "text": "4.4 Exercise\nConsider the data in the table below that shows the hypothetical treatment effect of cognitive behavioral therapy on depression for six participants. For the sake of this example, the six participants represent the population of interest.\n\n\n\nCase\nAge\nGender\nEffect\n\n\n\n\nA\n15\nMan\n7\n\n\nB\n40\nWoman\n3\n\n\nC\n30\nWoman\n7\n\n\nD\n20\nNon-binary\n8\n\n\nE\n15\nMan\n7\n\n\nF\n25\nWoman\n4\n\n\n\n\nWhat is the overall average treatment effect for the population?\nWhat is the average treatment effect for Women?\nIf nearly all Non-binary people get treated, and about half of all Women get treated, and we control for the differences between Women and Non-binary people, what kind of treatment effect average will we get, and what can we say about the numerical estimate we’ll get?\nIf we assume that, in the absence of treatment, everyone would have had the same outcome, and also only teenagers (19 or younger) ever receive treatment, and we compare treated people to control people, what kind of treatment effect average will we get, and what can we say about the numerical estimate we’ll get?",
    "crumbs": [
      "Part 1",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Week 4</span>"
    ]
  },
  {
    "objectID": "week04.html#exercise-4",
    "href": "week04.html#exercise-4",
    "title": "4  Week 4",
    "section": "4.5 Exercise",
    "text": "4.5 Exercise\nGive an example where the average treatment effect on the treated would be more useful to consider than the overall average treatment effect, and explain why.",
    "crumbs": [
      "Part 1",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Week 4</span>"
    ]
  },
  {
    "objectID": "week04.html#exercise-5",
    "href": "week04.html#exercise-5",
    "title": "4  Week 4",
    "section": "4.6 Exercise",
    "text": "4.6 Exercise\nWhich of the following describes the average treatment effect of assigning treatment, whether or not treatment is actually received?\n\nLocal average treatment effect\nAverage treatment on the treated\nIntent-to-treat\nVariance-weighted average treatment effect",
    "crumbs": [
      "Part 1",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Week 4</span>"
    ]
  },
  {
    "objectID": "week04.html#exercise-6",
    "href": "week04.html#exercise-6",
    "title": "4  Week 4",
    "section": "4.7 Exercise",
    "text": "4.7 Exercise\nSuppose you are conducting an experiment to see whether pricing cookies at $1.99 versus $2 affects the decision to purchase the cookies. The population of interest is all adults in the United States. You recruit people from your university to participate and randomize them to either see cookies priced as $1.99 or $2, then write down whether they purchased cookies. What kind of average treatment effect can you identify from this experiment?",
    "crumbs": [
      "Part 1",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Week 4</span>"
    ]
  },
  {
    "objectID": "week04.html#exercise-7",
    "href": "week04.html#exercise-7",
    "title": "4  Week 4",
    "section": "4.8 Exercise",
    "text": "4.8 Exercise\nFor each of the following identification strategies, what kind of treatment effect(s) is most likely to be identified?\n\nA randomized experiment using a representative sample\nTrue randomization within only a certain demographic group\nClosing back door paths connected to variation in treatment\nIsolating the part of the variation in treatment variable that is driven by an exogenous variable\nThe control group is comparable to the treatment group, but treatment effects may be different across these groups",
    "crumbs": [
      "Part 1",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Week 4</span>"
    ]
  },
  {
    "objectID": "week05.html",
    "href": "week05.html",
    "title": "5  Week 5",
    "section": "",
    "text": "5.1 Instructions\nBoth functions have an argument called newdata, which you can use doing something similar to this toy example:\nCode\nols &lt;- lm(mpg ~ disp + am, data = mtcars)\n\nnew_am0 &lt;- mtcars |&gt; \n  mutate(am = 0)\n\nnew_am1 &lt;- mtcars |&gt; \n  mutate(am = 1)\n\np0 &lt;- predict(ols, newdata = new_am0) ## predictions for am == 0\np1 &lt;- predict(ols, newdata = new_am1) ## predictions for am == 1",
    "crumbs": [
      "Part 1",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Week 5</span>"
    ]
  },
  {
    "objectID": "week05.html#instructions",
    "href": "week05.html#instructions",
    "title": "5  Week 5",
    "section": "",
    "text": "This homework has three sections.\nEach exercise has two marginaleffects outputs: (1) the ATE estimate and (2) the ATT/ATU estimates.\nYou will have to reproduce these estimates without using marginaleffects. There are a couple of ways to do this, but you will probably end up using the predict() function (from base R), or the augment() function (from the broom package).\n\n\n\n\n\n\n\n\n\n\n\nBonus\nThe avg_slopes() function has an arguments called hypothesis which lets you estimate a standard error for the difference between the ATT and the ATU (among other things). This shows up in Steve’s code for this week.\nIf you are done early, I suggest you try and calculate one of these standard errors without avg_slopes (e.g., using a bootstrap).",
    "crumbs": [
      "Part 1",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Week 5</span>"
    ]
  },
  {
    "objectID": "week05.html#linear-regression",
    "href": "week05.html#linear-regression",
    "title": "5  Week 5",
    "section": "5.2 Linear Regression",
    "text": "5.2 Linear Regression\nWe will use this data.\n\n\nCode\nd &lt;- gss2022 |&gt; \n  select(tvhours, degree, madeg, padeg) |&gt; \n  mutate(pardeg = pmax(madeg, padeg, na.rm = TRUE),\n         college = if_else(degree &gt;= 3, 1L, 0L),\n         parcol = if_else(pardeg &gt;= 3, 1L, 0L)) |&gt;\n  select(tvhours, college, parcol) |&gt; \n  drop_na()\n\n\n\n5.2.1 Exercise\nAdditive link function, no interactions\n\n\nCode\nmod1 &lt;- lm(tvhours ~ college + parcol, data = d)\n\n# ATE estimate\navg_slopes(mod1, variables = \"college\") |&gt; \n  tidy()\n\n\n# A tibble: 1 × 9\n  term  contrast estimate std.error statistic p.value s.value conf.low conf.high\n  &lt;chr&gt; &lt;chr&gt;       &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1 coll… 1 - 0      -0.796     0.151     -5.27 1.35e-7    22.8    -1.09    -0.500\n\n\nANSWER GOES HERE\n\n\nCode\n# ATT/ATU estimate\navg_slopes(\n  model = mod1, \n  variables = \"college\",\n  by = \"college\" # separately by treatment group\n) |&gt; \n  tidy()\n\n\n# A tibble: 2 × 13\n  term    contrast college estimate std.error statistic p.value s.value conf.low\n  &lt;chr&gt;   &lt;chr&gt;      &lt;int&gt;    &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;\n1 college mean(1)…       0   -0.796     0.151     -5.27 1.35e-7    22.8    -1.09\n2 college mean(1)…       1   -0.796     0.151     -5.27 1.35e-7    22.8    -1.09\n# ℹ 4 more variables: conf.high &lt;dbl&gt;, predicted_lo &lt;dbl&gt;, predicted_hi &lt;dbl&gt;,\n#   predicted &lt;dbl&gt;\n\n\nANSWER GOES HERE\n\n\n5.2.2 Exercise\nAdditive link function, with interactions\n\n\nCode\nmod2 &lt;- lm(tvhours ~ college * parcol, data = d)\n\n# ATE estimate\navg_slopes(mod2, variables = \"college\") |&gt; \n  tidy()\n\n\n# A tibble: 1 × 9\n  term  contrast estimate std.error statistic p.value s.value conf.low conf.high\n  &lt;chr&gt; &lt;chr&gt;       &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1 coll… 1 - 0      -0.803     0.152     -5.29 1.20e-7    23.0    -1.10    -0.506\n\n\nANSWER GOES HERE\n\n\nCode\n# ATT/ATU estimate\navg_slopes(\n  model = mod2, \n  variables = \"college\",\n  by = \"college\" # separately by treatment group\n) |&gt; \n  tidy()\n\n\n# A tibble: 2 × 13\n  term    contrast college estimate std.error statistic p.value s.value conf.low\n  &lt;chr&gt;   &lt;chr&gt;      &lt;int&gt;    &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;\n1 college mean(1)…       0   -0.821     0.160     -5.14 2.70e-7    21.8    -1.13\n2 college mean(1)…       1   -0.772     0.159     -4.87 1.13e-6    19.8    -1.08\n# ℹ 4 more variables: conf.high &lt;dbl&gt;, predicted_lo &lt;dbl&gt;, predicted_hi &lt;dbl&gt;,\n#   predicted &lt;dbl&gt;\n\n\nANSWER GOES HERE",
    "crumbs": [
      "Part 1",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Week 5</span>"
    ]
  },
  {
    "objectID": "week05.html#poisson-regression",
    "href": "week05.html#poisson-regression",
    "title": "5  Week 5",
    "section": "5.3 Poisson Regression",
    "text": "5.3 Poisson Regression\nWe will use this data.\n\n\nCode\nd &lt;- gss2022 |&gt;\n  filter(wrkstat == 1) |&gt; # full time workers\n  select(realrinc, degree, madeg, padeg, sex, age) |&gt; \n  mutate(pardeg = pmax(madeg, padeg, na.rm = TRUE),\n         college = if_else(degree &gt;= 3, 1L, 0L),\n         parcol = if_else(pardeg &gt;= 3, 1L, 0L),\n         female = if_else(sex == 2, 1L, 0L),\n         realrinc = floor(realrinc)) |&gt;             # integer\n  select(realrinc, college, parcol, female, age) |&gt; \n  drop_na()\n\n\n\n5.3.1 Exercise\nUsing the log-counts, no interactions\n\n\nCode\nqp1 &lt;- glm(realrinc ~ college + (parcol + female + age + I(age^2)), \n           data = d,\n           family = \"quasipoisson\")\n\navg_slopes(qp1,\n           variables = \"college\",\n           type = \"link\") |&gt; \n  tidy()\n\n\n# A tibble: 1 × 9\n  term    contrast estimate std.error statistic  p.value s.value conf.low\n  &lt;chr&gt;   &lt;chr&gt;       &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;\n1 college 1 - 0       0.599    0.0510      11.7 7.45e-32    103.    0.499\n# ℹ 1 more variable: conf.high &lt;dbl&gt;\n\n\nANSWER GOES HERE\n\n\nCode\navg_slopes(qp1,\n           variables = \"college\",\n           type = \"link\",\n           by = \"college\") |&gt; # separately by treatment group\n  tidy()\n\n\n# A tibble: 2 × 13\n  term   contrast college estimate std.error statistic  p.value s.value conf.low\n  &lt;chr&gt;  &lt;chr&gt;      &lt;int&gt;    &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;\n1 colle… mean(1)…       0    0.599    0.0510      11.7 7.45e-32    103.    0.499\n2 colle… mean(1)…       1    0.599    0.0510      11.7 7.45e-32    103.    0.499\n# ℹ 4 more variables: conf.high &lt;dbl&gt;, predicted_lo &lt;dbl&gt;, predicted_hi &lt;dbl&gt;,\n#   predicted &lt;dbl&gt;\n\n\nANSWER GOES HERE\n\n\n5.3.2 Exercise\nNon-linear response, no interactions\n\n\nCode\navg_slopes(qp1,\n           variables = \"college\",\n           type = \"response\") |&gt; \n  tidy()\n\n\n# A tibble: 1 × 9\n  term    contrast estimate std.error statistic  p.value s.value conf.low\n  &lt;chr&gt;   &lt;chr&gt;       &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;\n1 college 1 - 0      21237.     1831.      11.6 4.18e-31    101.   17649.\n# ℹ 1 more variable: conf.high &lt;dbl&gt;\n\n\nANSWER GOES HERE\n\n\nCode\navg_slopes(qp1,\n           variables = \"college\",\n           type = \"response\",\n           by = \"college\") |&gt; # separately by treatment group\n  tidy()\n\n\n# A tibble: 2 × 13\n  term   contrast college estimate std.error statistic  p.value s.value conf.low\n  &lt;chr&gt;  &lt;chr&gt;      &lt;int&gt;    &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;\n1 colle… mean(1)…       0   20636.     1861.      11.1 1.41e-28    92.5   16988.\n2 colle… mean(1)…       1   21977.     1816.      12.1 1.05e-33   110.    18417.\n# ℹ 4 more variables: conf.high &lt;dbl&gt;, predicted_lo &lt;dbl&gt;, predicted_hi &lt;dbl&gt;,\n#   predicted &lt;dbl&gt;\n\n\n\n\n5.3.3 Exercise\nUsing the log-counts, with interactions\n\n\nCode\nqp2 &lt;- glm(realrinc ~ college * (parcol + female + age + I(age^2)), \n           data = d,\n           family = \"quasipoisson\")\n\navg_slopes(qp2,\n           variables = \"college\",\n           type = \"link\") |&gt; \n  tidy()\n\n\n# A tibble: 1 × 9\n  term    contrast estimate std.error statistic  p.value s.value conf.low\n  &lt;chr&gt;   &lt;chr&gt;       &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;\n1 college 1 - 0       0.580    0.0543      10.7 1.20e-26    86.1    0.474\n# ℹ 1 more variable: conf.high &lt;dbl&gt;\n\n\nANSWER GOES HERE\n\n\nCode\navg_slopes(qp2,\n           variables = \"college\",\n           type = \"link\",\n           by = \"college\") |&gt; # separately by treatment group\n  tidy()\n\n\n# A tibble: 2 × 13\n  term   contrast college estimate std.error statistic  p.value s.value conf.low\n  &lt;chr&gt;  &lt;chr&gt;      &lt;int&gt;    &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;\n1 colle… mean(1)…       0    0.567    0.0571      9.94 2.77e-23    74.9    0.455\n2 colle… mean(1)…       1    0.596    0.0600      9.94 2.87e-23    74.9    0.479\n# ℹ 4 more variables: conf.high &lt;dbl&gt;, predicted_lo &lt;dbl&gt;, predicted_hi &lt;dbl&gt;,\n#   predicted &lt;dbl&gt;\n\n\nANSWER GOES HERE\n\n\n5.3.4 Exercise\nNon-linear response, with interactions\n\n\nCode\navg_slopes(qp2,\n           variables = \"college\",\n           type = \"response\") |&gt; \n  tidy()\n\n\n# A tibble: 1 × 9\n  term    contrast estimate std.error statistic  p.value s.value conf.low\n  &lt;chr&gt;   &lt;chr&gt;       &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;\n1 college 1 - 0      21190.     1817.      11.7 1.99e-31    102.   17629.\n# ℹ 1 more variable: conf.high &lt;dbl&gt;\n\n\nANSWER GOES HERE\n\n\nCode\navg_slopes(qp2,\n           variables = \"college\",\n           type = \"response\",\n           by = \"college\") |&gt; # separately by treatment group\n  tidy()\n\n\n# A tibble: 2 × 13\n  term   contrast college estimate std.error statistic  p.value s.value conf.low\n  &lt;chr&gt;  &lt;chr&gt;      &lt;int&gt;    &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;\n1 colle… mean(1)…       0   20196.     1937.      10.4 1.90e-25    82.1   16400.\n2 colle… mean(1)…       1   22411.     1963.      11.4 3.51e-30    97.8   18563.\n# ℹ 4 more variables: conf.high &lt;dbl&gt;, predicted_lo &lt;dbl&gt;, predicted_hi &lt;dbl&gt;,\n#   predicted &lt;dbl&gt;\n\n\nANSWER GOES HERE",
    "crumbs": [
      "Part 1",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Week 5</span>"
    ]
  },
  {
    "objectID": "week05.html#logistic-regression",
    "href": "week05.html#logistic-regression",
    "title": "5  Week 5",
    "section": "5.4 Logistic Regression",
    "text": "5.4 Logistic Regression\nWe will use this data.\n\n\nCode\nd &lt;- gss2022 |&gt;\n  select(abany, degree, madeg, padeg, sex, age) |&gt; \n  mutate(pardeg = pmax(madeg, padeg, na.rm = TRUE),\n         college = if_else(degree &gt;= 3, 1L, 0L),\n         parcol = if_else(pardeg &gt;= 3, 1L, 0L),\n         female = if_else(sex == 2, 1L, 0L),\n         abany = if_else(abany == 1, 1L, 0L)) |&gt;\n  select(abany, college, parcol, female, age) |&gt; \n  drop_na()\n\n\n\n5.4.1 Exercise\nUsing log-odds, no interactions\n\n\nCode\nlr1 &lt;- glm(abany ~ college + (parcol + female + age + I(age^2)),\n          data = d,\n          family = binomial)\n\n# ATE estimate\navg_slopes(lr1,\n           variables = \"college\",\n           type = \"link\") |&gt; \n  tidy()\n\n\n# A tibble: 1 × 9\n  term  contrast estimate std.error statistic p.value s.value conf.low conf.high\n  &lt;chr&gt; &lt;chr&gt;       &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1 coll… 1 - 0       0.438     0.146      3.00 0.00273    8.52    0.151     0.724\n\n\nANSWER GOES HERE\n\n\nCode\navg_slopes(lr1,\n           variables = \"college\",\n           by = \"college\",\n           type = \"link\") |&gt; \n  tidy()\n\n\n# A tibble: 2 × 13\n  term    contrast college estimate std.error statistic p.value s.value conf.low\n  &lt;chr&gt;   &lt;chr&gt;      &lt;int&gt;    &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;\n1 college mean(1)…       0    0.438     0.146      3.00 0.00273    8.52    0.151\n2 college mean(1)…       1    0.438     0.146      3.00 0.00273    8.52    0.151\n# ℹ 4 more variables: conf.high &lt;dbl&gt;, predicted_lo &lt;dbl&gt;, predicted_hi &lt;dbl&gt;,\n#   predicted &lt;dbl&gt;\n\n\nANSWER GOES HERE\n\n\n5.4.2 Exercise\nUsing non-linear response (aka probabilities), no interactions\n\n\nCode\n# ATE estimate\navg_slopes(lr1,\n           variables = \"college\",\n           type = \"response\") |&gt; \n  tidy()\n\n\n# A tibble: 1 × 9\n  term  contrast estimate std.error statistic p.value s.value conf.low conf.high\n  &lt;chr&gt; &lt;chr&gt;       &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1 coll… 1 - 0       0.102    0.0337      3.02 0.00249    8.65   0.0359     0.168\n\n\nANSWER GOES HERE\n\n\nCode\navg_slopes(lr1,\n           variables = \"college\",\n           by = \"college\",\n           type = \"response\") |&gt; \n  tidy()\n\n\n# A tibble: 2 × 13\n  term    contrast college estimate std.error statistic p.value s.value conf.low\n  &lt;chr&gt;   &lt;chr&gt;      &lt;int&gt;    &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;\n1 college mean(1)…       0   0.104     0.0341      3.04 0.00235    8.73   0.0369\n2 college mean(1)…       1   0.0989    0.0330      2.99 0.00275    8.51   0.0342\n# ℹ 4 more variables: conf.high &lt;dbl&gt;, predicted_lo &lt;dbl&gt;, predicted_hi &lt;dbl&gt;,\n#   predicted &lt;dbl&gt;\n\n\nANSWER GOES HERE\n\n\n5.4.3 Exercise\nUsing log-odds, with interactions\n\n\nCode\nlr2 &lt;- glm(abany ~ college * (parcol + female + age + I(age^2)),\n          data = d,\n          family = binomial)\n\n# ATE estimate\navg_slopes(lr2,\n           variables = \"college\",\n           type = \"link\") |&gt; \n  tidy()\n\n\n# A tibble: 1 × 9\n  term  contrast estimate std.error statistic p.value s.value conf.low conf.high\n  &lt;chr&gt; &lt;chr&gt;       &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1 coll… 1 - 0       0.453     0.149      3.04 0.00239    8.71    0.161     0.745\n\n\nANSWER GOES HERE\n\n\nCode\navg_slopes(lr2,\n           variables = \"college\",\n           by = \"college\",\n           type = \"link\") |&gt; \n  tidy()\n\n\n# A tibble: 2 × 13\n  term    contrast college estimate std.error statistic p.value s.value conf.low\n  &lt;chr&gt;   &lt;chr&gt;      &lt;int&gt;    &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;\n1 college mean(1)…       0    0.450     0.159      2.83 0.00460    7.76    0.139\n2 college mean(1)…       1    0.458     0.161      2.85 0.00435    7.84    0.143\n# ℹ 4 more variables: conf.high &lt;dbl&gt;, predicted_lo &lt;dbl&gt;, predicted_hi &lt;dbl&gt;,\n#   predicted &lt;dbl&gt;\n\n\nANSWER GOES HERE\n\n\n5.4.4 Exercise\nUsing non-linear response (aka probabilities), with interactions\n\n\nCode\n# ATE estimate\navg_slopes(lr2,\n           variables = \"college\",\n           type = \"response\") |&gt; \n  tidy()\n\n\n# A tibble: 1 × 9\n  term  contrast estimate std.error statistic p.value s.value conf.low conf.high\n  &lt;chr&gt; &lt;chr&gt;       &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1 coll… 1 - 0       0.103    0.0338      3.05 0.00228    8.78   0.0369     0.169\n\n\nANSWER GOES HERE\n\n\nCode\navg_slopes(lr2,\n           variables = \"college\",\n           by = \"college\",\n           type = \"response\") |&gt; \n  tidy()\n\n\n# A tibble: 2 × 13\n  term    contrast college estimate std.error statistic p.value s.value conf.low\n  &lt;chr&gt;   &lt;chr&gt;      &lt;int&gt;    &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;\n1 college mean(1)…       0    0.104    0.0363      2.87 0.00407    7.94   0.0332\n2 college mean(1)…       1    0.101    0.0353      2.87 0.00415    7.91   0.0320\n# ℹ 4 more variables: conf.high &lt;dbl&gt;, predicted_lo &lt;dbl&gt;, predicted_hi &lt;dbl&gt;,\n#   predicted &lt;dbl&gt;\n\n\nANSWER GOES HERE",
    "crumbs": [
      "Part 1",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Week 5</span>"
    ]
  },
  {
    "objectID": "sec-solutions.html",
    "href": "sec-solutions.html",
    "title": "Solutions 3",
    "section": "",
    "text": "Exercise\nHouse of DAG Simulation.\nI’ve included a little script with a couple of functions meant to illustrate the connection between DAGs and the estimands we saw in class (ATE, ATT, ATC).\nSave it to your project and load it using the source() function.\nYou should see a function called hod_simulation() which creates a dataset that corresponds to the following DAG:\nThe hod_simulation() function has the following arguments:\nNote. There’s bunch of stuff going on under the hood, but we won’t worry about that this week.\nThis is the dataset it creates:\nCode\nsource(\"hod_simulation_functions.R\")\n\n\nStandard Error ~  0.322 \nPower ~  0.873\n\n\nJoining with `by = join_by(variable)`\n\n\n# A tibble: 4 × 3\n  variable    sd  mean\n  &lt;chr&gt;    &lt;dbl&gt; &lt;dbl&gt;\n1 y        5.17  1.63 \n2 t        0.500 0.484\n3 x        1.00  0.971\n4 s        1.01  0.964\n\n\nCode\nset.seed(12345) ## include this so that grading is easier for me.\nd &lt;- hod_simulation(N = 1e3, Bt = 2, Bx = 4, rho = 0.8)\n\n\nStandard Error ~  0.405 \nPower ~  0.999\n\n\nJoining with `by = join_by(variable)`\n\n\n# A tibble: 4 × 3\n  variable    sd  mean\n  &lt;chr&gt;    &lt;dbl&gt; &lt;dbl&gt;\n1 y        6.75   4.98\n2 t        0.500  0.52\n3 x        1.02   1.00\n4 s        1.01   1.02\nNote. Ignore the “Standard Error” and “Power” messages.\nCode\nd\n\n\n# A tibble: 1,000 × 6\n       y0     y1     t      y       x      s\n *  &lt;dbl&gt;  &lt;dbl&gt; &lt;int&gt;  &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;\n 1  3.03   5.03      1  5.03   1.90   1.84  \n 2  5.20   7.20      1  7.20   0.545  0.699 \n 3 -0.351  1.65      1  1.65  -0.355  0.729 \n 4 -2.76  -0.764     1 -0.764  1.03   1.44  \n 5 -3.79  -1.79      0 -3.79   0.0507 0.335 \n 6 12.9   14.9       0 12.9    2.57   1.71  \n 7  6.70   8.70      0  6.70   1.63   1.56  \n 8  3.68   5.68      0  3.68   1.40   0.694 \n 9 -1.65   0.353     1  0.353  0.307  0.0589\n10 10.8   12.8       0 10.8    1.77   2.14  \n# ℹ 990 more rows\nCode\nd |&gt; \n  group_by(t) |&gt; \n  summarize(y = mean(y))\n\n\n# A tibble: 2 × 2\n      t     y\n  &lt;int&gt; &lt;dbl&gt;\n1     0  2.00\n2     1  7.72\n\n\nCode\nlm(y ~ t, data = d)\n\n\n\nCall:\nlm(formula = y ~ t, data = d)\n\nCoefficients:\n(Intercept)            t  \n      2.002        5.718\nCode\nd &lt;- hod_simulation(N = 1e5, Bt = 2, Bx = 4, rho = -0.8)\n\n\nStandard Error ~  0.04 \nPower ~  1\n\n\nJoining with `by = join_by(variable)`\n\n\n# A tibble: 4 × 3\n  variable    sd  mean\n  &lt;chr&gt;    &lt;dbl&gt; &lt;dbl&gt;\n1 y        6.21  5.01 \n2 t        0.500 0.498\n3 x        1.00  1.01 \n4 s        1.00  0.997\n\n\nCode\nd |&gt; \n  group_by(t) |&gt; \n  summarize(y = mean(y))\n\n\n# A tibble: 2 × 2\n      t     y\n  &lt;int&gt; &lt;dbl&gt;\n1     0  5.82\n2     1  4.19\n\n\nCode\nlm(y ~ t, data = d)\n\n\n\nCall:\nlm(formula = y ~ t, data = d)\n\nCoefficients:\n(Intercept)            t  \n      5.818       -1.628",
    "crumbs": [
      "Solutions 3"
    ]
  },
  {
    "objectID": "sec-solutions.html#exercise",
    "href": "sec-solutions.html#exercise",
    "title": "Solutions 3",
    "section": "",
    "text": "\\(Y\\): outcome\n\\(T\\): treatment\n\\(U\\): unobserved confounder\n\\(S\\): affects selection into \\(T\\)\n\\(X\\): affects \\(Y\\) directly\n\n\n\n\n\nN: Sample Size\nrho: The correlation between \\(S\\) and \\(X\\), it accepts values between -1 and 1.\nBt: this is the treatment effect.\nBx: this is the direct effect of \\(X\\) on \\(Y\\)\n\n\n\n\n\n\n\nWithout looking at the results just yet… do you think the naive estimate will be larger or smaller than the “real” estimate ( \\(ATE = 2\\) )?\nCheck your answer. What are the results given by the naive estimator?\n\n\n\nRe-do this but set rho to -0.8 (so that \\(S\\) and \\(X\\) are now negatively correlated).",
    "crumbs": [
      "Solutions 3"
    ]
  },
  {
    "objectID": "sec-solutions.html#exercise-1",
    "href": "sec-solutions.html#exercise-1",
    "title": "Solutions 3",
    "section": "Exercise",
    "text": "Exercise\nTake the dataset d created in the previous question and modify it so that the treatment is now randomized (this will destroy the path between \\(S\\) and \\(T\\)).\n\n\nCode\nd$t &lt;- sample(d$t)\n\nd &lt;- d |&gt; \n  mutate(y = ifelse(\n    test = as.logical(t), \n    yes = y1, \n    no = y0\n  )\n)\n\n\n\n\nCode\nlm(y ~ t, data = d)\n\n\n\nCall:\nlm(formula = y ~ t, data = d)\n\nCoefficients:\n(Intercept)            t  \n      4.032        1.958  \n\n\n\n\n\n\n\n\nTip\n\n\n\nHint: You can achieve this using the sample() function on d$t.\nYou will also want to create a new d$y using the ifelse() function (or something similar to that).\n\n\n\nWithout looking at the results just yet… do you think the naive estimate will be larger or smaller than the “real” estimate ( \\(ATE = 2\\) )?\nCheck your answer. What are the results given by the naive estimator?\nUse lm() to predict the newly created y from t. What are the coefficient values?\nUse lm() to predict the newly created y from t and x. What are the coefficient values?",
    "crumbs": [
      "Solutions 3"
    ]
  },
  {
    "objectID": "solutions01.html",
    "href": "solutions01.html",
    "title": "6  Solutions 1",
    "section": "",
    "text": "6.1 Exercise\nGlossary",
    "crumbs": [
      "Solutions 3",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Solutions 1</span>"
    ]
  },
  {
    "objectID": "solutions01.html#exercise",
    "href": "solutions01.html#exercise",
    "title": "6  Solutions 1",
    "section": "6.2 Exercise",
    "text": "6.2 Exercise\n\n\nPackages\nlibrary(tidyverse)\ntheme_set(theme_light(base_family = \"Optima\"))\n\n\nThe following data frame contains the potential outcomes for 8 individuals.\n\n\nCode\nd &lt;- data.frame(\n  T = c(0, 0, 1, 0, 0, 1, 1, 1),\n  Y0 = c(5, 8, 5, 12, 4, 8, 4, 9),\n  Y1 = c(5, 10, 3, 13, 2, 9, 1, 13), \n  id = LETTERS[1:8]\n)\n\nd\n\n\n  T Y0 Y1 id\n1 0  5  5  A\n2 0  8 10  B\n3 1  5  3  C\n4 0 12 13  D\n5 0  4  2  E\n6 1  8  9  F\n7 1  4  1  G\n8 1  9 13  H\n\n\nThe variable T depicts whether someone got the “treatment” or not.\nCreate a new variable called Y that contains the observed outcomes.\n\n\nCode\nd &lt;- d |&gt; \n  mutate(Y = ifelse(as.logical(T), Y1, Y0))\n\nd\n\n\n  T Y0 Y1 id  Y\n1 0  5  5  A  5\n2 0  8 10  B  8\n3 1  5  3  C  3\n4 0 12 13  D 12\n5 0  4  2  E  4\n6 1  8  9  F  9\n7 1  4  1  G  1\n8 1  9 13  H 13\n\n\nWhat is the Average Treatment Effect (ATE) for this 8 person experiment?\n\n\nCode\n## ate --- based on \"complete\" data\nd |&gt; \n  mutate(TE = Y1 - Y0) |&gt; \n  summarize(ATE = mean(TE))\n\n\n    ATE\n1 0.125\n\n\nCode\n## ate --- \"naive\" estimate based on observed data\nmean(d$Y[d$T == 1]) - mean(d$Y[d$T == 0])\n\n\n[1] -0.75",
    "crumbs": [
      "Solutions 3",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Solutions 1</span>"
    ]
  },
  {
    "objectID": "solutions01.html#exercise-1",
    "href": "solutions01.html#exercise-1",
    "title": "6  Solutions 1",
    "section": "6.3 Exercise",
    "text": "6.3 Exercise\nSimulate a new completely randomized experiment on these 8 people; that is, re sample \\(T\\) at random so that equal numbers get the treatment and the control.\n\n\nCode\nd$T &lt;- sample(d$T)\n\n\nCreate a new variable called Y that contains the observed outcomes.\n\n\nCode\nd &lt;- d |&gt; \n  mutate(Y = ifelse(as.logical(T), Y1, Y0))\n\n\nWhat is the Average Treatment Effect (ATE) for this 8 person experiment?\n\n\nCode\n## naive estimate\nmean(d$Y[d$T == 1]) - mean(d$Y[d$T == 0])\n\n\n[1] 1.25\n\n\nDo this a couple of times (at least 3) and note the differences.\nI will do this a couple of hundred times.\n\n\nCode\nout &lt;- replicate(1e3, {\n  d$T &lt;- sample(d$T)\n  d$Y &lt;- ifelse(as.logical(d$T), d$Y1, d$Y0)\n  mean(d$Y[d$T == 1]) - mean(d$Y[d$T == 0])\n})\n\nsummary(out)\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n-6.5000 -1.7500  0.2500  0.1625  2.0000  6.7500 \n\n\nHow do these estimates compare to the “real” ATE?\n\n\nCode\ntibble(x = out) |&gt; \n  ggplot(aes(x, y = \"\")) + \n  geom_boxplot() + \n  geom_jitter(height = 1/10, alpha = 1/4) + \n  geom_point(x = 0.125, fill = \"pink\", shape = 21, size = 5) +\n  labs(y = NULL)\n\n\n\n\n\n\n\n\n\nThey are all over the place. But… on average they’re sort of close.",
    "crumbs": [
      "Solutions 3",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Solutions 1</span>"
    ]
  },
  {
    "objectID": "solutions01.html#exercise-2",
    "href": "solutions01.html#exercise-2",
    "title": "6  Solutions 1",
    "section": "6.4 Exercise",
    "text": "6.4 Exercise\nObviously, an experiment of 8 people will not give you enough “statistical power.”\nAssuming the ATE is \\(0.125\\), how many people would you need to enroll in this experiment to have enough statistical power?\n\n\n\n\n\n\nTip\n\n\n\nHint: There are a few different ways of giving a reasonable answer to this question. The wording of this problem is ambiguous.\n\n\nTo get at questions of statistical power we need to establish two things from the outset: (1) the “effect” we believe exists out there and (2) the desired standard error. For the latter, this usually means choosing a sample size so that the resulting standard error that will allow me to have a false negative rate of at least 80% But this is just a convention.\nSteve’s code already shows how to do this with the built-in t.test and power.t.test functions. He also had to make assumptions about the the distribution of the potential outcomes in the population. We all have to do this, except that sometimes we don’t realize it because the assumptions are hidden away in some kind of Internet sample size calculator.\nThis is how I would have done it.\nStep 1. I will assume that the standard deviation for each potential outcome in the 8 person experiment is the same in the wider population. This is a big assumption, but it’s the one I’ll go with. This is the main difference between what Steve did and what I did (his assumptions about the population variance are hidden in lines 54-56).\n\n\nCode\nsdY0 &lt;- sd(d$Y0)\nsdY0\n\n\n[1] 2.850439\n\n\nCode\nsdY1 &lt;- sd(d$Y1)\nsdY1\n\n\n[1] 4.869732\n\n\nNote. Think about what I just did. I assumed the heterogeneity in treatment effects among those who got the treatment is larger than it is for those who didn’t. Would this make sense in real life? Maybe?\nStep 2. I will assume that half the sample gets a treatment and the other half does not. This allows me to calculate the standard error of the difference in means simply as:\n\\[\n\\text{SE} = \\sqrt{\\frac{\\sigma_1^2}{n_1} + \\frac{\\sigma_2^2}{n_2}} = \\sqrt{\\frac{2(\\sigma_1^2 + \\sigma_2^2)}{n}}\n\\]\n\n\\(\\frac{n}{2} = n_1 = n_2\\)\n\nStep 3. Choose a simple heuristic so that my standard error is good enough.\n\n\n\n\n\n\nWarning\n\n\n\nNote. You might be tempted to use the simple statistical significance heuristic, according to which you need a standard error that is half the size of the effect ( \\(\\text{SE} = 0.0625\\) ). But this is wrong. If this was the case, then you would get a statistically significant ( \\(\\alpha = 0.05\\) ) only half the time. Most people go for 80%\n\n\nCode\nggplot() + \n  xlim(-1/2, 1/2) + \n  stat_function(\n    fun = \\(x) dnorm(x, 0, 0.0625), \n    geom = \"area\", aes(fill = \"null distribution\")\n  ) + \n  stat_function(\n    fun = \\(x) dnorm(x, 0.125, 0.0625), \n    geom = \"area\", aes(fill = \"sampling distribution\"), alpha = 1/4\n  ) + \n  geom_vline(xintercept = qnorm(c(0.025, 0.975), 0, 0.0625), linetype = \"dashed\") + \n  labs(\n    y = \"density\", x = \"ATE\", fill = NULL,\n    caption = \"Note: dashed lines indicate traditional statistical significance break points\",\n    subtitle = \"\\\"Power\\\" with standard error half the size of the effect\"\n  )\n\n\n\n\n\n\n\n\n\n\n\nSo, I will use the simple heuristic that I want my standard error to be a third the size of my effect, so around 0.042.\n\n\nCode\n## sample size with simple algebra\n2*(sdY0^2 + sdY1^2) / 0.042^2 \n\n\n[1] 36098.96\n\n\nBased on this simple sketch, I anticipate that my sample size will have to be huge in order to detect such a small effect.\nHow does this look like?\n\n\nCode\nggplot() + \n  xlim(-1/3, 1/3) + \n  stat_function(\n    fun = \\(x) dnorm(x, 0, 0.042), \n    geom = \"area\", aes(fill = \"null distribution\")\n  ) + \n  stat_function(\n    fun = \\(x) dnorm(x, 0.125, 0.042), \n    geom = \"area\", aes(fill = \"sampling distribution\"), alpha = 1/4\n  ) + \n  geom_vline(xintercept = qnorm(c(0.025, 0.975), 0, 0.042), linetype = \"dashed\") +\n  labs(\n    y = \"density\", x = \"ATE\", fill = NULL,\n    caption = \"Note: dashed lines indicate traditional statistical significance break points\",\n    subtitle = str_glue(\"\\\"Power\\\" with sample size of 36,099\")\n  )\n\n\n\n\n\n\n\n\n\nTo calculate the statistical power we simply calculate the area under the sampling distribution above the desired cutoff point.\n\n\nCode\ncutoff &lt;- qnorm(0.975, mean = 0, sd = 0.042) ## null distribution\npnorm(cutoff, mean = 0.125, sd = 0.042, lower.tail = FALSE)\n\n\n[1] 0.8452392\n\n\nIf you want to get more exact numbers for the traditional 80% power (and this is just another fetishized number), we can use some algebra:\n\\[\n\\begin{align}\n0 + 1.96 \\cdot \\text{SE} &= \\overbrace{0.125}^\\text{ATE} - 0.84 \\cdot \\text{SE} \\\\\n\\text{SE} &=  0.125 / 2.8 \\\\ &\\approx 0.045\n\\end{align}\n\\]\n\n\n\nCode\nqnorm(0.2, mean = 0, sd = 1)\n\n\n[1] -0.8416212\n\n\n\nWhich we can unpack to solve for the sample size:\n\\[\n\\begin{align}\n\\sqrt{\\frac{2(\\sigma^1 + \\sigma^2)}{n}} &= 0.045 \\\\\n2\\times\\frac{\\sigma_1^2 + \\sigma_2^2}{0.045^2} &= n\n\\end{align}\n\\]\nOr you could use more complicated R functions… although I find this to be much easier than algebra.\n\n\nCode\nstat_power &lt;- function(n) {\n  ## this function will use our assumptions about standard deviations \n  ## in the population and output the statistical power that corresponds\n  ## to a specific sample size\n  se &lt;- sqrt(2*(sdY0^2 + sdY1^2) / n)       ## population variance assumption\n  cutoff &lt;- qnorm(0.975, mean = 0, sd = se) ## null distribution cutoff\n  pnorm(cutoff, mean = 0.125, sd = se, lower.tail = FALSE) ## stat power\n}\n\nsample_size &lt;- function(power = 0.8, interval = c(300, 1e6)) {\n  ## this function will find the value of \"n\" for which the output\n  ## of stat_power(n) - \"power\" is zero\n  out &lt;- uniroot(\\(n) stat_power(n) - power, interval = interval)\n  out$root\n}\n\nsample_size(power = 0.8)\n\n\n[1] 31987.55\n\n\nCode\nggplot() + \n  xlim(1e3, 50e3) + \n  geom_function(fun = stat_power) + \n  geom_hline(yintercept = 0.8, linetype = \"dashed\") + \n  geom_vline(xintercept = sample_size(power = 0.8), linetype = \"dashed\") + \n  labs(x = \"Sample Size\", y = \"Statistical Power\")",
    "crumbs": [
      "Solutions 3",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Solutions 1</span>"
    ]
  },
  {
    "objectID": "solutions03.html",
    "href": "solutions03.html",
    "title": "8  Solutions 3",
    "section": "",
    "text": "8.1 Exercise\nThe missing cells are like this because of the ignorability assumption in a perfectly executed experiment.\nATE: 5,000",
    "crumbs": [
      "Solutions 3",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Solutions 3</span>"
    ]
  },
  {
    "objectID": "solutions03.html#exercise",
    "href": "solutions03.html#exercise",
    "title": "8  Solutions 3",
    "section": "",
    "text": "Group (\\(T\\))\n\\(E[Y^1]\\)\n\\(E[Y^0]\\)\n\n\n\n\n\\(T = 1\\)\n10,000\n5,000\n\n\n\\(T = 0\\)\n10,000\n5,000",
    "crumbs": [
      "Solutions 3",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Solutions 3</span>"
    ]
  },
  {
    "objectID": "solutions03.html#exercise-1",
    "href": "solutions03.html#exercise-1",
    "title": "8  Solutions 3",
    "section": "8.2 Exercise",
    "text": "8.2 Exercise\nDraw a causal diagram for the research question “do long shift hours make doctors give lower-quality care?” that incorporates the following features (and only the following features):\n\nLong shift hours affect both how tired doctors are, and how much experience they have, both of which affect the quality of care\nHow long shifts are is often decided by the hospital the doctor works at. There are plenty of other things about a given hospital that also affect the quality of care, like its funding level, how crowded it is, and so on\nNew policies that reduce shift times may be implemented at the same time (with the timing determined by some unobservable change in policy preferences) as other policies that also attempt to improve the quality of care",
    "crumbs": [
      "Solutions 3",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Solutions 3</span>"
    ]
  },
  {
    "objectID": "solutions03.html#exercise-2",
    "href": "solutions03.html#exercise-2",
    "title": "8  Solutions 3",
    "section": "8.3 Exercise",
    "text": "8.3 Exercise\nConsider this research question: Does the funding level of public schools affect student achievement for students in your country?\n\nWhat is the treatment and what is the outcome of interest?\nOutcome: student achievement. Treatment: funding level.\nWrite down a list of relevant variables.\nRelevant variables: government budget, student SES, class size, staff salary, other social welfare policies.\nWhich of the variables in your list in part b are causes of both treatment and outcome?\nGovernment budget\nWhy might we want to pay extra attention to the variables listed in part c?\nBecause they are confounders\nDraw a causal diagram of the variables listed in part b.\n\nSimplify the diagram from part e.",
    "crumbs": [
      "Solutions 3",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Solutions 3</span>"
    ]
  },
  {
    "objectID": "solutions03.html#exercise-3",
    "href": "solutions03.html#exercise-3",
    "title": "8  Solutions 3",
    "section": "8.4 Exercise",
    "text": "8.4 Exercise\nHow can a causal diagram be modified so as to avoid cyclic relationships?\nConsider the diagram below. It depicts a cyclical relationship between student achievement and motivation. If students achieve more (i.e., score well on exams), then their motivation goes up, and if their motivation goes up, they achieve more. Change the diagram so that the relationship is not cyclic anymore.\n\\[\n\\text{Student Achievement} \\longleftrightarrow \\text{Motivation}\n\\]\nPossible answer:",
    "crumbs": [
      "Solutions 3",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Solutions 3</span>"
    ]
  },
  {
    "objectID": "solutions03.html#exercise-4",
    "href": "solutions03.html#exercise-4",
    "title": "8  Solutions 3",
    "section": "8.5 Exercise",
    "text": "8.5 Exercise\nAssuming that a path has no colliders on it, what is the difference between a path being Open and Closed?\nAt least one of the variables in the path has been adjusted for—i.e., variation is removed or “controlled” for.\nNote. Remember that you’ll always sound smarter if you say “adjusted” instead of “controlled” in the context of regression with observational data.",
    "crumbs": [
      "Solutions 3",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Solutions 3</span>"
    ]
  },
  {
    "objectID": "solutions03.html#exercise-5",
    "href": "solutions03.html#exercise-5",
    "title": "8  Solutions 3",
    "section": "8.6 Exercise",
    "text": "8.6 Exercise\nConsider the below generic causal diagram.\n\n\n\n\n\n\nList every path from X to Y.\n\\[\n\\begin{align}\n&1. &&X \\to A \\to Y, \\\\\n&2. &&X \\leftarrow B \\to Y, \\\\\n&3. &&X \\leftarrow B \\leftarrow D \\to Y, \\\\\n&4. &&X \\to C \\leftarrow D \\to Y, \\\\\n&5. &&X \\to C \\leftarrow D \\to B \\to Y\n\\end{align}\n\\]\nWhich of the paths are front-door paths?\n1\nWhich of the paths are open back-door paths?\n2 and 3\nWhat variables must be controlled for in order to identify the effect of X on Y? (only list what must be controlled for, not anything that additionally could be controlled for).\n\\(B\\)",
    "crumbs": [
      "Solutions 3",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Solutions 3</span>"
    ]
  },
  {
    "objectID": "solutions03.html#exercise-6",
    "href": "solutions03.html#exercise-6",
    "title": "8  Solutions 3",
    "section": "8.7 Exercise",
    "text": "8.7 Exercise\nWhich of the following describes a causal path where all the arrows point away from the treatment?\n\nOpen Path\nClosed Path\nFront Door Path (this one)\nBack Door Path",
    "crumbs": [
      "Solutions 3",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Solutions 3</span>"
    ]
  },
  {
    "objectID": "solutions03.html#exercise-7",
    "href": "solutions03.html#exercise-7",
    "title": "8  Solutions 3",
    "section": "8.8 Exercise",
    "text": "8.8 Exercise\nConsider the figure below, which depicts the relationship between teaching quality, number of publications (e.g., articles, books), and popularity among scholars and students in a population of professors.\n\n\n\n\n\n\nWhat type of variable is Popularity in one path on this diagram?\nDiscuss what would happen if you controlled for Popularity.\n\nPopularity is a collider variable. If we “control” (remove variation) for Popularity we will artificially create a negative association between Teaching Quality and Number of Publications",
    "crumbs": [
      "Solutions 3",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Solutions 3</span>"
    ]
  },
  {
    "objectID": "solutions03.html#exercise-8",
    "href": "solutions03.html#exercise-8",
    "title": "8  Solutions 3",
    "section": "8.9 Exercise",
    "text": "8.9 Exercise\nGo to the app Steve showed us in class.\nhttps://cbdrh.shinyapps.io/daggle/\nSpend some time noodling around with it and upload screenshots with the right answer for three DAGs with 4, 6, and 8 nodes each. Set the complexity to “difficult.”\nGrading based is conditional on screenshots.",
    "crumbs": [
      "Solutions 3",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Solutions 3</span>"
    ]
  },
  {
    "objectID": "solutions03.html#exercise-9",
    "href": "solutions03.html#exercise-9",
    "title": "8  Solutions 3",
    "section": "8.10 Exercise",
    "text": "8.10 Exercise\nHouse of DAG Simulation.\nI’ve included a little script with a couple of functions meant to illustrate the connection between DAGs and the estimands we saw in class (ATE, ATT, ATC).\nSave it to your project and load it using the source() function.\nYou should see a function called hod_simulation() which creates a dataset that corresponds to the following DAG:\n\n\n\n\n\n\n\\(Y\\): outcome\n\\(T\\): treatment\n\\(U\\): unobserved confounder\n\\(S\\): affects selection into \\(T\\)\n\\(X\\): affects \\(Y\\) directly\n\n\n\nThe hod_simulation() function has the following arguments:\n\nN: Sample Size\nrho: The correlation between \\(S\\) and \\(X\\), it accepts values between -1 and 1.\nBt: this is the treatment effect.\nBx: this is the direct effect of \\(X\\) on \\(Y\\)\n\nThis is the dataset it creates:\n\n\nCode\nsource(\"hod_simulation_functions.R\")\n\n\nStandard Error ~  0.322 \nPower ~  0.873# A tibble: 4 × 3\n  variable    sd  mean\n  &lt;chr&gt;    &lt;dbl&gt; &lt;dbl&gt;\n1 y        5.23  1.53 \n2 t        0.500 0.493\n3 x        1.02  0.999\n4 s        0.985 1.00 \n\n\nCode\nset.seed(12345) ## include this so that grading is easier for me.\nd &lt;- hod_simulation(N = 1e3, Bt = 2, Bx = 4, rho = 0.8)\n\n\nStandard Error ~  0.405 \nPower ~  0.999# A tibble: 4 × 3\n  variable    sd  mean\n  &lt;chr&gt;    &lt;dbl&gt; &lt;dbl&gt;\n1 y        6.75   4.98\n2 t        0.500  0.52\n3 x        1.02   1.00\n4 s        1.01   1.02\n\n\nNote. Ignore the “Standard Error” and “Power” messages.\n\n\nCode\nglimpse(d)\n\n\nRows: 1,000\nColumns: 6\n$ y0 &lt;dbl&gt; 3.0258462, 5.2008689, -0.3510375, -2.7643240, -3.7947830, 12.879242…\n$ y1 &lt;dbl&gt; 5.0258462, 7.2008689, 1.6489625, -0.7643240, -1.7947830, 14.8792426…\n$ t  &lt;int&gt; 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1…\n$ y  &lt;dbl&gt; 5.0258462, 7.2008689, 1.6489625, -0.7643240, -3.7947830, 12.8792426…\n$ x  &lt;dbl&gt; 1.89642215, 0.54549787, -0.35506814, 1.03476207, 0.05065330, 2.5734…\n$ s  &lt;dbl&gt; 1.84099574, 0.69942604, 0.72890759, 1.44006399, 0.33470650, 1.70875…\n\n\nWithout looking at the results just yet… do you think the naive estimate will be larger or smaller than the “real” estimate ( \\(ATE = 2\\) )?\nThe results should be larger because people who get the treatment should also a higher \\(X\\), and the effect of \\(X\\) is positive.\nCheck your answer. What are the results given by the naive estimator?\n\n\nCode\nd |&gt; \n  group_by(t) |&gt; \n  summarize(across(c(y0, y1), mean))\n\n\n# A tibble: 2 × 3\n      t    y0    y1\n  &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     0  2.00  4.00\n2     1  5.72  7.72\n\n\nCode\nmean(d$y[d$t == 1]) - mean(d$y[d$t == 0])\n\n\n[1] 5.718145\n\n\nRe-do this but set rho to -0.8 (so that \\(S\\) and \\(X\\) are now negatively correlated).\n\n\nCode\nd &lt;- hod_simulation(N = 1e3, Bt = 2, Bx = 4, rho = -0.8)\n\n\nStandard Error ~  0.405 \nPower ~  0.999\n\n\nJoining with `by = join_by(variable)`\n\n\n# A tibble: 4 × 3\n  variable    sd  mean\n  &lt;chr&gt;    &lt;dbl&gt; &lt;dbl&gt;\n1 y        6.35  4.89 \n2 t        0.500 0.512\n3 x        0.992 0.962\n4 s        1.00  1.03 \n\n\nCode\nd |&gt; \n  group_by(t) |&gt; \n  summarize(across(c(y0, y1), mean))\n\n\n# A tibble: 2 × 3\n      t    y0    y1\n  &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     0  5.70  7.70\n2     1  2.12  4.12\n\n\nCode\nmean(d$y[d$t == 1]) - mean(d$y[d$t == 0])\n\n\n[1] -1.576179\n\n\nNote. You should have been able to figure out that the “naive estimator” (difference between groups) was going to be biased in the opposite side (i.e., smaller or even negative).",
    "crumbs": [
      "Solutions 3",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Solutions 3</span>"
    ]
  },
  {
    "objectID": "solutions03.html#exercise-10",
    "href": "solutions03.html#exercise-10",
    "title": "8  Solutions 3",
    "section": "8.11 Exercise",
    "text": "8.11 Exercise\nTake the dataset d created in the previous question and modify it so that the treatment is now randomized (this will destroy the path between \\(S\\) and \\(T\\)).\n\n\nCode\nd$t &lt;- sample(d$t)\nd$y &lt;- ifelse(as.logical(d$t), d$y1, d$y0)\n\n\nWithout looking at the results just yet… do you think the naive estimate will be larger or smaller than the “real” estimate ( \\(ATE = 2\\) )?\nThe answers should be roughly the same as the “real” estimate because we have effectively destroyed the backdoor path via the randomization of the treatment. But there’s still margin for sampling error, so we should also look at the standard error.\nCheck your answer. What are the results given by the naive estimator?\n\n\nCode\nmean(d$y[d$t == 1]) - mean(d$y[d$t == 0])\n\n\n[1] 1.537577\n\n\nUse lm() to predict the newly created y from t. What are the coefficient values?\n\n\nCode\nlm(y ~ t, data = d) |&gt; \n  broom::tidy(conf.int = TRUE) ## 95% confidence interval\n\n\n# A tibble: 2 × 7\n  term        estimate std.error statistic  p.value conf.low conf.high\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)     4.11     0.297     13.8  5.15e-40    3.52       4.69\n2 t               1.54     0.414      3.71 2.18e- 4    0.724      2.35\n\n\nUse lm() to predict the newly created y from t and x. What are the coefficient values?\n\n\nCode\nlm(y ~ t + x, data = d) |&gt; \n  broom::tidy(conf.int = TRUE) ## 95% confidence interval\n\n\n# A tibble: 3 × 7\n  term        estimate std.error statistic   p.value conf.low conf.high\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)    0.197     0.281     0.703 4.82e-  1   -0.354     0.748\n2 t              1.49      0.325     4.57  5.53e-  6    0.848     2.13 \n3 x              4.09      0.164    24.9   6.09e-107    3.77      4.41 \n\n\nNote that in both of these cases the estimate was correctly “identified” (from a causal inference perspective) but the second answer is closer to the “truth.” Look at the standard errors, they are smaller!",
    "crumbs": [
      "Solutions 3",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Solutions 3</span>"
    ]
  },
  {
    "objectID": "solutions04.html",
    "href": "solutions04.html",
    "title": "9  Solutions 4",
    "section": "",
    "text": "9.1 Exercise\nColliders\nFor this exercise I am going to ask you to create the following simulated dataset.\nCode\nN &lt;- 1e3\n\nd &lt;- tibble(\n  x = rnorm(N, 0, 1),\n  y = rnorm(N, 0, 1)\n)\nThe relationship between \\(x\\) and \\(y\\) should look something like this:\nCode\nd |&gt; \n  ggplot(aes(x, y)) + \n  geom_point(shape = 21, fill = \"grey90\") +\n  geom_ribbon(\n    stat = \"smooth\", method = \"lm\", color = \"black\", \n    alpha = 1/4, linetype = \"dashed\", linewidth = 1/2\n  ) +\n  geom_smooth(method = \"lm\", show.legend = FALSE, color = \"black\", linewidth = 1/2) +\n  coord_fixed(ratio = 1)\n\n\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'\nNow I am going to ask you to create an association between \\(x\\) and \\(y\\) via some form of collider bias.\n\\[\nx \\longrightarrow \\underbrace{\\text{z}}_{\\small {\\text{collider}}} \\longleftarrow y\n\\]\nYou are tasked to do this in four different ways.",
    "crumbs": [
      "Solutions 3",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Solutions 4</span>"
    ]
  },
  {
    "objectID": "solutions04.html#exercise",
    "href": "solutions04.html#exercise",
    "title": "9  Solutions 4",
    "section": "",
    "text": "9.1.1 Panel 1\n\n\nCode\nd &lt;- d |&gt; mutate(z = x &lt;= 0 & y &lt;= 0)\nd &lt;- d |&gt; mutate(z = ifelse(as.logical(z), \"yes\", \"no\"))\n\nd |&gt; \n  filter(z == \"no\") |&gt; \n  ggplot(aes(x, y)) + \n  geom_point() + \n  geom_point(\n    data = d |&gt; filter(z == \"yes\"),\n    color = \"grey90\", alpha = 1/2\n  ) +\n  geom_ribbon(\n    stat = \"smooth\", method = \"lm\", color = \"black\", \n    alpha = 1/4, linetype = \"dashed\", linewidth = 1/2\n  ) +\n  geom_smooth(method = \"lm\", show.legend = FALSE, color = \"red\", linewidth = 1/2) +\n  coord_fixed(ratio = 1) +\n  geom_abline(intercept = 0, slope = -1)\n\n\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nCode\n# ggsave(\"images/collider1.png\", device = \"png\", dpi = \"print\", width = 5, height = 5)\n\n\n\n\n9.1.2 Panel 2\n\n\nCode\nd &lt;- d |&gt; \n  mutate(z = x + y &lt;= 0) |&gt; \n  mutate(z = ifelse(as.logical(z), \"yes\", \"no\"))\n\nd |&gt; \n  filter(z == \"no\") |&gt; \n  ggplot(aes(x, y)) + \n  geom_point() + \n  geom_point(\n    data = d |&gt; filter(z == \"yes\"),\n    color = \"grey90\", alpha = 1/2\n  ) +\n  geom_ribbon(\n    stat = \"smooth\", method = \"lm\", color = \"black\", \n    alpha = 1/4, linetype = \"dashed\", linewidth = 1/2\n  ) +\n  geom_smooth(method = \"lm\", show.legend = FALSE, color = \"red\", linewidth = 1/2) +\n  coord_fixed(ratio = 1) +\n  geom_abline(intercept = 0, slope = -1)\n\n\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nCode\n# ggsave(\"images/collider2.png\", device = \"png\", dpi = \"print\", width = 5, height = 5)\n\n\n\n\n9.1.3 Panel 3\n\n\nCode\nd &lt;- d |&gt; \n  mutate(z = (x + y &gt;= -1.5) & (x + y &lt;= 1.5)) |&gt; \n  mutate(z = ifelse(as.logical(z), \"yes\", \"no\"))\n\nd |&gt; \n  filter(z == \"yes\") |&gt; \n  ggplot(aes(x, y)) + \n  geom_point() + \n  geom_point(\n    data = d |&gt; filter(z == \"no\"),\n    color = \"grey90\", alpha = 1/2\n  ) +\n  geom_ribbon(\n    stat = \"smooth\", method = \"lm\", color = \"black\", \n    alpha = 1/4, linetype = \"dashed\", linewidth = 1/2\n  ) +\n  geom_smooth(method = \"lm\", show.legend = FALSE, color = \"red\", linewidth = 1/2) +\n  coord_fixed(ratio = 1) +\n  geom_abline(intercept = c(-1.5, 1.5), slope = -1)\n\n\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nCode\n## ggsave(\"images/collider3.png\", device = \"png\", dpi = \"print\", width = 5, height = 5)\n\n\n\n\n9.1.4 Panel 4\n\n\nCode\nd &lt;- d |&gt; \n  mutate(p = plogis(2*x + 2*y)) |&gt; \n  mutate(z = rbinom(N, 1, p)) |&gt; \n  mutate(z = ifelse(as.logical(z), \"yes\", \"no\"))\n\nd |&gt; \n  ggplot(aes(x, y, fill = z)) + \n  geom_point(alpha = 3/4, shape = 21) + \n  geom_ribbon(\n    stat = \"smooth\", method = \"lm\", color = \"black\", \n    alpha = 1/4, linetype = \"dashed\", linewidth = 1/2, \n    show.legend = FALSE\n  ) +\n  geom_smooth(method = \"lm\", show.legend = FALSE, color = \"black\", linewidth = 1/2) +\n  coord_fixed(ratio = 1) +\n  scale_fill_manual(values = c(\"grey90\", \"black\")) +\n  guides(fill = guide_legend(override.aes = list(size = 3, alpha = 1))) +\n  theme(legend.position = c(.9, .9), legend.background = element_blank())\n\n\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'",
    "crumbs": [
      "Solutions 3",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Solutions 4</span>"
    ]
  },
  {
    "objectID": "solutions04.html#exercise-1",
    "href": "solutions04.html#exercise-1",
    "title": "9  Solutions 4",
    "section": "9.2 Exercise",
    "text": "9.2 Exercise\nConditional average treatment effect\nThe average treatment effect across all units that have certain values of \\(X\\)—i.e., we are conditioning on \\(X\\).\nAverage treatment on the treated\nThe average treatment effect among those who actually received the treatment. The effect of taking away the treatment.\n\\[\n\\text{ATT} = \\operatorname{E} \\big[ Y^1 - Y^0 \\mid T =1 \\big] = \\operatorname{E} \\big[ Y^1  \\mid T =1 \\big] - \\underbrace{\\operatorname{E} \\big[Y^0 \\mid T =1 \\big]}_\\text{unobservable}\n\\]\nAverage treatment on the untreated\nThe average treatment effect among those who did not actually receive the treatment. The effect of adding treatment.\n\\[\n\\text{ATT} = \\operatorname{E} \\big[ Y^1 - Y^0 \\mid T = 0 \\big] = \\underbrace{\\operatorname{E} \\big[ Y^1  \\mid T = 0 \\big]}_\\text{unobservable} - \\operatorname{E} \\big[Y^0 \\mid T = 0 \\big]\n\\]",
    "crumbs": [
      "Solutions 3",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Solutions 4</span>"
    ]
  },
  {
    "objectID": "solutions04.html#exercise-2",
    "href": "solutions04.html#exercise-2",
    "title": "9  Solutions 4",
    "section": "9.3 Exercise",
    "text": "9.3 Exercise\nProvide an example of a treatment effect that you would expect to be highly heterogeneous, and explain why you think it is likely to be heterogeneous.\n\nThe effect of money on happiness. The effect of earnings on happiness will vary (aka interact) across a wide range of things (e.g., neighborhood, job, and many, many others).\nThe effects of political cues on environmental behavior. We would expect that this effect is heterogeneous, as complying with these cues depends on one’s political awareness and motivation to comply.",
    "crumbs": [
      "Solutions 3",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Solutions 4</span>"
    ]
  },
  {
    "objectID": "solutions04.html#exercise-3",
    "href": "solutions04.html#exercise-3",
    "title": "9  Solutions 4",
    "section": "9.4 Exercise",
    "text": "9.4 Exercise\nConsider the data in the table below that shows the hypothetical treatment effect of cognitive behavioral therapy on depression for six participants. For the sake of this example, the six participants represent the population of interest.\n\n\n\nCase\nAge\nGender\nEffect\n\n\n\n\nA\n15\nMan\n7\n\n\nB\n40\nWoman\n3\n\n\nC\n30\nWoman\n7\n\n\nD\n20\nNon-binary\n8\n\n\nE\n15\nMan\n7\n\n\nF\n25\nWoman\n4\n\n\n\nWhat is the overall average treatment effect for the population?\n\n\nCode\nmean(c(7, 3, 7, 8, 7, 4))\n\n\n[1] 6\n\n\nWhat is the average treatment effect for Women?\n\n\nCode\nmean(c(3, 7, 4))\n\n\n[1] 4.666667\n\n\nIf nearly all Non-binary people get treated, and about half of all Women get treated, and we control for the differences between Women and Non-binary people, what kind of treatment effect average will we get, and what can we say about the numerical estimate we’ll get?\nNHK refers to variance-weighted ATE in the context of regression!\nIf we assume that, in the absence of treatment, everyone would have had the same outcome, and also only teenagers (19 or younger) ever receive treatment, and we compare treated people to control people, what kind of treatment effect average will we get, and what can we say about the numerical estimate we’ll get?\nWe could get a conditional ATE",
    "crumbs": [
      "Solutions 3",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Solutions 4</span>"
    ]
  },
  {
    "objectID": "solutions04.html#exercise-4",
    "href": "solutions04.html#exercise-4",
    "title": "9  Solutions 4",
    "section": "9.5 Exercise",
    "text": "9.5 Exercise\nGive an example where the average treatment effect on the treated would be more useful to consider than the overall average treatment effect, and explain why.\nJob training program. Typically people who don’t need it, don’t receive it. And we would never think about giving these to people who don’t need it. So, there’s no sense in estimating the ATE—e.g., what’s the point of estimating a millionaire’s counterfactual after said job training program (it’s also probably negative).\nIf we have reason to believe that the distribution of treatment effects is heterogenous, then we might think twice before trying to estimate the ATE. For example, if job trainings help poor people but hurt rich people, why would we use the ATE and average across both groups???",
    "crumbs": [
      "Solutions 3",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Solutions 4</span>"
    ]
  },
  {
    "objectID": "solutions04.html#exercise-5",
    "href": "solutions04.html#exercise-5",
    "title": "9  Solutions 4",
    "section": "9.6 Exercise",
    "text": "9.6 Exercise\nWhich of the following describes the average treatment effect of assigning treatment, whether or not treatment is actually received?\n\nLocal average treatment effect\nAverage treatment on the treated\nIntent-to-treat (this one)\nVariance-weighted average treatment effect",
    "crumbs": [
      "Solutions 3",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Solutions 4</span>"
    ]
  },
  {
    "objectID": "solutions04.html#exercise-6",
    "href": "solutions04.html#exercise-6",
    "title": "9  Solutions 4",
    "section": "9.7 Exercise",
    "text": "9.7 Exercise\nSuppose you are conducting an experiment to see whether pricing cookies at $1.99 versus $2 affects the decision to purchase the cookies. The population of interest is all adults in the United States. You recruit people from your university to participate and randomize them to either see cookies priced as $1.99 or $2, then write down whether they purchased cookies. What kind of average treatment effect can you identify from this experiment?\nThis is an ATE, conditional on people attending this particular university.\nNote. NHK will sometimes speak of a CATE.",
    "crumbs": [
      "Solutions 3",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Solutions 4</span>"
    ]
  },
  {
    "objectID": "solutions04.html#exercise-7",
    "href": "solutions04.html#exercise-7",
    "title": "9  Solutions 4",
    "section": "9.8 Exercise",
    "text": "9.8 Exercise\nFor each of the following identification strategies, what kind of treatment effect(s) is most likely to be identified?\nA randomized experiment using a representative sample\nATE\nTrue randomization within only a certain demographic group\nCATE\nClosing back door paths connected to variation in treatment\nVariance-Weighted ATE—i.e., ATE using Linear Regression.\nIsolating the part of the variation in treatment variable that is driven by an exogenous variable\nLATE (you should have been able to figure this out from the textbook!). It stands from local average treatment effect.\nThe control group is comparable to the treatment group, but treatment effects may be different across these groups\nBoth ATU and ATT, which should allow us to estimate the ATE.",
    "crumbs": [
      "Solutions 3",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Solutions 4</span>"
    ]
  },
  {
    "objectID": "solutions05.html",
    "href": "solutions05.html",
    "title": "10  Solutions 5",
    "section": "",
    "text": "10.1 Linear Regression\nWe will use this data.\nCode\ndols &lt;- gss2022 |&gt; \n  select(tvhours, degree, madeg, padeg) |&gt; \n  mutate(pardeg = pmax(madeg, padeg, na.rm = TRUE),\n         college = if_else(degree &gt;= 3, 1L, 0L),\n         parcol = if_else(pardeg &gt;= 3, 1L, 0L)) |&gt;\n  select(tvhours, college, parcol) |&gt; \n  drop_na()",
    "crumbs": [
      "Solutions 3",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Solutions 5</span>"
    ]
  },
  {
    "objectID": "solutions05.html#linear-regression",
    "href": "solutions05.html#linear-regression",
    "title": "10  Solutions 5",
    "section": "",
    "text": "10.1.1 Exercise\nAdditive link function, no interactions\n\n\nCode\nmod1 &lt;- glm(tvhours ~ college + parcol, data = dols)\n\n# ATE estimate\navg_slopes(mod1, variables = \"college\") |&gt; \n  tidy()\n\n\n# A tibble: 1 × 9\n  term  contrast estimate std.error statistic p.value s.value conf.low conf.high\n  &lt;chr&gt; &lt;chr&gt;       &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1 coll… 1 - 0      -0.796     0.151     -5.27 1.35e-7    22.8    -1.09    -0.500\n\n\nAnswer (using base R):\n\n\nCode\np0 &lt;- predict(mod1, newdata = dols |&gt; mutate(college = 0))\np1 &lt;- predict(mod1, newdata = dols |&gt; mutate(college = 1))\n\nmean(p1 - p0)\n\n\n[1] -0.7957246\n\n\nATT/ATU estimate\n\n\nCode\navg_slopes(\n  model = mod1, \n  variables = \"college\",\n  by = \"college\" # separately by treatment group\n) |&gt; \n  tidy()\n\n\n# A tibble: 2 × 13\n  term    contrast college estimate std.error statistic p.value s.value conf.low\n  &lt;chr&gt;   &lt;chr&gt;      &lt;int&gt;    &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;\n1 college mean(1)…       0   -0.796     0.151     -5.27 1.35e-7    22.8    -1.09\n2 college mean(1)…       1   -0.796     0.151     -5.27 1.35e-7    22.8    -1.09\n# ℹ 4 more variables: conf.high &lt;dbl&gt;, predicted_lo &lt;dbl&gt;, predicted_hi &lt;dbl&gt;,\n#   predicted &lt;dbl&gt;\n\n\nAnswer (using using base R):\nNote. You should be able to know that the answer will be exactly the same because of the model being used.\n\n\nCode\ndols$p0 &lt;- predict(mod1, newdata = dols |&gt; mutate(college = 0))\ndols$p1 &lt;- predict(mod1, newdata = dols |&gt; mutate(college = 1))\n\ndols |&gt; \n  mutate(estimate = p1 - p0) |&gt; \n  group_by(college) |&gt; \n  summarize(estimate = mean(estimate))\n\n\n# A tibble: 2 × 2\n  college estimate\n    &lt;int&gt;    &lt;dbl&gt;\n1       0   -0.796\n2       1   -0.796\n\n\n\n\n10.1.2 Exercise\nAdditive link function, with interactions\n\n\nCode\nmod2 &lt;- lm(tvhours ~ college * parcol, data = dols)\n\n# ATE estimate\navg_slopes(mod2, variables = \"college\") |&gt; \n  tidy()\n\n\n# A tibble: 1 × 9\n  term  contrast estimate std.error statistic p.value s.value conf.low conf.high\n  &lt;chr&gt; &lt;chr&gt;       &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1 coll… 1 - 0      -0.803     0.152     -5.29 1.20e-7    23.0    -1.10    -0.506\n\n\nAnswer (using matrix algebra):\n\n\nCode\nbeta &lt;- coefficients(mod2) |&gt; as.matrix(ncol = 1) ## ensures this is a column vector\n\nM0 &lt;- model.matrix(tvhours ~ college * parcol, data = dols |&gt; mutate(college = 0))\npred0 &lt;- M0 %*% beta\n\nM1 &lt;- model.matrix(tvhours ~ college * parcol, data = dols |&gt; mutate(college = 1))\npred1 &lt;- M1 %*% beta\n\nmean(pred1 - pred0)\n\n\n[1] -0.8033206\n\n\nATT/ATU estimate\n\n\nCode\navg_slopes(\n  model = mod2, \n  variables = \"college\",\n  by = \"college\" # separately by treatment group\n) |&gt; \n  tidy()\n\n\n# A tibble: 2 × 13\n  term    contrast college estimate std.error statistic p.value s.value conf.low\n  &lt;chr&gt;   &lt;chr&gt;      &lt;int&gt;    &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;\n1 college mean(1)…       0   -0.821     0.160     -5.14 2.70e-7    21.8    -1.13\n2 college mean(1)…       1   -0.772     0.159     -4.87 1.13e-6    19.8    -1.08\n# ℹ 4 more variables: conf.high &lt;dbl&gt;, predicted_lo &lt;dbl&gt;, predicted_hi &lt;dbl&gt;,\n#   predicted &lt;dbl&gt;\n\n\nAnswer (using previous calculations)\n\n\nCode\ndols$p1 &lt;- pred1\ndols$p0 &lt;- pred0\n\ndols |&gt; \n  group_by(college) |&gt; \n  summarize(estimate = mean(p1 - p0))\n\n\n# A tibble: 2 × 2\n  college estimate\n    &lt;int&gt;    &lt;dbl&gt;\n1       0   -0.821\n2       1   -0.772",
    "crumbs": [
      "Solutions 3",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Solutions 5</span>"
    ]
  },
  {
    "objectID": "solutions05.html#poisson-regression",
    "href": "solutions05.html#poisson-regression",
    "title": "10  Solutions 5",
    "section": "10.2 Poisson Regression",
    "text": "10.2 Poisson Regression\nWe will use this data.\n\n\nCode\ndqp &lt;- gss2022 |&gt;\n  filter(wrkstat == 1) |&gt; # full time workers\n  select(realrinc, degree, madeg, padeg, sex, age) |&gt; \n  mutate(pardeg = pmax(madeg, padeg, na.rm = TRUE),\n         college = if_else(degree &gt;= 3, 1L, 0L),\n         parcol = if_else(pardeg &gt;= 3, 1L, 0L),\n         female = if_else(sex == 2, 1L, 0L),\n         realrinc = floor(realrinc)) |&gt;             # integer\n  select(realrinc, college, parcol, female, age) |&gt; \n  drop_na()\n\n\n\n10.2.1 Exercise\nUsing the log-counts, no interactions\n\n\nCode\nqp1 &lt;- glm(realrinc ~ college + (parcol + female + age + I(age^2)), \n           data = dqp,\n           family = \"quasipoisson\")\n\navg_slopes(qp1,\n           variables = \"college\",\n           type = \"link\") |&gt; \n  tidy()\n\n\n# A tibble: 1 × 9\n  term    contrast estimate std.error statistic  p.value s.value conf.low\n  &lt;chr&gt;   &lt;chr&gt;       &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;\n1 college 1 - 0       0.599    0.0510      11.7 7.45e-32    103.    0.499\n# ℹ 1 more variable: conf.high &lt;dbl&gt;\n\n\nAnswer:\n\n\nCode\npred0 &lt;- predict(qp1, newdata = mutate(dqp, college = 0), type = \"link\")\npred1 &lt;- predict(qp1, newdata = mutate(dqp, college = 1), type = \"link\")\n\nmean(pred1 - pred0)\n\n\n[1] 0.5994521\n\n\nATU/ATT\n\n\nCode\navg_slopes(qp1,\n           variables = \"college\",\n           type = \"link\",\n           by = \"college\") |&gt; # separately by treatment group\n  tidy()\n\n\n# A tibble: 2 × 13\n  term   contrast college estimate std.error statistic  p.value s.value conf.low\n  &lt;chr&gt;  &lt;chr&gt;      &lt;int&gt;    &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;\n1 colle… mean(1)…       0    0.599    0.0510      11.7 7.45e-32    103.    0.499\n2 colle… mean(1)…       1    0.599    0.0510      11.7 7.45e-32    103.    0.499\n# ℹ 4 more variables: conf.high &lt;dbl&gt;, predicted_lo &lt;dbl&gt;, predicted_hi &lt;dbl&gt;,\n#   predicted &lt;dbl&gt;\n\n\nAnswer:\n\n\nCode\ndqp$pred0 &lt;- pred0\ndqp$pred1 &lt;- pred1\n\ndqp |&gt; \n  group_by(college) |&gt; \n  summarize(estimate = mean(pred1 - pred0))\n\n\n# A tibble: 2 × 2\n  college estimate\n    &lt;int&gt;    &lt;dbl&gt;\n1       0    0.599\n2       1    0.599\n\n\n\n\n10.2.2 Exercise\nNon-linear response, no interactions\n\n\nCode\navg_slopes(qp1,\n           variables = \"college\",\n           type = \"response\") |&gt; \n  tidy()\n\n\n# A tibble: 1 × 9\n  term    contrast estimate std.error statistic  p.value s.value conf.low\n  &lt;chr&gt;   &lt;chr&gt;       &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;\n1 college 1 - 0      21237.     1831.      11.6 4.18e-31    101.   17649.\n# ℹ 1 more variable: conf.high &lt;dbl&gt;\n\n\nAnswer:\n\n\nCode\n## use appropriate link function for Poisson\ndqp$pred0 &lt;- exp(pred0)\ndqp$pred1 &lt;- exp(pred1)\n\ndqp |&gt; \n  summarize(ate = mean(pred1 - pred0))\n\n\n# A tibble: 1 × 1\n     ate\n   &lt;dbl&gt;\n1 21237.\n\n\nATT/ ATU\n\n\nCode\navg_slopes(qp1,\n           variables = \"college\",\n           type = \"response\",\n           by = \"college\") |&gt; # separately by treatment group\n  tidy()\n\n\n# A tibble: 2 × 13\n  term   contrast college estimate std.error statistic  p.value s.value conf.low\n  &lt;chr&gt;  &lt;chr&gt;      &lt;int&gt;    &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;\n1 colle… mean(1)…       0   20636.     1861.      11.1 1.41e-28    92.5   16988.\n2 colle… mean(1)…       1   21977.     1816.      12.1 1.05e-33   110.    18417.\n# ℹ 4 more variables: conf.high &lt;dbl&gt;, predicted_lo &lt;dbl&gt;, predicted_hi &lt;dbl&gt;,\n#   predicted &lt;dbl&gt;\n\n\nAnswer:\n\n\nCode\ndqp |&gt; \n  group_by(college) |&gt; \n  summarize(ate = mean(pred1 - pred0))\n\n\n# A tibble: 2 × 2\n  college    ate\n    &lt;int&gt;  &lt;dbl&gt;\n1       0 20636.\n2       1 21977.\n\n\n\n\n10.2.3 Exercise\nUsing the log-counts, with interactions\n\n\nCode\nqp2 &lt;- glm(realrinc ~ college * (parcol + female + age + I(age^2)), \n           data = dqp,\n           family = \"quasipoisson\")\n\navg_slopes(qp2,\n           variables = \"college\",\n           type = \"link\") |&gt; \n  tidy()\n\n\n# A tibble: 1 × 9\n  term    contrast estimate std.error statistic  p.value s.value conf.low\n  &lt;chr&gt;   &lt;chr&gt;       &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;\n1 college 1 - 0       0.580    0.0543      10.7 1.20e-26    86.1    0.474\n# ℹ 1 more variable: conf.high &lt;dbl&gt;\n\n\nAnswer:\n\n\nCode\ndqp$pred0 &lt;- predict(qp2, newdata = mutate(dqp, college = 0), type = \"link\")\ndqp$pred1 &lt;- predict(qp2, newdata = mutate(dqp, college = 1), type = \"link\")\n\ndqp |&gt; \n  summarize(estimate = mean(pred1 - pred0))\n\n\n# A tibble: 1 × 1\n  estimate\n     &lt;dbl&gt;\n1    0.580\n\n\n\n\nCode\navg_slopes(qp2,\n           variables = \"college\",\n           type = \"link\",\n           by = \"college\") |&gt; # separately by treatment group\n  tidy()\n\n\n# A tibble: 2 × 13\n  term   contrast college estimate std.error statistic  p.value s.value conf.low\n  &lt;chr&gt;  &lt;chr&gt;      &lt;int&gt;    &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;\n1 colle… mean(1)…       0    0.567    0.0571      9.94 2.77e-23    74.9    0.455\n2 colle… mean(1)…       1    0.596    0.0600      9.94 2.87e-23    74.9    0.479\n# ℹ 4 more variables: conf.high &lt;dbl&gt;, predicted_lo &lt;dbl&gt;, predicted_hi &lt;dbl&gt;,\n#   predicted &lt;dbl&gt;\n\n\n\n\nCode\ndqp |&gt; \n  group_by(college) |&gt; \n  summarize(estimate = mean(pred1 - pred0))\n\n\n# A tibble: 2 × 2\n  college estimate\n    &lt;int&gt;    &lt;dbl&gt;\n1       0    0.567\n2       1    0.596\n\n\n\n\n10.2.4 Exercise\nNon-linear response, with interactions\n\n\nCode\navg_slopes(qp2,\n           variables = \"college\",\n           type = \"response\") |&gt; \n  tidy()\n\n\n# A tibble: 1 × 9\n  term    contrast estimate std.error statistic  p.value s.value conf.low\n  &lt;chr&gt;   &lt;chr&gt;       &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;\n1 college 1 - 0      21190.     1817.      11.7 1.99e-31    102.   17629.\n# ℹ 1 more variable: conf.high &lt;dbl&gt;\n\n\nAnswer:\n\n\nCode\ndqp$pred0 &lt;- predict(qp2, newdata = mutate(dqp, college = 0), type = \"response\")\ndqp$pred1 &lt;- predict(qp2, newdata = mutate(dqp, college = 1), type = \"response\")\n\ndqp |&gt; \n  summarize(estimate = mean(pred1 - pred0))\n\n\n# A tibble: 1 × 1\n  estimate\n     &lt;dbl&gt;\n1   21190.\n\n\n\n\nCode\navg_slopes(qp2,\n           variables = \"college\",\n           type = \"response\",\n           by = \"college\") |&gt; # separately by treatment group\n  tidy()\n\n\n# A tibble: 2 × 13\n  term   contrast college estimate std.error statistic  p.value s.value conf.low\n  &lt;chr&gt;  &lt;chr&gt;      &lt;int&gt;    &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;\n1 colle… mean(1)…       0   20196.     1937.      10.4 1.90e-25    82.1   16400.\n2 colle… mean(1)…       1   22411.     1963.      11.4 3.51e-30    97.8   18563.\n# ℹ 4 more variables: conf.high &lt;dbl&gt;, predicted_lo &lt;dbl&gt;, predicted_hi &lt;dbl&gt;,\n#   predicted &lt;dbl&gt;\n\n\nAnswer:\n\n\nCode\ndqp |&gt; \n  group_by(college) |&gt; \n  summarize(estimate = mean(pred1 - pred0))\n\n\n# A tibble: 2 × 2\n  college estimate\n    &lt;int&gt;    &lt;dbl&gt;\n1       0   20196.\n2       1   22411.",
    "crumbs": [
      "Solutions 3",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Solutions 5</span>"
    ]
  },
  {
    "objectID": "solutions05.html#logistic-regression",
    "href": "solutions05.html#logistic-regression",
    "title": "10  Solutions 5",
    "section": "10.3 Logistic Regression",
    "text": "10.3 Logistic Regression\nWe will use this data.\n\n\nCode\ndlr &lt;- gss2022 |&gt;\n  select(abany, degree, madeg, padeg, sex, age) |&gt; \n  mutate(pardeg = pmax(madeg, padeg, na.rm = TRUE),\n         college = if_else(degree &gt;= 3, 1L, 0L),\n         parcol = if_else(pardeg &gt;= 3, 1L, 0L),\n         female = if_else(sex == 2, 1L, 0L),\n         abany = if_else(abany == 1, 1L, 0L)) |&gt;\n  select(abany, college, parcol, female, age) |&gt; \n  drop_na()\n\n\n\n10.3.1 Exercise\nUsing log-odds, no interactions\n\n\nCode\nlr1 &lt;- glm(abany ~ college + (parcol + female + age + I(age^2)),\n          data = dlr,\n          family = binomial)\n\n# ATE estimate\navg_slopes(lr1,\n           variables = \"college\",\n           type = \"link\") |&gt; \n  tidy()\n\n\n# A tibble: 1 × 9\n  term  contrast estimate std.error statistic p.value s.value conf.low conf.high\n  &lt;chr&gt; &lt;chr&gt;       &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1 coll… 1 - 0       0.438     0.146      3.00 0.00273    8.52    0.151     0.724\n\n\nAnswer:\n\n\nCode\ndlr$pred0 &lt;- predict(lr1, newdata = dlr |&gt; mutate(college = 0), type = \"link\")\ndlr$pred1 &lt;- predict(lr1, newdata = dlr |&gt; mutate(college = 1), type = \"link\")\n\ndlr |&gt; \n  summarize(estimate = mean(pred1 - pred0))\n\n\n# A tibble: 1 × 1\n  estimate\n     &lt;dbl&gt;\n1    0.438\n\n\nATT/ATU\n\n\nCode\navg_slopes(lr1,\n           variables = \"college\",\n           by = \"college\",\n           type = \"link\") |&gt; \n  tidy()\n\n\n# A tibble: 2 × 13\n  term    contrast college estimate std.error statistic p.value s.value conf.low\n  &lt;chr&gt;   &lt;chr&gt;      &lt;int&gt;    &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;\n1 college mean(1)…       0    0.438     0.146      3.00 0.00273    8.52    0.151\n2 college mean(1)…       1    0.438     0.146      3.00 0.00273    8.52    0.151\n# ℹ 4 more variables: conf.high &lt;dbl&gt;, predicted_lo &lt;dbl&gt;, predicted_hi &lt;dbl&gt;,\n#   predicted &lt;dbl&gt;\n\n\n\n\nCode\ndlr |&gt; \n  group_by(college) |&gt; \n  summarize(estimate = mean(pred1 - pred0))\n\n\n# A tibble: 2 × 2\n  college estimate\n    &lt;int&gt;    &lt;dbl&gt;\n1       0    0.438\n2       1    0.438\n\n\n\n\n10.3.2 Exercise\nUsing non-linear response (aka probabilities), no interactions\n\n\nCode\n# ATE estimate\navg_slopes(lr1,\n           variables = \"college\",\n           type = \"response\") |&gt; \n  tidy()\n\n\n# A tibble: 1 × 9\n  term  contrast estimate std.error statistic p.value s.value conf.low conf.high\n  &lt;chr&gt; &lt;chr&gt;       &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1 coll… 1 - 0       0.102    0.0337      3.02 0.00249    8.65   0.0359     0.168\n\n\nAnswer:\n\n\nCode\ndlr |&gt; \n  mutate(across(starts_with(\"pred\"), plogis)) |&gt; \n  summarize(estimate = mean(pred1 - pred0))\n\n\n# A tibble: 1 × 1\n  estimate\n     &lt;dbl&gt;\n1    0.102\n\n\nATT/ATU\n\n\nCode\navg_slopes(lr1,\n           variables = \"college\",\n           by = \"college\",\n           type = \"response\") |&gt; \n  tidy()\n\n\n# A tibble: 2 × 13\n  term    contrast college estimate std.error statistic p.value s.value conf.low\n  &lt;chr&gt;   &lt;chr&gt;      &lt;int&gt;    &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;\n1 college mean(1)…       0   0.104     0.0341      3.04 0.00235    8.73   0.0369\n2 college mean(1)…       1   0.0989    0.0330      2.99 0.00275    8.51   0.0342\n# ℹ 4 more variables: conf.high &lt;dbl&gt;, predicted_lo &lt;dbl&gt;, predicted_hi &lt;dbl&gt;,\n#   predicted &lt;dbl&gt;\n\n\nAnswer:\n\n\nCode\ndlr |&gt; \n  mutate(across(starts_with(\"pred\"), plogis)) |&gt; \n  group_by(college) |&gt; \n  summarize(estimate = mean(pred1 - pred0))\n\n\n# A tibble: 2 × 2\n  college estimate\n    &lt;int&gt;    &lt;dbl&gt;\n1       0   0.104 \n2       1   0.0989\n\n\n\n\n10.3.3 Exercise\nUsing log-odds, with interactions\n\n\nCode\nlr2 &lt;- glm(abany ~ college * (parcol + female + age + I(age^2)),\n          data = dlr,\n          family = binomial)\n\n# ATE estimate\navg_slopes(lr2,\n           variables = \"college\",\n           type = \"link\") |&gt; \n  tidy()\n\n\n# A tibble: 1 × 9\n  term  contrast estimate std.error statistic p.value s.value conf.low conf.high\n  &lt;chr&gt; &lt;chr&gt;       &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1 coll… 1 - 0       0.453     0.149      3.04 0.00239    8.71    0.161     0.745\n\n\nAnswer:\n\n\nCode\ndlr$pred0 &lt;- predict(lr2, newdata = dlr |&gt; mutate(college = 0), type = \"link\")\ndlr$pred1 &lt;- predict(lr2, newdata = dlr |&gt; mutate(college = 1), type = \"link\")\n\ndlr |&gt; \n  summarize(estimate = mean(pred1 - pred0))\n\n\n# A tibble: 1 × 1\n  estimate\n     &lt;dbl&gt;\n1    0.453\n\n\nATT/ATU:\n\n\nCode\navg_slopes(lr2,\n           variables = \"college\",\n           by = \"college\",\n           type = \"link\") |&gt; \n  tidy()\n\n\n# A tibble: 2 × 13\n  term    contrast college estimate std.error statistic p.value s.value conf.low\n  &lt;chr&gt;   &lt;chr&gt;      &lt;int&gt;    &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;\n1 college mean(1)…       0    0.450     0.159      2.83 0.00460    7.76    0.139\n2 college mean(1)…       1    0.458     0.161      2.85 0.00435    7.84    0.143\n# ℹ 4 more variables: conf.high &lt;dbl&gt;, predicted_lo &lt;dbl&gt;, predicted_hi &lt;dbl&gt;,\n#   predicted &lt;dbl&gt;\n\n\nAnswer:\n\n\nCode\ndlr |&gt; \n  group_by(college) |&gt; \n  summarize(estimate = mean(pred1 - pred0))\n\n\n# A tibble: 2 × 2\n  college estimate\n    &lt;int&gt;    &lt;dbl&gt;\n1       0    0.450\n2       1    0.458\n\n\n\n\n10.3.4 Exercise\nUsing non-linear response (aka probabilities), with interactions\n\n\nCode\n# ATE estimate\navg_slopes(lr2,\n           variables = \"college\",\n           type = \"response\") |&gt; \n  tidy()\n\n\n# A tibble: 1 × 9\n  term  contrast estimate std.error statistic p.value s.value conf.low conf.high\n  &lt;chr&gt; &lt;chr&gt;       &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1 coll… 1 - 0       0.103    0.0338      3.05 0.00228    8.78   0.0369     0.169\n\n\nAnswer:\n\n\nCode\ndlr$pred0 &lt;- predict(lr2, newdata = dlr |&gt; mutate(college = 0), type = \"response\")\ndlr$pred1 &lt;- predict(lr2, newdata = dlr |&gt; mutate(college = 1), type = \"response\")\n\ndlr |&gt; \n  summarize(estimate = mean(pred1 - pred0))\n\n\n# A tibble: 1 × 1\n  estimate\n     &lt;dbl&gt;\n1    0.103\n\n\nATT/ATU\n\n\nCode\navg_slopes(lr2,\n           variables = \"college\",\n           by = \"college\",\n           type = \"response\") |&gt; \n  tidy()\n\n\n# A tibble: 2 × 13\n  term    contrast college estimate std.error statistic p.value s.value conf.low\n  &lt;chr&gt;   &lt;chr&gt;      &lt;int&gt;    &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;\n1 college mean(1)…       0    0.104    0.0363      2.87 0.00407    7.94   0.0332\n2 college mean(1)…       1    0.101    0.0353      2.87 0.00415    7.91   0.0320\n# ℹ 4 more variables: conf.high &lt;dbl&gt;, predicted_lo &lt;dbl&gt;, predicted_hi &lt;dbl&gt;,\n#   predicted &lt;dbl&gt;\n\n\nAnswer:\n\n\nCode\ndlr |&gt; \n  group_by(college) |&gt; \n  summarize(estimate = mean(pred1 - pred0))\n\n\n# A tibble: 2 × 2\n  college estimate\n    &lt;int&gt;    &lt;dbl&gt;\n1       0    0.104\n2       1    0.101",
    "crumbs": [
      "Solutions 3",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Solutions 5</span>"
    ]
  },
  {
    "objectID": "solutions05.html#standard-errors",
    "href": "solutions05.html#standard-errors",
    "title": "10  Solutions 5",
    "section": "10.4 Standard Errors",
    "text": "10.4 Standard Errors\nSo far, we haven’t calculated the standard errors at all. And yet marginaleffects does this seamlessly.\nHere is a potential bootstrap implementation. To make things easier, I will wrap up the calculation of the ATE in a function.\nNote. This code is inefficient and will only work with “glm” objects.\n\n\nCode\nbootstrap_ate &lt;- function(obj, variable, type = c(\"link\", \"response\"), S = 1e3) {\n  \n  type &lt;- match.arg(type)\n  data &lt;- obj$data\n  data0 &lt;- mutate(data, {{variable}} := 0)\n  data1 &lt;- mutate(data, {{variable}} := 1)\n  \n  replicate(S, {\n    i &lt;- sample(nrow(data), replace = TRUE)\n    nobj &lt;- glm(obj$formula, data = data[i, ], family = obj$family)\n    pred0 &lt;- predict(nobj, newdata = data0[i, ], type = type)\n    pred1 &lt;- predict(nobj, newdata = data1[i, ], type = type)\n    mean(pred1 - pred0)\n  })\n}\n\n\nLogistic Regression using link function:\n\n\nCode\navg_slopes(lr2, variables = \"college\", type = \"link\")\n\n\n\n    Term Contrast Estimate Std. Error    z Pr(&gt;|z|)   S 2.5 % 97.5 %\n college    1 - 0    0.453      0.149 3.04  0.00239 8.7 0.161  0.745\n\nColumns: term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \nType:  link \n\n\nCode\nout &lt;- bootstrap_ate(lr2, college, type = \"link\")\nc(\"estimate\" = mean(out), \"std. error\" = sd(out))\n\n\n  estimate std. error \n 0.4562075  0.1522344 \n\n\nLogistic Regression using response function:\n\n\nCode\navg_slopes(lr2, variables = \"college\", type = \"response\")\n\n\n\n    Term Contrast Estimate Std. Error    z Pr(&gt;|z|)   S  2.5 % 97.5 %\n college    1 - 0    0.103     0.0338 3.05  0.00228 8.8 0.0369  0.169\n\nColumns: term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \nType:  response \n\n\nCode\nout &lt;- bootstrap_ate(lr2, college, type = \"response\")\nc(\"estimate\" = mean(out), \"std. error\" = sd(out))\n\n\n  estimate std. error \n0.10400151 0.03336594 \n\n\nPoisson Regression using link function:\n\n\nCode\navg_slopes(qp1, variables = \"college\", type = \"link\")\n\n\n\n    Term Contrast Estimate Std. Error    z Pr(&gt;|z|)     S 2.5 % 97.5 %\n college    1 - 0    0.599      0.051 11.7   &lt;0.001 103.4 0.499  0.699\n\nColumns: term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \nType:  link \n\n\nCode\nout &lt;- bootstrap_ate(qp1, college, type = \"link\")\nc(\"estimate\" = mean(out), \"std. error\" = sd(out))\n\n\n  estimate std. error \n0.60034626 0.05221928 \n\n\nPoisson Regression using response function:\n\n\nCode\navg_slopes(qp2, variables = \"college\", type = \"response\")\n\n\n\n    Term Contrast Estimate Std. Error    z Pr(&gt;|z|)     S 2.5 % 97.5 %\n college    1 - 0    21190       1817 11.7   &lt;0.001 102.0 17629  24751\n\nColumns: term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \nType:  response \n\n\nCode\nout &lt;- bootstrap_ate(qp2, college, type = \"response\")\nc(\"estimate\" = mean(out), \"std. error\" = sd(out))\n\n\n  estimate std. error \n 21155.944   1968.515",
    "crumbs": [
      "Solutions 3",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Solutions 5</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Angrist, Joshua D., and Jörn-Steffen Pischke. 2009. Mostly Harmless\nEconometrics. Princeton university press.\n\n\nAshworth, Scott, Christopher R. Berry, and Ethan Bueno de Mesquita.\n2021. Theory and Credibility: Integrating Theoretical and Empirical\nSocial Science. Princeton University Press.\n\n\nCinelli, Carlos, Andrew Forney, and Judea Pearl. 2022. “A Crash Course in Good\nand Bad Controls.” Sociological Methods &\nResearch 00491241221099552.\n\n\nCunningham, Scott. 2021. Causal Inference. Yale University\nPress.\n\n\nGelman, Andrew, Jennifer Hill, and Aki Vehtari. 2020. Regression and\nOther Stories. Cambridge University Press.\n\n\nHernan, Miquel A., and James M. Robins. 2023. Causal Inference: What\nIf. CRC Press.\n\n\nHolland, Paul W. 1986. “Statistics and Causal\nInference.” Journal of the American Statistical\nAssociation 81(396): 945–60.\n\n\nHünermund, Paul, and Beyers Louw. 2023. “On the Nuisance of\nControl Variables in Causal Regression Analysis.”\nOrganizational Research Methods 10944281231219274.\n\n\nHuntington-Klein, Nick. 2021. The Effect: An Introduction\nto Research Design and Causality. New York: Chapman; Hall/CRC.\n\n\nKeele, Luke, Randolph T. Stevenson, and Felix Elwert. 2020. “The\nCausal Interpretation of Estimated Associations in Regression\nModels.” Political Science Research and Methods 8(1):\n113.\n\n\nLundberg, Ian, Rebecca Johnson, and Brandon M. Stewart. 2021. “What Is Your Estimand?\nDefining the Target Quantity Connects Statistical Evidence to\nTheory.” American Sociological Review\n00031224211004187.\n\n\nMorgan, Stephen L., and Christopher Winship. 2014. Counterfactuals\nand Causal Inference: Methods and Principles for Social Research.\n2nd edition. 2nd edition. New York, NY: Cambridge University Press.\n\n\nPearl, Judea. 2009. Causality: Models, Reasoning and Inference.\n2nd edition. 2nd edition. Cambridge, U.K. ; New York: Cambridge\nUniversity Press.\n\n\nRohrer, Julia M. 2018. “Thinking Clearly about Correlations and\nCausation: Graphical Causal Models for Observational Data.”\nAdvances in Methods and Practices in Psychological Science\n1(1): 2742.\n\n\nSen, Maya, and Omar Wasow. 2016. “Race as a\nBundle of Sticks: Designs That Estimate Effects of Seemingly Immutable\nCharacteristics.” Annual Review of Political Science\n19(1): 499–522.\n\n\nWestreich, Daniel, and Sander Greenland. 2013. “The Table 2 Fallacy:\nPresenting and Interpreting Confounder and Modifier\nCoefficients.” American Journal of Epidemiology\n177(4): 292–98.",
    "crumbs": [
      "References"
    ]
  }
]