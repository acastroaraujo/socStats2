[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Social Statistics II (Exercises)",
    "section": "",
    "text": "Preface\nSyllabus\nHi everyone, I will be uploading the homework questions to this website.\nFeel free to reach out to me with any questions.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#resources",
    "href": "index.html#resources",
    "title": "Social Statistics II (Exercises)",
    "section": "Resources",
    "text": "Resources\nThese are my personal recommendations for resources to start getting interested with causal inference.\nClass Resources:\n\nThe Effect by NHK (Huntington-Klein 2021)\n\nShort:\n\nResearch questions and estimands (Lundberg et al. 2021)\nIntroduction to DAGs (Rohrer 2018; Cinelli et al. 2022)\nThe Causal Diagrams edX course by Miguel Hernán is pretty good too.\nDon’t interpret “control” or “adjustment” variables (Westreich and Greenland 2013; Keele et al. 2020; Hünermund and Louw 2023).\nThe effects of seemingly immutable characteristics (Sen and Wasow 2016; cf. Holland 1986).\n\nGood to play around with:\n\nCounterfactuals and Causal Inference (Morgan and Winship 2014)\nI’ve been told many times that the first edition is better, so maybe try that one instead.\nCausal Inference: The Mixtape (Cunningham 2021)\nCovers a lot of the same ground as NHK, but has more math. It is also written by an economist.\nRegression and Other Stories (Gelman et al. 2020)\nChapters 18-21\nTheory & Credibility (Ashworth et al. 2021)\n\nLongitudinal Data Analysis\n\nRohrer and Murayama (2023)\n\n\n\nDifference-in-differences: Callaway and Sant’Anna (2021), Goodman-Bacon (2021)\n\nAdvanced Resources:\n\nMostly Harmless Econometrics (Angrist and Pischke 2009)\nCausality: Models, Reasoning and Inference (Pearl 2009)\nCausal Inference: What If (Hernan and Robins 2023)\nThis book gets used a lot in epidemiology.\n\n\n\n\n\n\n\nAngrist, Joshua D., and Jörn-Steffen Pischke. 2009. Mostly Harmless Econometrics. Princeton university press.\n\n\nAshworth, Scott, Christopher R. Berry, and Ethan Bueno de Mesquita. 2021. Theory and Credibility: Integrating Theoretical and Empirical Social Science. Princeton University Press.\n\n\nCallaway, Brantly, and Pedro H. C. Sant’Anna. 2021. “Difference-in-Differences with Multiple Time Periods.” Journal of Econometrics 225(2): 200–230.\n\n\nCinelli, Carlos, Andrew Forney, and Judea Pearl. 2022. “A Crash Course in Good and Bad Controls.” Sociological Methods & Research 00491241221099552.\n\n\nCunningham, Scott. 2021. Causal Inference. Yale University Press.\n\n\nGelman, Andrew, Jennifer Hill, and Aki Vehtari. 2020. Regression and Other Stories. Cambridge University Press.\n\n\nGoodman-Bacon, Andrew. 2021. “Difference-in-Differences with Variation in Treatment Timing.” Journal of Econometrics 225(2): 254277.\n\n\nHernan, Miquel A., and James M. Robins. 2023. Causal Inference: What If. CRC Press.\n\n\nHolland, Paul W. 1986. “Statistics and Causal Inference.” Journal of the American Statistical Association 81(396): 945–60.\n\n\nHünermund, Paul, and Beyers Louw. 2023. “On the Nuisance of Control Variables in Causal Regression Analysis.” Organizational Research Methods 10944281231219274.\n\n\nHuntington-Klein, Nick. 2021. The Effect: An Introduction to Research Design and Causality. New York: Chapman; Hall/CRC.\n\n\nKeele, Luke, Randolph T. Stevenson, and Felix Elwert. 2020. “The Causal Interpretation of Estimated Associations in Regression Models.” Political Science Research and Methods 8(1): 113.\n\n\nLundberg, Ian, Rebecca Johnson, and Brandon M. Stewart. 2021. “What Is Your Estimand? Defining the Target Quantity Connects Statistical Evidence to Theory.” American Sociological Review 00031224211004187.\n\n\nMorgan, Stephen L., and Christopher Winship. 2014. Counterfactuals and Causal Inference: Methods and Principles for Social Research. 2nd edition. 2nd edition. New York, NY: Cambridge University Press.\n\n\nPearl, Judea. 2009. Causality: Models, Reasoning and Inference. 2nd edition. 2nd edition. Cambridge, U.K. ; New York: Cambridge University Press.\n\n\nRohrer, Julia M. 2018. “Thinking Clearly about Correlations and Causation: Graphical Causal Models for Observational Data.” Advances in Methods and Practices in Psychological Science 1(1): 2742.\n\n\nRohrer, Julia M., and Kou Murayama. 2023. “These Are Not the Effects You Are Looking for: Causality and the Within-/Between-Persons Distinction in Longitudinal Data Analysis.” Advances in Methods and Practices in Psychological Science 6(1): 25152459221140842.\n\n\nSen, Maya, and Omar Wasow. 2016. “Race as a Bundle of Sticks: Designs That Estimate Effects of Seemingly Immutable Characteristics.” Annual Review of Political Science 19(1): 499–522.\n\n\nWestreich, Daniel, and Sander Greenland. 2013. “The Table 2 Fallacy: Presenting and Interpreting Confounder and Modifier Coefficients.” American Journal of Epidemiology 177(4): 292–98.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "sec01.html",
    "href": "sec01.html",
    "title": "Part 1",
    "section": "",
    "text": "…",
    "crumbs": [
      "Part 1"
    ]
  },
  {
    "objectID": "week01.html",
    "href": "week01.html",
    "title": "1  Week 1",
    "section": "",
    "text": "1.1 Exercise\nWe will be digesting this article more fully in the coming weeks, so don’t worry if you don’t understand everything yet.",
    "crumbs": [
      "Part 1",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Week 1</span>"
    ]
  },
  {
    "objectID": "week01.html#sec-glossary",
    "href": "week01.html#sec-glossary",
    "title": "1  Week 1",
    "section": "",
    "text": "Start reading What Is Your Estimand? Defining the Target Quantity Connects Statistical Evidence to Theory.\nKeep track of at least 5 terms that are new to you or that perhaps you don’t fully understand yet (e.g., estimand, DAG, potential outcome).\nI will be asking you to build a glossary of terms related to causal inference. I will not grade this yet, but still do your best attempt at defining each of these terms.",
    "crumbs": [
      "Part 1",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Week 1</span>"
    ]
  },
  {
    "objectID": "week01.html#exercise",
    "href": "week01.html#exercise",
    "title": "1  Week 1",
    "section": "1.2 Exercise",
    "text": "1.2 Exercise\n\n\nPackages\nlibrary(tidyverse)\nlibrary(gt)\n\n\nThe following data frame contains the potential outcomes for 8 individuals.\n\n\nCode\nd &lt;- data.frame(\n  T = c(0, 0, 1, 0, 0, 1, 1, 1),\n  Y0 = c(5, 8, 5, 12, 4, 8, 4, 9),\n  Y1 = c(5, 10, 3, 13, 2, 9, 1, 13), \n  id = LETTERS[1:8]\n)\n\ngt(d, rowname_col = \"id\") # gt is used for fancy printing of tables\n\n\n\n\n\n\n\n\n\nT\nY0\nY1\n\n\n\n\nA\n0\n5\n5\n\n\nB\n0\n8\n10\n\n\nC\n1\n5\n3\n\n\nD\n0\n12\n13\n\n\nE\n0\n4\n2\n\n\nF\n1\n8\n9\n\n\nG\n1\n4\n1\n\n\nH\n1\n9\n13\n\n\n\n\n\n\n\nThe variable T depicts whether someone got the “treatment” or not.\n\n\n\n\n\n\n\nCreate a new variable called Y that contains the observed outcomes.\nWhat is the Average Treatment Effect (ATE) for this 8 person experiment?\n\n\n\n\nNote. I’m only asking for a simple difference in means; this is also sometimes called the “naive estimator” (or “naive comparison”), which works just fine in simple experimental settings.",
    "crumbs": [
      "Part 1",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Week 1</span>"
    ]
  },
  {
    "objectID": "week01.html#exercise-1",
    "href": "week01.html#exercise-1",
    "title": "1  Week 1",
    "section": "1.3 Exercise",
    "text": "1.3 Exercise\n\n\n\n\n\n\n\nSimulate a new completely randomized experiment on these 8 people; that is, re sample \\(T\\) at random so that equal numbers get the treatment and the control.\nCreate a new variable called Y that contains the observed outcomes.\nWhat is the Average Treatment Effect (ATE) for this 8 person experiment?\nDo this a couple of times (at least 3) and note the differences.\n\n\n\n\n\n\n\n\n\n\nHint: You can re-sample \\(T\\) very easily using the sample() function.\nFor example:\n\n\nCode\nsample(d$T)\n\n\n[1] 1 0 1 0 0 1 0 1\n\n\n\n\n\n\n\n\n\n\n\nHow do these estimates compare to the “real” ATE?\nWhen I say “the real ATE” I basically mean this:\n\n\nCode\nmean(d$Y1 - d$Y0)\n\n\n[1] 0.125",
    "crumbs": [
      "Part 1",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Week 1</span>"
    ]
  },
  {
    "objectID": "week01.html#exercise-2",
    "href": "week01.html#exercise-2",
    "title": "1  Week 1",
    "section": "1.4 Exercise",
    "text": "1.4 Exercise\n\n\n\n\n\n\nObviously, an experiment of 8 people will not give you enough “statistical power.”\n\nAssuming the ATE is \\(0.125\\), how many people would you need to enroll in this experiment to have enough statistical power?\n\n\n\n\n\n\n\n\n\n\nHint: There are a few different ways of giving a reasonable answer to this question. The wording of this problem is ambiguous.",
    "crumbs": [
      "Part 1",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Week 1</span>"
    ]
  },
  {
    "objectID": "week02.html",
    "href": "week02.html",
    "title": "2  Week 2",
    "section": "",
    "text": "2.1 Exercise\nDear all,\nCovid forbids me from getting creative with homework this week.\nInstead, I will borrow some problems from NHK (chapter 5).",
    "crumbs": [
      "Part 1",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Week 2</span>"
    ]
  },
  {
    "objectID": "week02.html#exercise",
    "href": "week02.html#exercise",
    "title": "2  Week 2",
    "section": "",
    "text": "Which of the following is the best definition of the term identified as in “this variation has identified the effect we’re interested in”?\n\nWe’ve generated the data by conducting a controlled experiment in which treatment is randomly assigned.\nIn the data generating process, the only reason why we see variation in the outcome variable is because of the treatment variable.\nThe relationship we are looking at in the data actually tests a hypothesis.\nIn the variation we use, there’s no reason we’d see any relationship at all except for the effect we’re interested in.",
    "crumbs": [
      "Part 1",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Week 2</span>"
    ]
  },
  {
    "objectID": "week02.html#exercise-1",
    "href": "week02.html#exercise-1",
    "title": "2  Week 2",
    "section": "2.2 Exercise",
    "text": "2.2 Exercise\n\n\n\n\n\n\nYou read about a new study with the headline “eating caviar linked to longer lifespan.” The study’s research question is “does eating caviar make you live longer?” In the study’s data, they find that people who eat caviar have, on average, longer lifespans than people who don’t.\n\nWhat are some alternate explanations for this relationship?\nWhat sort of variation would identify the answer to the research question?\nGive one suggestion for how the study authors might isolate variation that would identify the answer to the research question",
    "crumbs": [
      "Part 1",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Week 2</span>"
    ]
  },
  {
    "objectID": "week02.html#exercise-2",
    "href": "week02.html#exercise-2",
    "title": "2  Week 2",
    "section": "2.3 Exercise",
    "text": "2.3 Exercise\n\n\n\n\n\n\nFor each of the following news headlines, assume that the underlying data actually only shows a correlation between the two variables mentioned. Give an alternate explanation for the correlation other than the causal relationship implied by the headline.\n\n“As stock market drops, presidential approval ratings decline.”\n“Dates are announced for the downtown summer concert series, driving up sales at downtown restaurants.”\n“Unsanitary? Hospital visits linked to 20% increased risk of disease.”\n“Dress for success! Every CEO follows this office-wear rule.”",
    "crumbs": [
      "Part 1",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Week 2</span>"
    ]
  },
  {
    "objectID": "week02.html#exercise-3",
    "href": "week02.html#exercise-3",
    "title": "2  Week 2",
    "section": "2.4 Exercise",
    "text": "2.4 Exercise\n\n\n\n\n\n\nWhy is a variable that causes both the “treatment” and “outcome” variables especially concerning for identification? You may want to use the phrase “alternate explanation” in your answer.",
    "crumbs": [
      "Part 1",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Week 2</span>"
    ]
  },
  {
    "objectID": "week02.html#exercise-4",
    "href": "week02.html#exercise-4",
    "title": "2  Week 2",
    "section": "2.5 Exercise",
    "text": "2.5 Exercise\n\n\n\n\n\n\nShoe company Crikey claims that people who wear their fancy and expensive professional running-shoe Cool Mistrunner brand run 4 to 5% faster than if they wore an average shoe.\n\nIn a few sentences, describe the data-generating process (you will probably leave some things out, that’s okay).\nWhat are possible alternative explanations for this claim, aside from the shoe making the person run faster?\nIn running their study, the researchers accounted for some alternative explanations, including: gender, enthusiasm for running, and whether runners have participated in marathons and/or half marathons. Think of an alternative explanation not on this list. What is the implication of not accounting for this alternative explanation?",
    "crumbs": [
      "Part 1",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Week 2</span>"
    ]
  },
  {
    "objectID": "week03.html",
    "href": "week03.html",
    "title": "3  Week 3",
    "section": "",
    "text": "3.1 Exercise\nSet up\nNote. I will be asking you to draw some DAGs. There are many ways to do this—e.g., dagitty, ggdag, shinydag. I tend to use this website and then copy-paste a screen shot of the result. You can use whatever you want (whatever is easier, including Word or PowerPoint if you must insist).\nIgnorability\nExperiments work because they make the distribution of potential outcomes the same across levels of the treatment variable. In other words, the potential outcomes and the treatment indicator are independent.\nThis is how this idea was introduced in class:\n\\[\n\\begin{align}\nY^0 \\perp T, && Y^1 \\perp T\n\\end{align}\n\\]\nAssume the following table comes from perfectly executed experiment.\n*30% of the population is in group T = 1",
    "crumbs": [
      "Part 1",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Week 3</span>"
    ]
  },
  {
    "objectID": "week03.html#exercise",
    "href": "week03.html#exercise",
    "title": "3  Week 3",
    "section": "",
    "text": "Table 3.1: Perfect Experiment Example*\n\n\n\n\n\nGroup (\\(T\\))\n\\(E[Y^1]\\)\n\\(E[Y^0]\\)\n\n\n\n\n\\(T = 1\\)\n10,000\n?\n\n\n\\(T = 0\\)\n?\n5,000\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFill in the missing cells.\nWhat is the ATE?",
    "crumbs": [
      "Part 1",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Week 3</span>"
    ]
  },
  {
    "objectID": "week03.html#exercise-1",
    "href": "week03.html#exercise-1",
    "title": "3  Week 3",
    "section": "3.2 Exercise",
    "text": "3.2 Exercise\nGo back to the glossary I asked you to start creating during Week 1.\nMake sure to add the following terms:\n\n\n\n\n\n\n\nDAG.\nPaths.\nDirect effects.\nIndirect effects.\nTotal effects.\nFront door paths.\nBack door paths.\nConfounding.\nCollider.\nOpen Path.\nClosed Path.\n\n\n\n\n\nSee: Section 1.1",
    "crumbs": [
      "Part 1",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Week 3</span>"
    ]
  },
  {
    "objectID": "week03.html#exercise-2",
    "href": "week03.html#exercise-2",
    "title": "3  Week 3",
    "section": "3.3 Exercise",
    "text": "3.3 Exercise\nDrawing DAGs.\nThe following exercises are problems from NHK (Chapter 7).\n\n\n\n\n\n\nDraw a causal diagram for the research question “do long shift hours make doctors give lower-quality care?” that incorporates the following features (and only the following features):\n\nLong shift hours affect both how tired doctors are, and how much experience they have, both of which affect the quality of care\nHow long shifts are is often decided by the hospital the doctor works at. There are plenty of other things about a given hospital that also affect the quality of care, like its funding level, how crowded it is, and so on\nNew policies that reduce shift times may be implemented at the same time (with the timing determined by some unobservable change in policy preferences) as other policies that also attempt to improve the quality of care",
    "crumbs": [
      "Part 1",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Week 3</span>"
    ]
  },
  {
    "objectID": "week03.html#exercise-3",
    "href": "week03.html#exercise-3",
    "title": "3  Week 3",
    "section": "3.4 Exercise",
    "text": "3.4 Exercise\n\n\n\n\n\n\nConsider this research question: Does the funding level of public schools affect student achievement for students in your country?\n\nWhat is the treatment and what is the outcome of interest?\nWrite down a list of relevant variables.\nWhich of the variables in your list in part b are causes of both treatment and outcome?\nWhy might we want to pay extra attention to the variables listed in part c?\nDraw a causal diagram of the variables listed in part b.\nSimplify the diagram from part e.",
    "crumbs": [
      "Part 1",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Week 3</span>"
    ]
  },
  {
    "objectID": "week03.html#exercise-4",
    "href": "week03.html#exercise-4",
    "title": "3  Week 3",
    "section": "3.5 Exercise",
    "text": "3.5 Exercise\n\n\n\n\n\n\nHow can a causal diagram be modified so as to avoid cyclic relationships?\nConsider the diagram below. It depicts a cyclical relationship between student achievement and motivation. If students achieve more (i.e., score well on exams), then their motivation goes up, and if their motivation goes up, they achieve more. Change the diagram so that the relationship is not cyclic anymore.\n\\[\n\\text{Student Achievement} \\longleftrightarrow \\text{Motivation}\n\\]\n\n\n\n\n\n\n\n\n\nHint: We didn’t see this in class, but you should be able to figure it out from the readings.",
    "crumbs": [
      "Part 1",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Week 3</span>"
    ]
  },
  {
    "objectID": "week03.html#exercise-5",
    "href": "week03.html#exercise-5",
    "title": "3  Week 3",
    "section": "3.6 Exercise",
    "text": "3.6 Exercise\nThe following exercises are problems from NHK (Chapter 8).\n\n\n\n\n\n\nAssuming that a path has no colliders on it, what is the difference between a path being Open and Closed?",
    "crumbs": [
      "Part 1",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Week 3</span>"
    ]
  },
  {
    "objectID": "week03.html#exercise-6",
    "href": "week03.html#exercise-6",
    "title": "3  Week 3",
    "section": "3.7 Exercise",
    "text": "3.7 Exercise\n\n\n\n\n\n\nConsider the below generic causal diagram.\n\n\n\n\n\n\nList every path from X to Y.\nWhich of the paths are front-door paths?\nWhich of the paths are open back-door paths?\nWhat variables must be controlled for in order to identify the effect of X on Y? (only list what must be controlled for, not anything that additionally could be controlled for).",
    "crumbs": [
      "Part 1",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Week 3</span>"
    ]
  },
  {
    "objectID": "week03.html#exercise-7",
    "href": "week03.html#exercise-7",
    "title": "3  Week 3",
    "section": "3.8 Exercise",
    "text": "3.8 Exercise\n\n\n\n\n\n\nWhich of the following describes a causal path where all the arrows point away from the treatment?\n\nOpen Path\nClosed Path\nFront Door Path\nBack Door Path",
    "crumbs": [
      "Part 1",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Week 3</span>"
    ]
  },
  {
    "objectID": "week03.html#exercise-8",
    "href": "week03.html#exercise-8",
    "title": "3  Week 3",
    "section": "3.9 Exercise",
    "text": "3.9 Exercise\n\n\n\n\n\n\nConsider the figure below, which depicts the relationship between teaching quality, number of publications (e.g., articles, books), and popularity among scholars and students in a population of professors.\n\n\n\n\n\n\nWhat type of variable is Popularity in one path on this diagram?\nDiscuss what would happen if you controlled for Popularity.",
    "crumbs": [
      "Part 1",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Week 3</span>"
    ]
  },
  {
    "objectID": "week03.html#exercise-9",
    "href": "week03.html#exercise-9",
    "title": "3  Week 3",
    "section": "3.10 Exercise",
    "text": "3.10 Exercise\n\n\n\n\n\n\nGo to the app Steve showed us in class.\nhttps://cbdrh.shinyapps.io/daggle/\nSpend some time noodling around with it and upload screenshots with the right answer for three DAGs with 4, 6, and 8 nodes each. Set the complexity to “difficult.”",
    "crumbs": [
      "Part 1",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Week 3</span>"
    ]
  },
  {
    "objectID": "week03.html#exercise-10",
    "href": "week03.html#exercise-10",
    "title": "3  Week 3",
    "section": "3.11 Exercise",
    "text": "3.11 Exercise\nHouse of DAG Simulation.\nI’ve included a little script with a couple of functions meant to illustrate the connection between DAGs and the estimands we saw in class (ATE, ATT, ATC).\nSave it to your project and load it using the source() function.\nYou should see a function called hod_simulation() which creates a dataset that corresponds to the following DAG:\n\n\n\n\n\n\n\\(Y\\): outcome\n\\(T\\): treatment\n\\(U\\): unobserved confounder\n\\(S\\): affects selection into \\(T\\)\n\\(X\\): affects \\(Y\\) directly\n\n\n\nThe hod_simulation() function has the following arguments:\n\nN: Sample Size\nrho: The correlation between \\(S\\) and \\(X\\), it accepts values between -1 and 1.\nBt: this is the treatment effect.\nBx: this is the direct effect of \\(X\\) on \\(Y\\)\n\nNote. There’s bunch of stuff going on under the hood, but we won’t worry about that this week.\nThis is the dataset it creates:\n\n\nCode\nsource(\"hod_simulation_functions.R\")\n\n\nStandard Error ~  0.322 \nPower ~  0.873\n\n\nJoining with `by = join_by(variable)`\n\n\n# A tibble: 4 × 3\n  variable    sd  mean\n  &lt;chr&gt;    &lt;dbl&gt; &lt;dbl&gt;\n1 y        5.09  1.50 \n2 t        0.500 0.512\n3 x        1.00  1.07 \n4 s        1.01  1.02 \n\n\nCode\nset.seed(12345) ## include this so that grading is easier for me.\nd &lt;- hod_simulation(N = 1e3, Bt = 2, Bx = 4, rho = 0.8)\n\n\nStandard Error ~  0.405 \nPower ~  0.999\n\n\nJoining with `by = join_by(variable)`\n\n\n# A tibble: 4 × 3\n  variable    sd  mean\n  &lt;chr&gt;    &lt;dbl&gt; &lt;dbl&gt;\n1 y        6.75   4.98\n2 t        0.500  0.52\n3 x        1.02   1.00\n4 s        1.01   1.02\n\n\nNote. Ignore the “Standard Error” and “Power” messages.\n\n\nCode\nd\n\n\n# A tibble: 1,000 × 6\n       y0     y1     t      y       x      s\n *  &lt;dbl&gt;  &lt;dbl&gt; &lt;int&gt;  &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;\n 1  3.03   5.03      1  5.03   1.90   1.84  \n 2  5.20   7.20      1  7.20   0.545  0.699 \n 3 -0.351  1.65      1  1.65  -0.355  0.729 \n 4 -2.76  -0.764     1 -0.764  1.03   1.44  \n 5 -3.79  -1.79      0 -3.79   0.0507 0.335 \n 6 12.9   14.9       0 12.9    2.57   1.71  \n 7  6.70   8.70      0  6.70   1.63   1.56  \n 8  3.68   5.68      0  3.68   1.40   0.694 \n 9 -1.65   0.353     1  0.353  0.307  0.0589\n10 10.8   12.8       0 10.8    1.77   2.14  \n# ℹ 990 more rows\n\n\n\n\n\n\n\n\n\nWithout looking at the results just yet… do you think the naive estimate will be larger or smaller than the “real” estimate ( \\(ATE = 2\\) )?\nCheck your answer. What are the results given by the naive estimator?\nRe-do this but set rho to -0.8 (so that \\(S\\) and \\(X\\) are now negatively correlated).\n\n\n\n\n\n\n\n\n\n\nHint: You can use group_by() and then summarize() to create a table just like Table 3.1.",
    "crumbs": [
      "Part 1",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Week 3</span>"
    ]
  },
  {
    "objectID": "week03.html#exercise-11",
    "href": "week03.html#exercise-11",
    "title": "3  Week 3",
    "section": "3.12 Exercise",
    "text": "3.12 Exercise\n\n\n\n\n\n\nTake the dataset d created in the previous question and modify it so that the treatment is now randomized (this will destroy the path between \\(S\\) and \\(T\\)).\n\n\n\n\n\n\n\n\n\nHint: You can achieve this using the sample() function on d$t.\nYou will also want to create a new d$y using the ifelse() function (or something similar to that).\n\n\n\n\n\n\n\n\n\n\nWithout looking at the results just yet… do you think the naive estimate will be larger or smaller than the “real” estimate ( \\(ATE = 2\\) )?\nCheck your answer. What are the results given by the naive estimator?\nUse lm() to predict the newly created y from t. What are the coefficient values?\nUse lm() to predict the newly created y from t and x. What are the coefficient values?",
    "crumbs": [
      "Part 1",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Week 3</span>"
    ]
  },
  {
    "objectID": "week04.html",
    "href": "week04.html",
    "title": "4  Week 4",
    "section": "",
    "text": "4.1 Exercise\nSet up\nColliders\nFor this exercise I am going to ask you to create the following simulated dataset.\nCode\nN &lt;- 1e4\n\nd &lt;- tibble(\n  x = rnorm(N, 0, 1),\n  y = rnorm(N, 0, 1)\n)\nThe relationship between \\(x\\) and \\(y\\) should look something like this:",
    "crumbs": [
      "Part 1",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Week 4</span>"
    ]
  },
  {
    "objectID": "week04.html#exercise",
    "href": "week04.html#exercise",
    "title": "4  Week 4",
    "section": "",
    "text": "Now I am going to ask you to create an association between \\(x\\) and \\(y\\) via some form of collider bias.\n\\[\nx \\longrightarrow \\underbrace{\\text{z}}_{\\small {\\text{collider}}} \\longleftarrow y\n\\]\nYou are tasked to do this in four different ways, each of them corresponding to one of the plots in Figure 4.1.\n\n\n\n\n\n\n\n\n\n\n\n(1)\n\n\n\n\n\n\n\n(2)\n\n\n\n\n\n\n\n\n\n(3)\n\n\n\n\n\n\n\n(4)\n\n\n\n\n\n\nFigure 4.1: Conditioning on a Collider\n\n\n\n\n\n\n\n\n\nHint: The first three plots represent a process in which “conditioning on a collider” means that some observations are removed from the d dataset. You can then calculate the slopes simply by doing something like this:\nlm(y ~ x, data = d_filtered)\nThe last plot represents a process in which the association between \\(x\\) and \\(y\\) is created by literally “conditioning on a collider.” Something like this:\nlm(y ~ x + z, data = d)",
    "crumbs": [
      "Part 1",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Week 4</span>"
    ]
  },
  {
    "objectID": "week04.html#exercise-1",
    "href": "week04.html#exercise-1",
    "title": "4  Week 4",
    "section": "4.2 Exercise",
    "text": "4.2 Exercise\nNote. The following exercises are problems from NHK (Chapter 10). If you have issues with some of the terminology used, you should be able to figure it out from reading the book.\nDefine in your own words (i.e., don’t just copy down what’s written in the glossary) each of the following terms:\n\nConditional average treatment effect\nAverage treatment on the treated\nAverage treatment on the untreated",
    "crumbs": [
      "Part 1",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Week 4</span>"
    ]
  },
  {
    "objectID": "week04.html#exercise-2",
    "href": "week04.html#exercise-2",
    "title": "4  Week 4",
    "section": "4.3 Exercise",
    "text": "4.3 Exercise\nProvide an example of a treatment effect that you would expect to be highly heterogeneous, and explain why you think it is likely to be heterogeneous.",
    "crumbs": [
      "Part 1",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Week 4</span>"
    ]
  },
  {
    "objectID": "week04.html#exercise-3",
    "href": "week04.html#exercise-3",
    "title": "4  Week 4",
    "section": "4.4 Exercise",
    "text": "4.4 Exercise\nConsider the data in the table below that shows the hypothetical treatment effect of cognitive behavioral therapy on depression for six participants. For the sake of this example, the six participants represent the population of interest.\n\n\n\nCase\nAge\nGender\nEffect\n\n\n\n\nA\n15\nMan\n7\n\n\nB\n40\nWoman\n3\n\n\nC\n30\nWoman\n7\n\n\nD\n20\nNon-binary\n8\n\n\nE\n15\nMan\n7\n\n\nF\n25\nWoman\n4\n\n\n\n\nWhat is the overall average treatment effect for the population?\nWhat is the average treatment effect for Women?\nIf nearly all Non-binary people get treated, and about half of all Women get treated, and we control for the differences between Women and Non-binary people, what kind of treatment effect average will we get, and what can we say about the numerical estimate we’ll get?\nIf we assume that, in the absence of treatment, everyone would have had the same outcome, and also only teenagers (19 or younger) ever receive treatment, and we compare treated people to control people, what kind of treatment effect average will we get, and what can we say about the numerical estimate we’ll get?",
    "crumbs": [
      "Part 1",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Week 4</span>"
    ]
  },
  {
    "objectID": "week04.html#exercise-4",
    "href": "week04.html#exercise-4",
    "title": "4  Week 4",
    "section": "4.5 Exercise",
    "text": "4.5 Exercise\nGive an example where the average treatment effect on the treated would be more useful to consider than the overall average treatment effect, and explain why.",
    "crumbs": [
      "Part 1",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Week 4</span>"
    ]
  },
  {
    "objectID": "week04.html#exercise-5",
    "href": "week04.html#exercise-5",
    "title": "4  Week 4",
    "section": "4.6 Exercise",
    "text": "4.6 Exercise\nWhich of the following describes the average treatment effect of assigning treatment, whether or not treatment is actually received?\n\nLocal average treatment effect\nAverage treatment on the treated\nIntent-to-treat\nVariance-weighted average treatment effect",
    "crumbs": [
      "Part 1",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Week 4</span>"
    ]
  },
  {
    "objectID": "week04.html#exercise-6",
    "href": "week04.html#exercise-6",
    "title": "4  Week 4",
    "section": "4.7 Exercise",
    "text": "4.7 Exercise\nSuppose you are conducting an experiment to see whether pricing cookies at $1.99 versus $2 affects the decision to purchase the cookies. The population of interest is all adults in the United States. You recruit people from your university to participate and randomize them to either see cookies priced as $1.99 or $2, then write down whether they purchased cookies. What kind of average treatment effect can you identify from this experiment?",
    "crumbs": [
      "Part 1",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Week 4</span>"
    ]
  },
  {
    "objectID": "week04.html#exercise-7",
    "href": "week04.html#exercise-7",
    "title": "4  Week 4",
    "section": "4.8 Exercise",
    "text": "4.8 Exercise\nFor each of the following identification strategies, what kind of treatment effect(s) is most likely to be identified?\n\nA randomized experiment using a representative sample\nTrue randomization within only a certain demographic group\nClosing back door paths connected to variation in treatment\nIsolating the part of the variation in treatment variable that is driven by an exogenous variable\nThe control group is comparable to the treatment group, but treatment effects may be different across these groups",
    "crumbs": [
      "Part 1",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Week 4</span>"
    ]
  },
  {
    "objectID": "week05.html",
    "href": "week05.html",
    "title": "5  Week 5",
    "section": "",
    "text": "5.1 Instructions\nHi everyone, this is marginaleffects appreciation week. I will be asking you to re-do some of the things we did with Steve without using this package and then compare the results.\nYou will need to run the following chunk of code before running anything else.\nNote. We will be using data from the GSS instead of simulated “toy data.” Don’t give too much thought to the “validity” of these estimates; think about them as applying “toy DAGs” to real data. I only care that you understand the mechanics average treatment effects using regression.\nIf you are done quickly, I recommend you skim this website:\nhttps://marginaleffects.com/\nBoth functions have an argument called newdata, which you can use doing something similar to this toy example:\nCode\nols &lt;- lm(mpg ~ disp + am, data = mtcars)\n\nnew_am0 &lt;- mtcars |&gt; \n  mutate(am = 0)\n\nnew_am1 &lt;- mtcars |&gt; \n  mutate(am = 1)\n\np0 &lt;- predict(ols, newdata = new_am0) ## predictions for am == 0\np1 &lt;- predict(ols, newdata = new_am1) ## predictions for am == 1",
    "crumbs": [
      "Part 1",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Week 5</span>"
    ]
  },
  {
    "objectID": "week05.html#instructions",
    "href": "week05.html#instructions",
    "title": "5  Week 5",
    "section": "",
    "text": "This homework has three sections.\nEach exercise has two marginaleffects outputs: (1) the ATE estimate and (2) the ATT/ATU estimates.\nYou will have to reproduce these estimates without using marginaleffects. There are a couple of ways to do this, but you will probably end up using the predict() function (from base R), or the augment() function (from the broom package).\n\n\n\n\n\n\n\n\n\n\n\nBonus\nThe avg_slopes() function has an arguments called hypothesis which lets you estimate a standard error for the difference between the ATT and the ATU (among other things). This shows up in Steve’s code for this week.\nIf you are done early, I suggest you try and calculate one of these standard errors without avg_slopes (e.g., using a bootstrap).",
    "crumbs": [
      "Part 1",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Week 5</span>"
    ]
  },
  {
    "objectID": "week05.html#linear-regression",
    "href": "week05.html#linear-regression",
    "title": "5  Week 5",
    "section": "5.2 Linear Regression",
    "text": "5.2 Linear Regression\nWe will use this data.\n\n\nCode\nd &lt;- gss2022 |&gt; \n  select(tvhours, degree, madeg, padeg) |&gt; \n  mutate(pardeg = pmax(madeg, padeg, na.rm = TRUE),\n         college = if_else(degree &gt;= 3, 1L, 0L),\n         parcol = if_else(pardeg &gt;= 3, 1L, 0L)) |&gt;\n  select(tvhours, college, parcol) |&gt; \n  drop_na()\n\n\n\n5.2.1 Exercise\nAdditive link function, no interactions\n\n\nCode\nmod1 &lt;- lm(tvhours ~ college + parcol, data = d)\n\n# ATE estimate\navg_slopes(mod1, variables = \"college\") |&gt; \n  tidy()\n\n\n# A tibble: 1 × 9\n  term  contrast estimate std.error statistic p.value s.value conf.low conf.high\n  &lt;chr&gt; &lt;chr&gt;       &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1 coll… 1 - 0      -0.796     0.151     -5.27 1.35e-7    22.8    -1.09    -0.500\n\n\nANSWER GOES HERE\n\n\nCode\n# ATT/ATU estimate\navg_slopes(\n  model = mod1, \n  variables = \"college\",\n  by = \"college\" # separately by treatment group\n) |&gt; \n  tidy()\n\n\n# A tibble: 2 × 13\n  term    contrast college estimate std.error statistic p.value s.value conf.low\n  &lt;chr&gt;   &lt;chr&gt;      &lt;int&gt;    &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;\n1 college mean(1)…       0   -0.796     0.151     -5.27 1.35e-7    22.8    -1.09\n2 college mean(1)…       1   -0.796     0.151     -5.27 1.35e-7    22.8    -1.09\n# ℹ 4 more variables: conf.high &lt;dbl&gt;, predicted_lo &lt;dbl&gt;, predicted_hi &lt;dbl&gt;,\n#   predicted &lt;dbl&gt;\n\n\nANSWER GOES HERE\n\n\n5.2.2 Exercise\nAdditive link function, with interactions\n\n\nCode\nmod2 &lt;- lm(tvhours ~ college * parcol, data = d)\n\n# ATE estimate\navg_slopes(mod2, variables = \"college\") |&gt; \n  tidy()\n\n\n# A tibble: 1 × 9\n  term  contrast estimate std.error statistic p.value s.value conf.low conf.high\n  &lt;chr&gt; &lt;chr&gt;       &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1 coll… 1 - 0      -0.803     0.152     -5.29 1.20e-7    23.0    -1.10    -0.506\n\n\nANSWER GOES HERE\n\n\nCode\n# ATT/ATU estimate\navg_slopes(\n  model = mod2, \n  variables = \"college\",\n  by = \"college\" # separately by treatment group\n) |&gt; \n  tidy()\n\n\n# A tibble: 2 × 13\n  term    contrast college estimate std.error statistic p.value s.value conf.low\n  &lt;chr&gt;   &lt;chr&gt;      &lt;int&gt;    &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;\n1 college mean(1)…       0   -0.821     0.160     -5.14 2.70e-7    21.8    -1.13\n2 college mean(1)…       1   -0.772     0.159     -4.87 1.13e-6    19.8    -1.08\n# ℹ 4 more variables: conf.high &lt;dbl&gt;, predicted_lo &lt;dbl&gt;, predicted_hi &lt;dbl&gt;,\n#   predicted &lt;dbl&gt;\n\n\nANSWER GOES HERE",
    "crumbs": [
      "Part 1",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Week 5</span>"
    ]
  },
  {
    "objectID": "week05.html#poisson-regression",
    "href": "week05.html#poisson-regression",
    "title": "5  Week 5",
    "section": "5.3 Poisson Regression",
    "text": "5.3 Poisson Regression\nWe will use this data.\n\n\nCode\nd &lt;- gss2022 |&gt;\n  filter(wrkstat == 1) |&gt; # full time workers\n  select(realrinc, degree, madeg, padeg, sex, age) |&gt; \n  mutate(pardeg = pmax(madeg, padeg, na.rm = TRUE),\n         college = if_else(degree &gt;= 3, 1L, 0L),\n         parcol = if_else(pardeg &gt;= 3, 1L, 0L),\n         female = if_else(sex == 2, 1L, 0L),\n         realrinc = floor(realrinc)) |&gt;             # integer\n  select(realrinc, college, parcol, female, age) |&gt; \n  drop_na()\n\n\n\n5.3.1 Exercise\nUsing the log-counts, no interactions\n\n\nCode\nqp1 &lt;- glm(realrinc ~ college + (parcol + female + age + I(age^2)), \n           data = d,\n           family = \"quasipoisson\")\n\navg_slopes(qp1,\n           variables = \"college\",\n           type = \"link\") |&gt; \n  tidy()\n\n\n# A tibble: 1 × 9\n  term    contrast estimate std.error statistic  p.value s.value conf.low\n  &lt;chr&gt;   &lt;chr&gt;       &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;\n1 college 1 - 0       0.599    0.0510      11.7 7.45e-32    103.    0.499\n# ℹ 1 more variable: conf.high &lt;dbl&gt;\n\n\nANSWER GOES HERE\n\n\nCode\navg_slopes(qp1,\n           variables = \"college\",\n           type = \"link\",\n           by = \"college\") |&gt; # separately by treatment group\n  tidy()\n\n\n# A tibble: 2 × 13\n  term   contrast college estimate std.error statistic  p.value s.value conf.low\n  &lt;chr&gt;  &lt;chr&gt;      &lt;int&gt;    &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;\n1 colle… mean(1)…       0    0.599    0.0510      11.7 7.45e-32    103.    0.499\n2 colle… mean(1)…       1    0.599    0.0510      11.7 7.45e-32    103.    0.499\n# ℹ 4 more variables: conf.high &lt;dbl&gt;, predicted_lo &lt;dbl&gt;, predicted_hi &lt;dbl&gt;,\n#   predicted &lt;dbl&gt;\n\n\nANSWER GOES HERE\n\n\n5.3.2 Exercise\nNon-linear response, no interactions\n\n\nCode\navg_slopes(qp1,\n           variables = \"college\",\n           type = \"response\") |&gt; \n  tidy()\n\n\n# A tibble: 1 × 9\n  term    contrast estimate std.error statistic  p.value s.value conf.low\n  &lt;chr&gt;   &lt;chr&gt;       &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;\n1 college 1 - 0      21237.     1831.      11.6 4.18e-31    101.   17649.\n# ℹ 1 more variable: conf.high &lt;dbl&gt;\n\n\nANSWER GOES HERE\n\n\nCode\navg_slopes(qp1,\n           variables = \"college\",\n           type = \"response\",\n           by = \"college\") |&gt; # separately by treatment group\n  tidy()\n\n\n# A tibble: 2 × 13\n  term   contrast college estimate std.error statistic  p.value s.value conf.low\n  &lt;chr&gt;  &lt;chr&gt;      &lt;int&gt;    &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;\n1 colle… mean(1)…       0   20636.     1861.      11.1 1.41e-28    92.5   16988.\n2 colle… mean(1)…       1   21977.     1816.      12.1 1.05e-33   110.    18417.\n# ℹ 4 more variables: conf.high &lt;dbl&gt;, predicted_lo &lt;dbl&gt;, predicted_hi &lt;dbl&gt;,\n#   predicted &lt;dbl&gt;\n\n\n\n\n5.3.3 Exercise\nUsing the log-counts, with interactions\n\n\nCode\nqp2 &lt;- glm(realrinc ~ college * (parcol + female + age + I(age^2)), \n           data = d,\n           family = \"quasipoisson\")\n\navg_slopes(qp2,\n           variables = \"college\",\n           type = \"link\") |&gt; \n  tidy()\n\n\n# A tibble: 1 × 9\n  term    contrast estimate std.error statistic  p.value s.value conf.low\n  &lt;chr&gt;   &lt;chr&gt;       &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;\n1 college 1 - 0       0.580    0.0543      10.7 1.20e-26    86.1    0.474\n# ℹ 1 more variable: conf.high &lt;dbl&gt;\n\n\nANSWER GOES HERE\n\n\nCode\navg_slopes(qp2,\n           variables = \"college\",\n           type = \"link\",\n           by = \"college\") |&gt; # separately by treatment group\n  tidy()\n\n\n# A tibble: 2 × 13\n  term   contrast college estimate std.error statistic  p.value s.value conf.low\n  &lt;chr&gt;  &lt;chr&gt;      &lt;int&gt;    &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;\n1 colle… mean(1)…       0    0.567    0.0571      9.94 2.77e-23    74.9    0.455\n2 colle… mean(1)…       1    0.596    0.0600      9.94 2.87e-23    74.9    0.479\n# ℹ 4 more variables: conf.high &lt;dbl&gt;, predicted_lo &lt;dbl&gt;, predicted_hi &lt;dbl&gt;,\n#   predicted &lt;dbl&gt;\n\n\nANSWER GOES HERE\n\n\n5.3.4 Exercise\nNon-linear response, with interactions\n\n\nCode\navg_slopes(qp2,\n           variables = \"college\",\n           type = \"response\") |&gt; \n  tidy()\n\n\n# A tibble: 1 × 9\n  term    contrast estimate std.error statistic  p.value s.value conf.low\n  &lt;chr&gt;   &lt;chr&gt;       &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;\n1 college 1 - 0      21190.     1817.      11.7 1.99e-31    102.   17629.\n# ℹ 1 more variable: conf.high &lt;dbl&gt;\n\n\nANSWER GOES HERE\n\n\nCode\navg_slopes(qp2,\n           variables = \"college\",\n           type = \"response\",\n           by = \"college\") |&gt; # separately by treatment group\n  tidy()\n\n\n# A tibble: 2 × 13\n  term   contrast college estimate std.error statistic  p.value s.value conf.low\n  &lt;chr&gt;  &lt;chr&gt;      &lt;int&gt;    &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;\n1 colle… mean(1)…       0   20196.     1937.      10.4 1.90e-25    82.1   16400.\n2 colle… mean(1)…       1   22411.     1963.      11.4 3.51e-30    97.8   18563.\n# ℹ 4 more variables: conf.high &lt;dbl&gt;, predicted_lo &lt;dbl&gt;, predicted_hi &lt;dbl&gt;,\n#   predicted &lt;dbl&gt;\n\n\nANSWER GOES HERE",
    "crumbs": [
      "Part 1",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Week 5</span>"
    ]
  },
  {
    "objectID": "week05.html#logistic-regression",
    "href": "week05.html#logistic-regression",
    "title": "5  Week 5",
    "section": "5.4 Logistic Regression",
    "text": "5.4 Logistic Regression\nWe will use this data.\n\n\nCode\nd &lt;- gss2022 |&gt;\n  select(abany, degree, madeg, padeg, sex, age) |&gt; \n  mutate(pardeg = pmax(madeg, padeg, na.rm = TRUE),\n         college = if_else(degree &gt;= 3, 1L, 0L),\n         parcol = if_else(pardeg &gt;= 3, 1L, 0L),\n         female = if_else(sex == 2, 1L, 0L),\n         abany = if_else(abany == 1, 1L, 0L)) |&gt;\n  select(abany, college, parcol, female, age) |&gt; \n  drop_na()\n\n\n\n5.4.1 Exercise\nUsing log-odds, no interactions\n\n\nCode\nlr1 &lt;- glm(abany ~ college + (parcol + female + age + I(age^2)),\n          data = d,\n          family = binomial)\n\n# ATE estimate\navg_slopes(lr1,\n           variables = \"college\",\n           type = \"link\") |&gt; \n  tidy()\n\n\n# A tibble: 1 × 9\n  term  contrast estimate std.error statistic p.value s.value conf.low conf.high\n  &lt;chr&gt; &lt;chr&gt;       &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1 coll… 1 - 0       0.438     0.146      3.00 0.00273    8.52    0.151     0.724\n\n\nANSWER GOES HERE\n\n\nCode\navg_slopes(lr1,\n           variables = \"college\",\n           by = \"college\",\n           type = \"link\") |&gt; \n  tidy()\n\n\n# A tibble: 2 × 13\n  term    contrast college estimate std.error statistic p.value s.value conf.low\n  &lt;chr&gt;   &lt;chr&gt;      &lt;int&gt;    &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;\n1 college mean(1)…       0    0.438     0.146      3.00 0.00273    8.52    0.151\n2 college mean(1)…       1    0.438     0.146      3.00 0.00273    8.52    0.151\n# ℹ 4 more variables: conf.high &lt;dbl&gt;, predicted_lo &lt;dbl&gt;, predicted_hi &lt;dbl&gt;,\n#   predicted &lt;dbl&gt;\n\n\nANSWER GOES HERE\n\n\n5.4.2 Exercise\nUsing non-linear response (aka probabilities), no interactions\n\n\nCode\n# ATE estimate\navg_slopes(lr1,\n           variables = \"college\",\n           type = \"response\") |&gt; \n  tidy()\n\n\n# A tibble: 1 × 9\n  term  contrast estimate std.error statistic p.value s.value conf.low conf.high\n  &lt;chr&gt; &lt;chr&gt;       &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1 coll… 1 - 0       0.102    0.0337      3.02 0.00249    8.65   0.0359     0.168\n\n\nANSWER GOES HERE\n\n\nCode\navg_slopes(lr1,\n           variables = \"college\",\n           by = \"college\",\n           type = \"response\") |&gt; \n  tidy()\n\n\n# A tibble: 2 × 13\n  term    contrast college estimate std.error statistic p.value s.value conf.low\n  &lt;chr&gt;   &lt;chr&gt;      &lt;int&gt;    &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;\n1 college mean(1)…       0   0.104     0.0341      3.04 0.00235    8.73   0.0369\n2 college mean(1)…       1   0.0989    0.0330      2.99 0.00275    8.51   0.0342\n# ℹ 4 more variables: conf.high &lt;dbl&gt;, predicted_lo &lt;dbl&gt;, predicted_hi &lt;dbl&gt;,\n#   predicted &lt;dbl&gt;\n\n\nANSWER GOES HERE\n\n\n5.4.3 Exercise\nUsing log-odds, with interactions\n\n\nCode\nlr2 &lt;- glm(abany ~ college * (parcol + female + age + I(age^2)),\n          data = d,\n          family = binomial)\n\n# ATE estimate\navg_slopes(lr2,\n           variables = \"college\",\n           type = \"link\") |&gt; \n  tidy()\n\n\n# A tibble: 1 × 9\n  term  contrast estimate std.error statistic p.value s.value conf.low conf.high\n  &lt;chr&gt; &lt;chr&gt;       &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1 coll… 1 - 0       0.453     0.149      3.04 0.00239    8.71    0.161     0.745\n\n\nANSWER GOES HERE\n\n\nCode\navg_slopes(lr2,\n           variables = \"college\",\n           by = \"college\",\n           type = \"link\") |&gt; \n  tidy()\n\n\n# A tibble: 2 × 13\n  term    contrast college estimate std.error statistic p.value s.value conf.low\n  &lt;chr&gt;   &lt;chr&gt;      &lt;int&gt;    &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;\n1 college mean(1)…       0    0.450     0.159      2.83 0.00460    7.76    0.139\n2 college mean(1)…       1    0.458     0.161      2.85 0.00435    7.84    0.143\n# ℹ 4 more variables: conf.high &lt;dbl&gt;, predicted_lo &lt;dbl&gt;, predicted_hi &lt;dbl&gt;,\n#   predicted &lt;dbl&gt;\n\n\nANSWER GOES HERE\n\n\n5.4.4 Exercise\nUsing non-linear response (aka probabilities), with interactions\n\n\nCode\n# ATE estimate\navg_slopes(lr2,\n           variables = \"college\",\n           type = \"response\") |&gt; \n  tidy()\n\n\n# A tibble: 1 × 9\n  term  contrast estimate std.error statistic p.value s.value conf.low conf.high\n  &lt;chr&gt; &lt;chr&gt;       &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1 coll… 1 - 0       0.103    0.0338      3.05 0.00228    8.78   0.0369     0.169\n\n\nANSWER GOES HERE\n\n\nCode\navg_slopes(lr2,\n           variables = \"college\",\n           by = \"college\",\n           type = \"response\") |&gt; \n  tidy()\n\n\n# A tibble: 2 × 13\n  term    contrast college estimate std.error statistic p.value s.value conf.low\n  &lt;chr&gt;   &lt;chr&gt;      &lt;int&gt;    &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;\n1 college mean(1)…       0    0.104    0.0363      2.87 0.00407    7.94   0.0332\n2 college mean(1)…       1    0.101    0.0353      2.87 0.00415    7.91   0.0320\n# ℹ 4 more variables: conf.high &lt;dbl&gt;, predicted_lo &lt;dbl&gt;, predicted_hi &lt;dbl&gt;,\n#   predicted &lt;dbl&gt;\n\n\nANSWER GOES HERE",
    "crumbs": [
      "Part 1",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Week 5</span>"
    ]
  },
  {
    "objectID": "week06.html",
    "href": "week06.html",
    "title": "6  Week 6",
    "section": "",
    "text": "6.1 Regression\nNote. There where two exercises here about simulation and omitted variable bias which you can now find in the solutions. It’s a story about how I used simulations to realize I was being misled by information contained in a famous textbook.",
    "crumbs": [
      "Part 1",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Week 6</span>"
    ]
  },
  {
    "objectID": "week06.html#balance-and-overlap",
    "href": "week06.html#balance-and-overlap",
    "title": "6  Week 6",
    "section": "6.2 Balance and Overlap",
    "text": "6.2 Balance and Overlap\nBalance\nThe sort of bias that we get from confounding can be interpreted more precisely as imbalance in the potential outcomes across treatment groups. This is the sort of imbalance is unlikely with randomization, but it’s almost guaranteed in observational studies.\nIn other words, imbalance occurs if the distributions of confounders differ for the treatment and control groups.\nOverlap\nFigure 6.1 shows what lack of complete overlap (with respect to \\(x\\)) might look like:\n\n\n\n\n\n\n\n\n\n\n\n(a) Two distributions with no overlap\n\n\n\n\n\n\n\n\n\n\n\n(b) Two distributions with partial overlap\n\n\n\n\n\n\n\n\n\n\n\n(c) The range of one distribution is a subset of the range of the other.\n\n\n\n\n\n\n\nFigure 6.1: Lack of complete overlap in distributions across treatment and control groups. Dashed lines indicate distributions for the control group; solid lines indicate distributions for the treatment group.\n\n\n\nLack of complete overlap or “common support” creates problems because in this setting we have treatment or control observations for which we have no empirical counterfactuals. Thus, knowledge about treatment effects is inherently limited in regions of non-overlap. Any causal inference in Figure 6.1 (a) would rely on modeling assumptions instead of having direct support from the data. In Figure 6.1 (c) causal inference is possible for the full treatment group but only for a subset of the control group.\nNote. This is the exact same thing we talked about when thinking about the potential outcomes for a cervical cancer vaccine in a population for men and women.\n\n6.2.1 Exercise\nLooking for imbalance.\nLoad the cattaneo2.dta data that Steve showed us in class.\n\n\nCode\nd &lt;- haven::read_dta(\"data/cattaneo2.dta\")\n\nd &lt;- d |&gt;  \n  haven::zap_labels() |&gt;             \n  select(bweight, lbweight, mbsmoke, mmarried, mage, medu, fbaby, alcohol, mrace, nprenatal)\n\nglimpse(d)\n\n\nRows: 4,642\nColumns: 10\n$ bweight   &lt;dbl&gt; 3459, 3260, 3572, 2948, 2410, 3147, 3799, 3629, 2835, 3880, …\n$ lbweight  &lt;dbl&gt; 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, …\n$ mbsmoke   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, …\n$ mmarried  &lt;dbl&gt; 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, …\n$ mage      &lt;dbl&gt; 24, 20, 22, 26, 20, 27, 27, 24, 21, 30, 26, 20, 34, 21, 23, …\n$ medu      &lt;dbl&gt; 14, 10, 9, 12, 12, 12, 12, 12, 12, 15, 12, 12, 14, 8, 12, 12…\n$ fbaby     &lt;dbl&gt; 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, …\n$ alcohol   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ mrace     &lt;dbl&gt; 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, …\n$ nprenatal &lt;dbl&gt; 10, 6, 10, 10, 12, 9, 16, 11, 20, 9, 14, 5, 13, 8, 4, 10, 13…\n\n\nWe can start checking for imbalance for several covariates by examining their absolute standardized difference in means—i.e., a balance plot. I’ve included a graph that shows the absolute standardized difference in means values for a set of confounding covariates that might predict both mbsmoke and birth weight.\n\n\n\n\n\n\n\n\n\n\n\nYou’re job is to reproduce something close to this figure.\nWhat do you think are the most important covariates you need to adjust for in terms of the potential biases in the treatment effect?\n\n\n\n\n\n\n\n\n\nI used geom_segment(), but you can just use geom_point().\nAn open question is exactly which “standard deviation” to use. I used the standard deviations of the treated group, but you are not required to use that one. This means that your final plot doesn’t have to reproduce",
    "crumbs": [
      "Part 1",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Week 6</span>"
    ]
  },
  {
    "objectID": "week06.html#matching",
    "href": "week06.html#matching",
    "title": "6  Week 6",
    "section": "6.3 Matching",
    "text": "6.3 Matching\nThis question is copied from NHK’s exercises. To answer this question you need to read sections 14.1 and 14.2 of The Effect.\n\n6.3.1 Exercise\n\nYou want to know whether practicing cursive improves your penmanship (on a 1-10 scale). You find that, among people who don’t practice cursive, average penmanship is 5, 10 people are left-handed, 2 are ambidextrous, and 88 are right-handed. Among people who do practice cursive, 6 are left-handed with average penmanship 7, 4 are ambidextrous with average penmanship 4, and 90 are right-handed with average penmanship 6.\n\nYou want to create a set of weights that will make the treated group match the control group on handedness. Follow the process in section 14.2, paying attention to why certain numbers are going in certain positions. What weights will be given to the left, ambidextrous, and right-handed people in the control group?\nWhat weights will be given to the left, ambidextrous, and right-handed people in the treated group?\nUse the weights from part b to calculate the proportion of left-handed people in the treated group, as well as the proportion of ambidextrous people and the proportion of right-handed people. If you don’t get 10%, 2%, and 88% (or very close with some rounding error), your weights are wrong, try again.\nWhat is the weighted average penmanship score in the treated group?\nWhat is the effect of practicing cursive that we would estimate using this data?",
    "crumbs": [
      "Part 1",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Week 6</span>"
    ]
  },
  {
    "objectID": "week07.html",
    "href": "week07.html",
    "title": "7  Week 7",
    "section": "",
    "text": "7.1 Matching and Weighting\nNote. The following exercises where adapted from last year’s class.\nFor these exercises, we are going to use one of the versions of the “Lalonde data,” which is used in almost every paper on matching.1 This is data on a job training program (the treatment) that was intended to raise future earnings (the outcome).\nYou can load the data by typing the following:\nCode\nload(\"data/exercise_data.Rdata\")\nThis will bring two objects into the global environment: d_exper, which is the experimental subset of the data and d, which comprises the treated cases and a sample of observational controls from the PSID. The treatment is treat and the outcome is re78, which is income in $1000s. We are going to use the experimental subset to set an experimental benchmark and then see how close we can get to this benchmark using various matching and weighting methods.\nThe rest of the variables are as follows:\nBefore starting the exercises, you may want to consider a few things that will make your life easier:\nYou can get by without doing these steps but they avoid extra typing down the road.\nYou will begin by looking at the experimental data (d_exper). After that, you will conduct various forms of matching and weighting on the observational data (d). For each exercise after the first four, your basic workflow will be:",
    "crumbs": [
      "Part 1",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Week 7</span>"
    ]
  },
  {
    "objectID": "week07.html#matching-and-weighting",
    "href": "week07.html#matching-and-weighting",
    "title": "7  Week 7",
    "section": "",
    "text": "Variable\nDescription\n\n\n\n\nage\nAge in years\n\n\neduc\nYears of education\n\n\nblack\n1 = Black; 0 otherwise\n\n\nhisp\n1 = Hispanic; 0 otherwise\n\n\nmarried\n1 = married; 0 otherwise\n\n\nnodegr\n1 = no degree; 0 otherwise\n\n\nre74\n1974 income in $1000s\n\n\nre75\n1975 income in $1000s\n\n\nu74\n1 = no ’74 income; 0 otherwise\n\n\nu75\n1 = no ’75 income; 0 otherwise\n\n\n\n\n\nadd a factor version of the treatment to the data frame for easy plotting\ncreate formula objects that contain the propensity score (or matching) models with and without quadratic terms\n\n\n\n\nMatch or weight, as directed\nCheck balance (overall, if applicable and by covariate) using graphical and numeric means\nEstimate the ATT\n\n\n7.1.1 Exercise\nUse the experimental data to estimate the effect of the job training treatment. How much does it appear to affect 1978 income? Now look at the observational data (for all exercises from now on). How large is the raw difference in 1978 income between the treatment group and the PSID comparison group?\n\n\n7.1.2 Exercise\nTry to estimate the effect of the treatment using regression. What does regression say the effect of the program is?\n\n\n7.1.3 Exercise\nBegin by exact matching on all the dummy variables. How many treated cases cannot be matched? What is the (FS)ATT estimate?\n\n\n7.1.4 Exercise\nUse the observational data to estimate each case’s propensity to receive treatment using glm(). Use a logistic regression with quadratic terms for age, education, 1974 income, and 1975 income. Spend a few moments thinking about what this model says. Look at the density plots of the p-score for treated and untreated groups.\n\n\n7.1.5 Exercise\nEstimate propensity scores and ATT weights using weightit(). Ignore the warning you get. We’ll discuss that more in class. Estimate the ATT. Check for covariate balance.\n\n\n7.1.6 Exercise\nNow do the same as above using “entropy balancing.” Confirm that you’ve achieved balance on the means and the variances of the covariates. Estimate the ATT.\n\n\n\n\n\n\nTip\n\n\n\nDon’t worry (for now) if you don’t understand what this is; simply change the method argument to \"ebal\", we will cover this in class.\n\n\nCode\nOUTPUT &lt;- weightit(FORMULA, \n                   data = d,\n                   method = \"ebal\",\n                   moments = 3,\n                   estimand = \"ATT\")\n\n\n\n\n\n\n7.1.7 Bonus\nImplement a bootstrap of your preferred estimate. What is the bootstrapped standard error?",
    "crumbs": [
      "Part 1",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Week 7</span>"
    ]
  },
  {
    "objectID": "week07.html#footnotes",
    "href": "week07.html#footnotes",
    "title": "7  Week 7",
    "section": "",
    "text": "Lalonde, R. (1986). “Evaluating the econometric evaluations of training programs with experimental data.” American Economic Review 76: 604-620.↩︎",
    "crumbs": [
      "Part 1",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Week 7</span>"
    ]
  },
  {
    "objectID": "week08.html",
    "href": "week08.html",
    "title": "8  Week 8",
    "section": "",
    "text": "8.1 Evaluating a child care program\nThis dataset is taken from Gelman et al. (2020). The dataset contains measurements on the development of nearly 4500 children born in the 1980s.\nNote. We should all be grateful for Noah Greifer’s WeightIt package. You should skim this website to get a sense of how the R code for this kind of analysis used to look like in the old days!\nThe outcome variable will be ppvtr.36 (which simply means “test score at age 3”).1\nHere is the data dictionary:\nCode\ndict_url &lt;- \"https://raw.githubusercontent.com/avehtari/ROS-Examples/master/Childcare/data/datadict.txt\"\n\nread_file(dict_url) |&gt; \n  writeLines()\n\n\nData Dictionary for Child Care example\n\nThe IHDP intervention was implemented in the 1980’s and targeted low\nbirth weight (less than 2500 grams), pre-term children.  They were\nrecruited at the time of birth.  It provided high quality child care\nand other services in their first 3 years of life.  The comparison\ngroup was pulled from a survey conducted during the same time period\ncalled the National Survey of Longitudinal Youth.  At the time of the\nintervention many of the original survey participants (first recruited\nin 1979) had children. We have data from those children and their\nmothers for an overlapping set of variables (below) as the IHDP\nchildren.\n\nThe intervention for low-birth-weight children is described by\n- Brooks-Gunn, J., Liaw, F. R., and Klebanov, P. K. (1992). Effects of\n  early intervention on cognitive function of low birth weight preterm\n  infants. Journal of Pediatrics 120, 350–359.\n- Hill, J. L., Brooks-Gunn, J., and Waldfogel, J. (2003). Sustained\n  effects of high participation in an early intervention for\n  low-birth-weight premature infants. Developmental Psychology 39,\n  730–744.\n\nData columns\n\n\"momage\"   \nmom age at time of birth\n\n\"b.marr\"   \nindicator for whether mom was married at birth\n\n\"momed\"    \nmother’s education level at the time she gave birth\n\n\"work.dur\" \nindicator for whether mom worked in the year before she gave birth\n\n\"prenatal\" \nindicator for whether mom received prenatal care\n\n\"cig\"     \n indicator for whether mom smoked cigarettes while pregnant\n\n\"booze\"    \nindicator for whether mom drank alcohol while pregnant\n\n\"sex\"      \nindicator for whether child was born male or female\n\n\"first\"    \nindicator for whether child was the first born for the mother\n\n\"bw\"       \nchild’s birth weight\n\n\"bwg\"      \nindicator for whether child was born low birth weight\n\n\"preterm\" \nnumber of weeks preterm child was born\n\n\"black\", \"hispanic\", \"white\"    \nindicators for child’s race/ethnicity\n\n\"lths\", \"hs\", \"ltcoll\", \"college\"  \nindicators for mother’s education at time of birth\n\n\"dayskidh\" \nnumber of days child was in the hospital after being born\n\n\"st5\", \"st9\", \"st12\", \"st25\", \"st36\", \"st42\", \"st48\", \"st53\"    \nindicator for state where household resides \n\n\"st99\"  \nindicator for whether family  was living in state served by the ihdp\n\n\"income\"   \nfamily income one year after the child was born\n\n\"treat\"   \nindicator for whether family was allowed to receive IHDP services (1 = yes) \n\n\"ppvtr.36\"\nIQ measured at age 36 months\nAnd here is the data:\nCode\nvar_names &lt;- c(\"momage\", \"b.marr\", \"momed\", \"work.dur\", \"prenatal\", \"cig\", \"booze\", \"sex\", \"first\", \"bw\", \"bwg\", \"preterm\", \"black\", \"hispanic\", \"white\", \"lths\", \"hs\", \"ltcoll\", \"college\", \"dayskidh\", \"st5\", \"st9\", \"st12\", \"st25\", \"st36\", \"st42\", \"st48\", \"st53\", \"st99\", \"income\", \"treat\", \"ppvtr.36\")\n\nurl &lt;- \"https://raw.githubusercontent.com/avehtari/ROS-Examples/master/Childcare/data/cc2.csv\"\n\nd &lt;- read_csv(url) |&gt; \n  select(all_of(var_names)) |&gt; \n  mutate(across(matches(\"st\\\\d{2}\"), as.integer))\n\nglimpse(d)\n\n\nRows: 4,381\nColumns: 32\n$ momage   &lt;dbl&gt; 33, 22, 13, 25, 19, 19, 26, 20, 23, 28, 32, 23, 29, 18, 25, 1…\n$ b.marr   &lt;dbl&gt; 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1…\n$ momed    &lt;dbl&gt; 4, 1, 1, 4, 1, 2, 1, 1, 2, 2, 1, 2, 1, 1, 4, 1, 1, 1, 2, 1, 3…\n$ work.dur &lt;dbl&gt; 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1…\n$ prenatal &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n$ cig      &lt;dbl&gt; 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1…\n$ booze    &lt;dbl&gt; 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ sex      &lt;dbl&gt; 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1…\n$ first    &lt;dbl&gt; 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1…\n$ bw       &lt;dbl&gt; 1559, 2240, 1900, 1550, 2270, 1550, 2330, 2410, 1776, 2140, 2…\n$ bwg      &lt;dbl&gt; 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0…\n$ preterm  &lt;dbl&gt; 10, 3, 6, 8, 5, 4, 9, 3, 6, 5, 5, 7, 6, 10, 8, 6, 7, 6, 6, 6,…\n$ black    &lt;dbl&gt; 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0…\n$ hispanic &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ white    &lt;dbl&gt; 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1…\n$ lths     &lt;dbl&gt; 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0…\n$ hs       &lt;dbl&gt; 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0…\n$ ltcoll   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1…\n$ college  &lt;dbl&gt; 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0…\n$ dayskidh &lt;dbl&gt; 31, 4, 9, 50, 4, 13, 8, 6, 30, 2, 3, 27, 13, 24, 71, 1, 6, 4,…\n$ st5      &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n$ st9      &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ st12     &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ st25     &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ st36     &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ st42     &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ st48     &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ st53     &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ st99     &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ income   &lt;dbl&gt; 42500.0000, 5000.0000, 12500.0000, 42500.0000, 5000.0000, 125…\n$ treat    &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n$ ppvtr.36 &lt;dbl&gt; 111.00000, 81.00000, 92.00000, 103.00000, 81.00000, 94.00000,…",
    "crumbs": [
      "Part 1",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Week 8</span>"
    ]
  },
  {
    "objectID": "week08.html#evaluating-a-child-care-program",
    "href": "week08.html#evaluating-a-child-care-program",
    "title": "8  Week 8",
    "section": "",
    "text": "A subset of 290 of these children received special services in the first few years of life, including high-quality child care (five full days a week) in the second and third years of life as part of a formal intervention, the Infant Health and Development Program (IHDP). These children were targeted because they were born prematurely, had low birth weight (less than or equal to 2500 grams), and lived in the eight cities where the intervention took place. Children in the sample who did not receive the intervention exhibited a more representative range of birth timing and birth weight.\nWe want to evaluate the impact of this intervention on the children’s subsequent cognitive outcomes by comparing the outcomes for children in the intervention group to the outcomes in a comparison group of 4091 children who did not participate in the program. The outcome of interest is test score at age 3; this test is similar to an IQ measure, so we simplistically refer to these scores as IQ scores from now on.\npp. 394-5\n\n\n\n\n\n\n\n\n8.1.1 Exercise\nGelman et al. (2020) say: We excluded the most severely low-birth-weight children (those at or below 1500 grams) from the sample because they are so different from the comparison sample.\n\n\n\n\n\n\nNote\n\n\n\nIn your own words.\n\nWhy did they decide to exclude these children? What problem could we encounter by not omitting them?\nWould you have excluded them from the dataset?2 Why?\n\n\n\n\n\n8.1.2 Exercise\n\n\n\n\n\n\nNote\n\n\n\nLook at the variables.\nWhich ones are you planning to use for covariate balancing? Justify your answer, but keep it short!\n\n\n\n\n\n\n\n\nTip\n\n\n\nNote.\nDepending on what you end up doing you might need to reconsider this answer.\n\n\n\n\n8.1.3 Exercise\nNote. This exercise should take most of your time. Basically, you have to re-do the kind of analysis we did for last week’s homework and at the beginning of this week with Steve (here).\n\n\n\n\n\n\nNote\n\n\n\nUse the WeightIt package and try to achieve balance before estimating the ATT for the effect of this child care program.\nYou will have to do this three separate times, using the following:\n\nPropensity Scores\nCBPS\nEntropy Balancing\n\n\n\n\n\n\n\n\n\nGelman, Andrew, Jennifer Hill, and Aki Vehtari. 2020. Regression and Other Stories. Cambridge University Press.",
    "crumbs": [
      "Part 1",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Week 8</span>"
    ]
  },
  {
    "objectID": "week08.html#footnotes",
    "href": "week08.html#footnotes",
    "title": "8  Week 8",
    "section": "",
    "text": "It’s not an IQ test, wtf.↩︎\nI wouldn’t!↩︎",
    "crumbs": [
      "Part 1",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Week 8</span>"
    ]
  },
  {
    "objectID": "week09.html",
    "href": "week09.html",
    "title": "9  Week 9",
    "section": "",
    "text": "9.1 End of an Era\nThis homework is relatively straightforward, but it might take some time.",
    "crumbs": [
      "Part 1",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Week 9</span>"
    ]
  },
  {
    "objectID": "week09.html#end-of-an-era",
    "href": "week09.html#end-of-an-era",
    "title": "9  Week 9",
    "section": "",
    "text": "9.1.1 Exercise\nLoad the GSS dataset for 2022 and choose one outcome variable and one “treatment” variable. They can be any two variables you want and the treatment can be either binary or continuous, it shouldn’t matter.\n\n\nCode\nlibrary(gssr)\ngss2022 &lt;- gss_get_yr(2022)\n\n\nWhat is the “naive” estimate for the effect of \\(T\\) on \\(Y\\)?\n\n\n9.1.2 Exercise\nSome theory.\nThink very hard about the list of possible confounding variables that might affect this estimate.\n\nDraw a DAG.\nWhat is your estimand (ATE, ATT, ATU)?\n\n\n\n9.1.3 Exercise\nSelect the appropriate covariates in the GSS that allow for causal identification.\nBe very careful when selecting these variables and make a note for any transformation you decide is adequate—e.g., coding a covariate to be binary.\nAre there any variables missing from the DAG you drew earlier?\n\n\n9.1.4 Exercise\nUse regression to get the effect of \\(T\\) on \\(Y\\).\nNote. Remember that it’s relatively straightforward to get the ATT using the marginaleffects package.\n\n\n9.1.5 Exercise\nWeighting.\nSpend some time trying to achieve covariate balancing. You should at least show a “Love plot.”\nNote. Use any method you think is appropriate (e.g., propensity scores, CBPS, entropy balancing).\nWhat is the effect of \\(T\\) on \\(Y\\)?\n\n\n9.1.6 Exercise\nDouble Robustness.\nCombining weighting and regression, what is the effect of \\(T\\) on \\(Y\\)?\n\n\n9.1.7 Exercise\nWrite 3-5 paragraphs explaining your research question, the methods you used, and the answer you came up with.",
    "crumbs": [
      "Part 1",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Week 9</span>"
    ]
  },
  {
    "objectID": "week10.html",
    "href": "week10.html",
    "title": "10  Week 10",
    "section": "",
    "text": "10.1 Exercise\nReshaping data.\nTake a look at the gapminder dataset contained in the gapminder pakacage.\nThe following chunk of code uses the pivot_wider() function to turn this dataset into wider form.\nCode\ngap_wide &lt;- gapminder::gapminder |&gt; \n  select(continent, country, year, lifeExp, gdpPercap) |&gt; \n  pivot_wider(\n    names_from = year, \n    values_from = c(lifeExp, gdpPercap), \n    names_sep = \"\"\n  )\n\ngap_wide\n\n\n# A tibble: 142 × 26\n   continent country lifeExp1952 lifeExp1957 lifeExp1962 lifeExp1967 lifeExp1972\n   &lt;fct&gt;     &lt;fct&gt;         &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;\n 1 Asia      Afghan…        28.8        30.3        32.0        34.0        36.1\n 2 Europe    Albania        55.2        59.3        64.8        66.2        67.7\n 3 Africa    Algeria        43.1        45.7        48.3        51.4        54.5\n 4 Africa    Angola         30.0        32.0        34          36.0        37.9\n 5 Americas  Argent…        62.5        64.4        65.1        65.6        67.1\n 6 Oceania   Austra…        69.1        70.3        70.9        71.1        71.9\n 7 Europe    Austria        66.8        67.5        69.5        70.1        70.6\n 8 Asia      Bahrain        50.9        53.8        56.9        59.9        63.3\n 9 Asia      Bangla…        37.5        39.3        41.2        43.5        45.3\n10 Europe    Belgium        68          69.2        70.2        70.9        71.4\n# ℹ 132 more rows\n# ℹ 19 more variables: lifeExp1977 &lt;dbl&gt;, lifeExp1982 &lt;dbl&gt;, lifeExp1987 &lt;dbl&gt;,\n#   lifeExp1992 &lt;dbl&gt;, lifeExp1997 &lt;dbl&gt;, lifeExp2002 &lt;dbl&gt;, lifeExp2007 &lt;dbl&gt;,\n#   gdpPercap1952 &lt;dbl&gt;, gdpPercap1957 &lt;dbl&gt;, gdpPercap1962 &lt;dbl&gt;,\n#   gdpPercap1967 &lt;dbl&gt;, gdpPercap1972 &lt;dbl&gt;, gdpPercap1977 &lt;dbl&gt;,\n#   gdpPercap1982 &lt;dbl&gt;, gdpPercap1987 &lt;dbl&gt;, gdpPercap1992 &lt;dbl&gt;,\n#   gdpPercap1997 &lt;dbl&gt;, gdpPercap2002 &lt;dbl&gt;, gdpPercap2007 &lt;dbl&gt;",
    "crumbs": [
      "Part 1",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Week 10</span>"
    ]
  },
  {
    "objectID": "week10.html#exercise",
    "href": "week10.html#exercise",
    "title": "10  Week 10",
    "section": "",
    "text": "Exercise:\n\nUse the panelr package to turn the dataset back to its original long form and save it as gap_long.\nUse the panelr package to turn the gap_long into gap_wide, essentially replicating what I did earlier with pivot_wider()\n\n\n\n\n\n\n\n\n\n\nHint: The functions you are looking for are called long_panel() and widen_panel().",
    "crumbs": [
      "Part 1",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Week 10</span>"
    ]
  },
  {
    "objectID": "week10.html#exercise-1",
    "href": "week10.html#exercise-1",
    "title": "10  Week 10",
    "section": "10.2 Exercise",
    "text": "10.2 Exercise\n\n\n\n\n\n\nSlide 33 contains a very simple visualization made using the line_plot() function.\nTry your best to make a similar graph for the gapminder dataset, with year on the x-axis and lifeExp on the y-axis for a random subset of 10 countries.\nUse ggplot2, do not use line_plot()",
    "crumbs": [
      "Part 1",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Week 10</span>"
    ]
  },
  {
    "objectID": "week10.html#exercise-2",
    "href": "week10.html#exercise-2",
    "title": "10  Week 10",
    "section": "10.3 Exercise",
    "text": "10.3 Exercise\nICC\n\n\n\n\n\n\nSteve introduced the measurement of “intra class correlation” (ICC) in class.\nWhat is the ICC for lifeExp, pop, and gdpPercap in the gapminder dataset?",
    "crumbs": [
      "Part 1",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Week 10</span>"
    ]
  },
  {
    "objectID": "week10.html#exercise-3",
    "href": "week10.html#exercise-3",
    "title": "10  Week 10",
    "section": "10.4 Exercise",
    "text": "10.4 Exercise\n\n\n\n\n\n\nUse the WageData from the panelr package. You don’t need to make a panel_data version of WageData for this analysis, but you can if you want. We will use it later. Estimate the following mixed models using lmer() with maximum likelihood (REML = FALSE):\n\nLog wage as a function of college and linear time\nAs #1, plus a random slope on time\nAs #2, but with time as a quadratic\n\nYou may get some warning messages. The correct specifications of these models fit well, but you can use lme4::allFit() if you really want to make sure that you’re getting trustworthy results.\nOnce you have estimated the models, compare their BIC values using BIC(). Select the best model (here, the one with the lowest BIC) then do the following:\n\nReport the estimated effect of college on log wages given the data and model. You can get this using tidy(), summary(), or any other function you prefer.\nUse ggpredict() |&gt; plot() (or another approach if you like) to plot predictions for a sample of 9 individuals over time.\n\n\n\nCode\ndata(\"WageData\", package = \"panelr\")\n\nWageData &lt;- WageData |&gt; \n  mutate(\n    college = if_else(ed &gt;= 16, 1L, 0L),  # college variable\n    t0 = t - 1                            # start time at 0\n  )\n\n\nThe model output that comes from the lmer() looks very different from the ones produced by lm() and glm(). Take note of anything you don’t understand in the output and be ready to ask questions in class about the things you don’t understand.",
    "crumbs": [
      "Part 1",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Week 10</span>"
    ]
  },
  {
    "objectID": "week11.html",
    "href": "week11.html",
    "title": "11  Week 11",
    "section": "",
    "text": "11.1 Exercise\nThis week’s homework is taken directly from NHK (Chapter 18) for the most part.",
    "crumbs": [
      "Part 1",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Week 11</span>"
    ]
  },
  {
    "objectID": "week11.html#exercise",
    "href": "week11.html#exercise",
    "title": "11  Week 11",
    "section": "",
    "text": "In the Event Studies chapter we estimated the effect of something that occurs at a specific time by just comparing before-event to after-event, without really using a control group. What assumption is made by no-control-group event studies that we don’t have to make with difference-in-differences?",
    "crumbs": [
      "Part 1",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Week 11</span>"
    ]
  },
  {
    "objectID": "week11.html#exercise-1",
    "href": "week11.html#exercise-1",
    "title": "11  Week 11",
    "section": "11.2 Exercise",
    "text": "11.2 Exercise\n\nWhich of the following potential back doors is controlled for by comparing the treated group to a control group?\n\nThe treated group may be following a trend, unique to the group, that would make the outcome change from before-treatment to after-treatment anyway\nThere may be events affecting everyone that would change the outcome from before-treatment to after-treatment anyway\nThere may be differences in typical outcome levels between the treated group and the untreated group\nThe decision to treat the treated group, rather than some other group, may be based on factors that are related to the outcome",
    "crumbs": [
      "Part 1",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Week 11</span>"
    ]
  },
  {
    "objectID": "week11.html#exercise-2",
    "href": "week11.html#exercise-2",
    "title": "11  Week 11",
    "section": "11.3 Exercise",
    "text": "11.3 Exercise\n\nConsider a treatment and control group. Looking only at the pre-treatment period, they have exactly the same outcomes (zero gap between them in each period).\n\nDespite having exactly the same outcomes pre-treatment, it happens to be the case that parallel trends is violated for these two groups. How is this possible? Explain what it means for parallel trends to be violated in this case, or give an example of how it could be violated.\nIf we estimate the causal effect in this case using difference-in-differences, even though parallel trends is violated, how much would our effect be off by? (note you won’t be able to give a specific number)",
    "crumbs": [
      "Part 1",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Week 11</span>"
    ]
  },
  {
    "objectID": "week11.html#exercise-3",
    "href": "week11.html#exercise-3",
    "title": "11  Week 11",
    "section": "11.4 Exercise",
    "text": "11.4 Exercise\n\nConsider the below graph showing the average outcome for treated and control groups in the lead up to treatment (indicated by the dashed line), and also after treatment\n\n\n\n\n\n\n\n\nBased on the prior trend, does it seem likely that parallel trends holds in this instance?\nIf we estimate difference-in-differences anyway, are we likely to overestimate the actual causal effect, underestimate it, or get it right on average?",
    "crumbs": [
      "Part 1",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Week 11</span>"
    ]
  },
  {
    "objectID": "week11.html#exercise-4",
    "href": "week11.html#exercise-4",
    "title": "11  Week 11",
    "section": "11.5 Exercise",
    "text": "11.5 Exercise\n\nIn mid-2020, during the COVID-19 pandemic, different countries pursued different courses of action. Some locked down fully, imposing harsh penalties to most people for leaving the house outside certain proscribed times. Some were looser and only suggested staying at home, and some had hardly any restrictions at all. You notice that COVID rates tend to spike dramatically in different countries at seemingly-random times, and want to know if certain restrictions helped.\nFrom March through May 2020, US and Canada COVID case rates followed similar trends (US rates were higher, but the trends were similar). You want to look at the effect of COVID restrictions enacted in Canada in late May 2020 on case rates. Is DID, with the US as a control group, a good way to estimate this effect? If not, what concerns would you have about this research design?",
    "crumbs": [
      "Part 1",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Week 11</span>"
    ]
  },
  {
    "objectID": "week11.html#exercise-5",
    "href": "week11.html#exercise-5",
    "title": "11  Week 11",
    "section": "11.6 Exercise",
    "text": "11.6 Exercise\n\nConsider the below table of mean outcomes, and calculate the difference-in-difference effect of treatment. Write out the equation you used to calculate it (i.e. show how the four numbers in the table are combined to get the estimate)\n\n\n\n\n\nBefore\nAfter\n\n\nTreated\n5\n9\n\n\nUntreated\n6\n7.5",
    "crumbs": [
      "Part 1",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Week 11</span>"
    ]
  },
  {
    "objectID": "week11.html#exercise-6",
    "href": "week11.html#exercise-6",
    "title": "11  Week 11",
    "section": "11.7 Exercise",
    "text": "11.7 Exercise\n\nYou are planning to estimate whether voter-protection laws increase voter turnout. You note that, in 2015, a lot of new voter-protection laws were enacted in some provinces but not in others. Conveniently, no new laws were enacted in 2012, 2014, or 2016, so you decide to use 2012 and 2014 as your “before” periods and 2016 as “after”.\n\nWhich of the following best describes what you’d want to regress state-and-year level “voter turnout” measures on?\n\nAn indicator for whether the state is treated, and an indicator for whether the year is 2016.\nA set of fixed effects for state, and a set of fixed effects for year.\nAn indicator for whether the state is treated, a set of fixed effects for year, and an indicator for whether the state is currently treated.\nA set of fixed effects for state, and for year, and an interaction between “is 2016” and “is a treated state”.\nThis design should not be estimated using a regression.\n\nUnless you chose the final option in the previous question, specify which coefficient in that regression would give you the DID estimate.",
    "crumbs": [
      "Part 1",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Week 11</span>"
    ]
  },
  {
    "objectID": "week11.html#exercise-7",
    "href": "week11.html#exercise-7",
    "title": "11  Week 11",
    "section": "11.8 Exercise",
    "text": "11.8 Exercise\nNot from NHK.\nIn your own words, describe what is the “two-way fixed effects difference-in-difference estimator.” What does this model assume about the effect of some treatment over time?\nYou might need to re-read this section:\nhttps://theeffectbook.net/ch-DifferenceinDifference.html#long-term-effects",
    "crumbs": [
      "Part 1",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Week 11</span>"
    ]
  },
  {
    "objectID": "week11.html#exercise-8",
    "href": "week11.html#exercise-8",
    "title": "11  Week 11",
    "section": "11.9 Exercise",
    "text": "11.9 Exercise\n\nConsider the below graph with estimates from a dynamic difference-in-differences model for a treatment that occurs between periods 4 and 5, with 95% confidence intervals shown.\n\n\n\n\n\n\n\n\nWhat about this graph might make us concerned about our identification assumptions?\nIgnoring any concerns we have, what would we say is the effect of treatment on Y in this case? (note the height of the line in period 5 is about 3, in period 6 is about 1, and in period 7 is about .5).",
    "crumbs": [
      "Part 1",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Week 11</span>"
    ]
  },
  {
    "objectID": "week11.html#exercise-9",
    "href": "week11.html#exercise-9",
    "title": "11  Week 11",
    "section": "11.10 Exercise",
    "text": "11.10 Exercise\nThis exercise is also taken directly from NHK.\n\nOne. In this assignment we will be walking through a very simple application of difference-in-differences that comes from Peter Nencka. In particular, it seemed that the beginning of the COVID-19 pandemic led to a brief craze for homemade sourdough bread, as people had to stay home, and stores were out of yeast (sourdough can be made at home using yeast from the air and does not require store-bought yeast). We will be estimating whether COVID lockdowns actually increased interest in sourdough bread,\nWe will be measuring interest in sourdough bread using Google Trends data in the USA. Google Trends tracks the popularity of different search terms over time. We will be comparing the popularity of the search term “sourdough” against the control groups: the search terms “cereal,” “soup,” and “sandwich,” the popularity of which we suspect might not have been meaningfully affected by COVID lockdowns.\n\nThis is the data:\n\n\nCode\nlibrary(tidyverse)\n\nurl &lt;- \"https://raw.githubusercontent.com/NickCH-K/TheEffectAssignments/main/sourdough_trends.csv\"\n\nsr &lt;- read_csv(url) |&gt; \n  select(date, keyword, hits) |&gt; \n  mutate(\n    date = as.Date(date),\n    keyword = factor(keyword)\n  )\n\nglimpse(sr)\n\n\nRows: 856\nColumns: 3\n$ date    &lt;date&gt; 2020-01-01, 2020-01-02, 2020-01-03, 2020-01-04, 2020-01-05, 2…\n$ keyword &lt;fct&gt; sourdough, sourdough, sourdough, sourdough, sourdough, sourdou…\n$ hits    &lt;dbl&gt; 3, 2, 3, 3, 3, 2, 2, 2, 2, 3, 4, 3, 2, 2, 2, 2, 3, 4, 3, 4, 2,…\n\n\n\nTwo. Make a line graph with date on the x-axis and hits on the y-axis, with a separate line for each keyword. Also add a vertical line for the “start of the pandemic” which we’ll decide for our purposes is March 15, 2020.\n\nHint. You’ll need to add geom_vline(xintercept = as.Date(\"2020-03-15\")).\n\nThree. Looking at your graph, comment on (a) whether it looks like the lockdown had an effect on the popularity of sourdough, (b) the shape that effect takes (i.e. is it a permanent increase in popularity? Temporary?), (c) whether you might be concerned about any of the control groups we’ve chosen\nFour. Create a “Treated” indicator that’s equal to 1 for sourdough and 0 otherwise (or True/False, either way). Do a test of whether the prior trends (keeping March 15 as the “treatment date”) differ between the treated and control groups, using a linear trend and doing a statistical significance test at the 95% level. Then, if you were concerned about any of the control groups in question 3c, drop any you were concerned about (and keep them dropped for the rest of the assignment) and rerun the test.\n\nNote. NHK refers to this kind of test as a “placebo test.” We are just trying to increase our confidence in the parallel trends assumption.\n\nWrite a line commenting on whether you can reject equal prior trends in your model(s).\nFive. Create a month variable by shifting the date variable back 15 days (so that the treatment day is the first day of the month) and then taking the month of the resulting date. Also create an After variable equal to 1/0 (or True/False) if the date is March 15 or afterwards.\nThen, take a look at the values of month you get and how they line up with date, and subtract a number from month so that the last period just before treatment (Feb 16-Mar 14) is 0. (Also, change the Jan 1-14 month so it’s one less than the Jan 15-Feb 14 month)\n(You can then use -lubridate::days() to subtract days from the date, and lubridate::month() to get the month from the date.)\nThen, use two-way fixed effects to estimate the difference-in-difference estimate of the effect of lockdown on sourdough popularity with keyword and month fixed effects, and standard errors clustered at the keyword level.\n\n\n\n\n\n\n\nLast class we had a little confusion with the p-values reported by the fixest::feols(). We will figure this out next week, so don’t worry too much about them.\n\n\n\n\n\n\n\n\n\nHint: This data-wrangling bit is trickier than it seems. Don’t feel discouraged!\n\n\n\n\n\n\n\n\n\nThe chapter introduces dynamic treatment effects, which where briefly discussed by Steve. One of the reasons fixest is becoming a popular package is because it makes estimating these models very easy, although it introduces a special syntax.\nThis is how we would estimate a difference-in-difference model allowing the effect to differ by month (using month = 0 as a reference period), with standard errors clustered at the keyword level.\n\n\nCode\ndynamic &lt;- feols(\n  hits ~ i(month, Treated, ref = 0) | keyword + month,\n  cluster = \"keyword\",\n  data = sr\n)\n\ncoefplot(dynamic)",
    "crumbs": [
      "Part 1",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Week 11</span>"
    ]
  },
  {
    "objectID": "sec-summary.html",
    "href": "sec-summary.html",
    "title": "12  Topics",
    "section": "",
    "text": "What is your estimand?\n\nestimands, estimators, estimates\n\nIdentification and causal diagrams\n\nfront doors\nback doors\ncolliders\npost-treatment bias\n\nMultiple regression and adjustment\n\nintepreting effect sizes\nusing marginaleffects\nfunctional form considerations\n\nMatching and weighting\n\nexact matching\npropensity scores (parametric and semi-parametric)\nnearest neighbor matching on the propensity score for ATT\nweighting using propensity scores for ATT and ATE\nweighting using non-propensity methods for ATT and ATE\n\nPanel data concepts\n\nwithin and between variance\nvariance components\nICC calculation\n\nMixed model for time-constant treatments\n\nadvantages over linear regression\nrandom growth curve models\nshrinkage\n\nWithin-subject models\n\nadvantages for estimating counterfactuals\ndisadvantages for generalization\npre-post\ntwo-period difference-in-differences\ntwo-way fixed effects\n\nWithin-between / correlated random effects models\n\nsplitting variables\ncombining strengths of mixed- and within-subject models\ninterpreting within, between, and contextual coefficients",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Topics</span>"
    ]
  },
  {
    "objectID": "glossary.html",
    "href": "glossary.html",
    "title": "Glossary",
    "section": "",
    "text": "Estimand\nJoint Submission. My comments are in the shaded boxes.\nThe parameter or quantity of interest that one aims to estimate in a statistical analysis. It defines what effect or measure is being targeted by the analysis, specifying how it relates to the variables and data involved.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Glossary</span>"
    ]
  },
  {
    "objectID": "glossary.html#counterfactual",
    "href": "glossary.html#counterfactual",
    "title": "Glossary",
    "section": "Counterfactual",
    "text": "Counterfactual\nThe counterfactual compares the observed results of something that did happen to the expected results if the treatment had not happened.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Glossary</span>"
    ]
  },
  {
    "objectID": "glossary.html#potential-outcomes",
    "href": "glossary.html#potential-outcomes",
    "title": "Glossary",
    "section": "Potential Outcomes",
    "text": "Potential Outcomes\nThe set of all possible outcomes for each individual in a study, under each possible treatment or intervention scenario. These outcomes reflect what could happen to each individual under different conditions.\n\nNote.\nI think it’s better to think of “potential outcomes.”\nFor example, in the case of a binary treatment, the potential outcomes framework involves the following:\n\n\\(T\\) is a treatment variable. The terms “treatment” and “cause” are used interchangeably.\n\\(Y\\) is the outcome we observe.\n\\(Y^0\\) is the the value the outcome would take if \\(T=0\\).\n\\(Y^1\\) is the value the outcome would take if \\(T=1\\).\n\\(Y^0\\) and \\(Y^1\\) are the potential outcomes.\nWe see \\(Y^0\\) or \\(Y^1\\) for the same unit, but never both.\nThis is the fundamental problem of causal inference.\nWhen \\(T=1\\), \\(Y^0\\) is the counterfactual.\nWhen \\(T=0\\), \\(Y^1\\) is the counterfactual.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Glossary</span>"
    ]
  },
  {
    "objectID": "glossary.html#theoretical-estimand",
    "href": "glossary.html#theoretical-estimand",
    "title": "Glossary",
    "section": "Theoretical Estimand",
    "text": "Theoretical Estimand\nThis is the actual thing we would like to know in our research question. The two components of the theoretical estimand are the unit-specific quantity and the target population. The theoretical estimand includes both observed and unobserved data (including counterfactuals).",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Glossary</span>"
    ]
  },
  {
    "objectID": "glossary.html#empirical-estimand",
    "href": "glossary.html#empirical-estimand",
    "title": "Glossary",
    "section": "Empirical Estimand",
    "text": "Empirical Estimand\nThe empirical estimand is the target of inference that only includes observable data and relies on identification assumptions.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Glossary</span>"
    ]
  },
  {
    "objectID": "glossary.html#causal-interaction",
    "href": "glossary.html#causal-interaction",
    "title": "Glossary",
    "section": "Causal Interaction",
    "text": "Causal Interaction\nA causal interaction is the intervention to two variables averaged over one population. In other words, the effect of one variable on the outcome is related to the effect of another.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Glossary</span>"
    ]
  },
  {
    "objectID": "glossary.html#baseline-bias",
    "href": "glossary.html#baseline-bias",
    "title": "Glossary",
    "section": "Baseline Bias",
    "text": "Baseline Bias\nWhen the treatment and the control group differ from one another for reasons other than the treatment they receive.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Glossary</span>"
    ]
  },
  {
    "objectID": "glossary.html#average-treatment-effect-ate",
    "href": "glossary.html#average-treatment-effect-ate",
    "title": "Glossary",
    "section": "Average Treatment Effect (ATE)",
    "text": "Average Treatment Effect (ATE)\nThe average treatment effect across the population. In other words, this is the effect of switching treatment statuses (e.g., from either treatment to control or from control to treatment).",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Glossary</span>"
    ]
  },
  {
    "objectID": "glossary.html#conditional-average-treatment-effect-cate",
    "href": "glossary.html#conditional-average-treatment-effect-cate",
    "title": "Glossary",
    "section": "Conditional Average Treatment Effect (CATE)",
    "text": "Conditional Average Treatment Effect (CATE)\nThe average treatment effect among a specific group as defined by certain control variables covariates.\nFor example, if the control variable covariate is \\(X\\), the CATE can be written as \\(E(Y^1-Y^0 \\mid X = x)\\).\n\nNote.\nThe word “control” has some baggage because it has the connotation that we can somehow manipulate \\(X\\). Here, we’d usually want to condition on variables like “gender” or “race.”",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Glossary</span>"
    ]
  },
  {
    "objectID": "glossary.html#average-treatment-effect-on-the-treated-att",
    "href": "glossary.html#average-treatment-effect-on-the-treated-att",
    "title": "Glossary",
    "section": "Average Treatment Effect on the Treated (ATT)",
    "text": "Average Treatment Effect on the Treated (ATT)\nThe average treatment effect, calculated only from the treatment group. To conceptualize the ATT, imagine taking away treatment from those who were treated and measuring the change.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Glossary</span>"
    ]
  },
  {
    "objectID": "glossary.html#average-treatment-effect-on-the-untreated-atu",
    "href": "glossary.html#average-treatment-effect-on-the-untreated-atu",
    "title": "Glossary",
    "section": "Average Treatment Effect on the Untreated (ATU)",
    "text": "Average Treatment Effect on the Untreated (ATU)\nThe average treatment effect, calculated only from the untreated group. To conceptualize the ATU, imagine giving the treatment to those who were not treated and measuring the change.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Glossary</span>"
    ]
  },
  {
    "objectID": "glossary.html#unit-specific-quantity",
    "href": "glossary.html#unit-specific-quantity",
    "title": "Glossary",
    "section": "Unit-Specific Quantity",
    "text": "Unit-Specific Quantity\nA measurement or outcome that pertains to an individual unit or subject in a study, often used in contexts where the effect of a treatment may vary from one unit to another.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Glossary</span>"
    ]
  },
  {
    "objectID": "glossary.html#treatment-effect-heterogeneity",
    "href": "glossary.html#treatment-effect-heterogeneity",
    "title": "Glossary",
    "section": "Treatment Effect Heterogeneity",
    "text": "Treatment Effect Heterogeneity\nWhen a treatment has different effects on a population.\nFor, example, a treatment for cervical cancer will have heterogeneous effects on people with cervixes versus people without cervixes, even if both groups in the population receive identical treatments. This is because people without cervixes will receive no treatment effect from drugs that target an organ they do not have.\n\n\nNote.\nThis is an extreme example. You’d usually want to remove men from design (i.e., not administering the treatment to people without cervixes).\nIn a less extreme example, the treatment for cervical cancer could have varying effects across women belonging to different age groups.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Glossary</span>"
    ]
  },
  {
    "objectID": "glossary.html#directed-acyclic-graph-dag",
    "href": "glossary.html#directed-acyclic-graph-dag",
    "title": "Glossary",
    "section": "Directed Acyclic Graph (DAG)",
    "text": "Directed Acyclic Graph (DAG)\nA tool used to depict the data-generating process with nodes to represent the variable and the arrows to represent the causal relationships.\n\nCausal diagrams or DAGs are graphical representations of a data generating process. Everything we draw is hopefully an informed assumption; everything that’s not in the diagram is also an assumption. In other words, DAGs encode identifying assumptions.\nThe idea of a directed acyclical graph (DAG) implies that there are no cycles. If a variable causes itself, it’s near impossible to isolate or identifying the cause of anything. The world is full with feedback loops of all sorts, but we deal with them through the incorporation of time or by isolating one effect through some kind of experimental scenario.\nNote that DAGs are agnostic about functional form. This includes interactions among variables! Some people deal with interactions by drawing arrows toward arrows or by representing interactions explicitly as separate nodes.\nThe nicest thing about DAGs is that they help us spell out the testable implications of our assumptions. For example, if our causal diagram implies that a relationship between variables is zero, we can check for that.\n\n\nUnobserved / Unmeasured Variable\nA variable that may be present in the researcher’s assumptions about how the world works, but that is not addressed or available in the data.\n\nNote.\nIn a DAG, these variables are usually depicted by being enclosed within a circle.\n\n\n\nPath\nA path is simply the steps from one variable to another on a causal diagram.\nFor example, \\(A \\to B \\to C\\) is a path.\n\n\nDirect Effects\nThe effect of only the treatment variable on the outcome variable.\n\n\nIndirect Effects\nThe portion of the change in an outcome that is mediated through one or more intermediate variables, reflecting the causal influence exerted through these mediating variables.\n\n\nTotal Effects\nThe sum of direct and indirect effects.\n\n\nFront-Door Path\nA causal path in which all of the arrows in the causal diagram point from the treatment variable and towards the outcome variable.\n\n\nBack-Door Path\nThe rest of the paths on a causal diagram [connecting treatment to outcome] that are not front-door paths.\n\n\nConfounding Variable\nA variable that affects both the treatment and outcome variables.\nNote: Not adjusting for a confounding variable threatens the causal relationship between the treatment and outcome variables.\n\n\nNote.\nThe expression “threatens the causal relationship” is vague and clunky. A better way to say this is that the causal effect is not identified when we don’t adjust for confounding paths.\nAn even better way to talk about confounding is to realize that “confounding” is a property of paths, it’s NOT a property of variables.\n\n\n\nCollider Variable\nA variable in a DAG that is influenced by two or more other variables. Conditioning on a collider can open a confounding path, inadvertently introducing bias into the analysis.\n\nA variable is a collider in a path iff both arrows point at it.\n\\[\na \\to b \\to c_\\text{ollider} \\leftarrow d \\leftarrow e \\to f\n\\]\nHere, \\(b\\) and \\(c\\) are unrelated unless we remove variation in \\(c\\) (e.g., by including it in a regression).\nWe close paths by removing variation from one variable along the path (i.e., adjusting); but if the variable is a collider, then removing variation will actually open a path that was already closed.\nAn often unacknowledged way of adjusting for colliders is during the sample selection phase. If we have a sample of college students, it means we are adjusting for college attendance.\n\n\n\nOpen Path\nA path on a causal diagram in which there is variation in all variables along the path (and no variation in any colliders on that path).\n\n\nClosed Path\nA path on a causal diagram in which one or more variables has no variation.\nNote: A path is also considered closed if there is a collider variable present on that path.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Glossary</span>"
    ]
  },
  {
    "objectID": "glossary.html#regression",
    "href": "glossary.html#regression",
    "title": "Glossary",
    "section": "Regression",
    "text": "Regression\nRegression focuses on estimating the effect of variables on an outcome through a mathematical model, assuming a specific form of relationship.\n\nNote.\nThis is NOT a good definition!!\nIn the usual context of regression, predictive inference relates to comparisons between units. In the context of causal inference, we attempt to make comparisons of different treatments as if applied to the same units.\nIn order to make causal interpretations of regression coefficients we rely very strong assumptions. In short, causal effects can be estimated with regression if the model includes all confounding variables and if the “functional form” is correct.\nTranslating DAGs into Regression\n\n\n\n\n\n\nFigure 13.1: Flow Chart For Constructing Regression Equations [@huntington-klein2021, pp. 199]",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Glossary</span>"
    ]
  },
  {
    "objectID": "glossary.html#matching-and-weighting",
    "href": "glossary.html#matching-and-weighting",
    "title": "Glossary",
    "section": "Matching and Weighting",
    "text": "Matching and Weighting\nMatching and weighting seeks to identify the treatment effect by adjusting for selection into treatment by comparing similar treatment and control cases based on certain attributes.\nNote: This is distinct from regression, which adjusts for variables that impact the outcome.\n\nBoth regression and matching/weighting are strategies to close the backdoor path between \\(T\\) and \\(Y\\). Both are conditional on observable covariates. However, matching/weighting provides a way to model treatment selection so that everyone looks the same on the pre-treatment covariates.\nMatching/weighting refers to a set of procedures that modify the original sample in preparation for a statistical analysis. It’s a form of data pre-processing. The goal is to create a sample that looks like it was created from a randomized experiment.\nHere are some benefits of matching/weighting procedures:\n\nThey are less restrictive about functional form than regression. We rely less on parametric assumptions. The intuition is the same as with an experiment: if we can create sufficient overlap and balance between treatment and control groups, then we should get a reasonable estimate of the treatment effect, even if the model used to estimate it is misspecified.\nRegression models on pre-processed data gives us two opportunities to close backdoor paths (i.e., “double robustness”).",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Glossary</span>"
    ]
  },
  {
    "objectID": "glossary.html#exact-matching",
    "href": "glossary.html#exact-matching",
    "title": "Glossary",
    "section": "Exact Matching",
    "text": "Exact Matching\nExact matching compares treatment and control cases that have exactly identical characteristics on a certain combination of variables. In this way, treatment cases are matched to control cases that share the exact same distribution of values for the matching variable(s) of interest, making the only difference between them is treatment status.\nThe three assumptions for exact matching are as follows:\n\nSelection of variables-conditional independence assumption.\nOverlap (any individual case has a non-zero probability of treatment).\nStable unit treatment value assumption (SUTVA; treatment status and effect of treatment is independent for each case).\n\n\n\nNote.\nThese assumptions are NOT unique to matching.\nA better discussion of overlap can be found here or in one of the textbooks.\nThis brief discussion from the MatchIt vignette is also useful:\nhttps://kosukeimai.github.io/MatchIt/articles/matching-methods.html#exact-matching-method-exact",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Glossary</span>"
    ]
  },
  {
    "objectID": "glossary.html#conditional-independence-assumption-cia",
    "href": "glossary.html#conditional-independence-assumption-cia",
    "title": "Glossary",
    "section": "Conditional Independence Assumption (CIA)",
    "text": "Conditional Independence Assumption (CIA)\nThe conditional independence assumption asserts that two events occur independent of one another (i.e., the two events do not influence one another).\n\nThe CIA (also known as “conditional ignorability” assumption) builds on the more general idea of ignorability.\nExperiments work because they make the distribution of potential outcomes the same across levels of the treatment variable. In other words, the potential outcomes and the treatment indicator are independent.\n\\[\nY^0, Y^1 \\perp T\n\\]\nHere, the \\(\\perp\\) symbol means independent.\nThe CIA is a necessary assumption we make when using both regression and matching/weighting to identify a causal effect.\nInstead of a simple independence assumption that we have for randomized experiments, we now have to rely on a conditional ignorability. Just like in the case of experiments, we want distribution of potential outcomes the same across levels of the treatment variable. In other words, the potential outcomes and the treatment indicator should be independent, conditional on the covariates \\(\\boldsymbol{X}\\) used in the analysis.\n\\[\nY^0, Y^1 \\perp T \\mid \\boldsymbol X\n\\]\nHere, \\(\\boldsymbol{X}\\) is meant to depict a collection of variables that will close the back-door path.\nThis strategy will get more complicated as the vector \\(\\boldsymbol X\\) grows in size.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Glossary</span>"
    ]
  },
  {
    "objectID": "glossary.html#region-of-common-support",
    "href": "glossary.html#region-of-common-support",
    "title": "Glossary",
    "section": "Region of Common Support",
    "text": "Region of Common Support\nThis is the assumption that either for distribution of a matching variable (in exact matching) or for distribution of propensity scores (in weighting), there are both treatment and control cases. If there is overlap, this is known as the region of common support. If there is not overlap in the distributions, that is “off support” and makes causal inference a little more tricky.\n\nNote.\nWhat makes it “tricky” is that we don’t have direct support from the data to estimate the relevant counterfactuals—e.g., regression will just extrapolate.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Glossary</span>"
    ]
  },
  {
    "objectID": "glossary.html#feasible-estimate-of-treatment-effects",
    "href": "glossary.html#feasible-estimate-of-treatment-effects",
    "title": "Glossary",
    "section": "Feasible Estimate of Treatment Effects",
    "text": "Feasible Estimate of Treatment Effects\nWhen calculating a treatment effect, it is sometimes necessary to drop cases that fall outside of the “region of common support.” Because cases are dropped, the “true” treatment effect cannot be calculated, which leaves us with calculating the “feasible” treatment effect instead.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Glossary</span>"
    ]
  },
  {
    "objectID": "glossary.html#propensity-score",
    "href": "glossary.html#propensity-score",
    "title": "Glossary",
    "section": "Propensity Score",
    "text": "Propensity Score\nA subject’s individual probability of receiving treatment. Propensity scores can then be used to match or weight control cases to mimic the treatment group.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Glossary</span>"
    ]
  },
  {
    "objectID": "glossary.html#doubly-robust",
    "href": "glossary.html#doubly-robust",
    "title": "Glossary",
    "section": "Doubly-Robust",
    "text": "Doubly-Robust\nUsing “doubly robust” methods refers to using both regression and balancing in an equation simultaneously. Employing doubly robust measures is a way to mitigate biases or confounding variables from influencing the “true” treatment effect of your model.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Glossary</span>"
    ]
  },
  {
    "objectID": "glossary.html#inverse-probability-of-treatment-weights-iptw",
    "href": "glossary.html#inverse-probability-of-treatment-weights-iptw",
    "title": "Glossary",
    "section": "Inverse probability of treatment weights (IPTW)",
    "text": "Inverse probability of treatment weights (IPTW)\nThis is the weight that is calculated from the propensity score (which is just the probability of treatment). The idea is to apply a weight to each case that makes the treatment and the control groups balanced on the propensity score and therefore (hopefully) on the individual covariates.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Glossary</span>"
    ]
  },
  {
    "objectID": "glossary.html#covariate-balance",
    "href": "glossary.html#covariate-balance",
    "title": "Glossary",
    "section": "Covariate Balance",
    "text": "Covariate Balance\nIn the workflow of propensity score matching and weighting, a key step is to verify if covariates are balanced or imbalanced. Covariate balance is the degree to which the distribution of covariates is similar across levels of treatment.\nNote: The convention we’ve been operating under is that the standardized difference in means between the treatment and control groups for each variable is &lt; 0.1 (1/10th of a standard deviation).",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Glossary</span>"
    ]
  },
  {
    "objectID": "glossary.html#kolmogorov-smirnov-distance-ksd",
    "href": "glossary.html#kolmogorov-smirnov-distance-ksd",
    "title": "Glossary",
    "section": "Kolmogorov-Smirnov Distance (KSD)",
    "text": "Kolmogorov-Smirnov Distance (KSD)\nThe KSD is the proportion of non-overlap between two distributions (or, if dealing with cumulative distributions, the maximum distance between two distributions).\nNote: A value of 0 means that the distributions perfectly overlap, while a value of 1 means that the distributions do not overlap at all. The rule of thumb here is that the maximum KSD should be &lt; 0.05.\nNote: When both standard mean differences (SMDs) and KSDs are in their respective acceptable ranges, you can confidently say covariates are balanced.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Glossary</span>"
    ]
  },
  {
    "objectID": "glossary.html#entropy-balancing",
    "href": "glossary.html#entropy-balancing",
    "title": "Glossary",
    "section": "Entropy Balancing",
    "text": "Entropy Balancing\nA method of covariate balancing that seeks to balance on mean (like SMDs), variance (like KSDs), and on skewness.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Glossary</span>"
    ]
  },
  {
    "objectID": "glossary.html#panel-data",
    "href": "glossary.html#panel-data",
    "title": "Glossary",
    "section": "Panel Data",
    "text": "Panel Data\n\nLong-Form Data\nIn long-form data, each row is a “unit-observation,” meaning that each row is exactly one observation. Thus, if person A is observed four times, her data will be stored in four separate rows.\n\n\nWide-Form Data\nIn wide-form data, each row contains all of the observations for that one specific unit. Thus, if person A is observed four times, her data will be stored in only one row.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Glossary</span>"
    ]
  },
  {
    "objectID": "glossary.html#within--vs.-between-subject-variance",
    "href": "glossary.html#within--vs.-between-subject-variance",
    "title": "Glossary",
    "section": "Within- vs. Between-Subject Variance",
    "text": "Within- vs. Between-Subject Variance\nWithin-Subject Variance is the variation that exists within individuals and changes over time. In contrast, Between-Subject Variance exists between individuals and is more or less stable over time.\nFor example, an individual being taller than someone else is considered between-subject variance, as it measures the differences between two different subjects. However, an individual being taller than he was ten years ago is an example of within-subject variance, because it measures the differences between the same subject at two different time periods.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Glossary</span>"
    ]
  },
  {
    "objectID": "glossary.html#intra-class-correlation-icc",
    "href": "glossary.html#intra-class-correlation-icc",
    "title": "Glossary",
    "section": "Intra-Class Correlation (ICC)",
    "text": "Intra-Class Correlation (ICC)\nThe ICC is a descriptive statistic that ranges from 0 to 1 and measures the proportion of variance that is between individuals versus within individuals.\nFor example, an ICC of 0.25 indicates that a quarter of the variance exists between individuals, and three-fourths of the variance occurs within individuals.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Glossary</span>"
    ]
  },
  {
    "objectID": "glossary.html#mixed-models",
    "href": "glossary.html#mixed-models",
    "title": "Glossary",
    "section": "Mixed Models",
    "text": "Mixed Models\nA mixed model is simply a linear regression with two error terms: level 1 units (observations) and level 2 units (people). Mixed models include a mix of within-subject variance and between-subject variance, weighing one or the other more heavily based on how much of the total variance it contributes.\n\nSo much more to say…\nThis might be useful to keep in mind:",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Glossary</span>"
    ]
  },
  {
    "objectID": "glossary.html#fixed-effects-model",
    "href": "glossary.html#fixed-effects-model",
    "title": "Glossary",
    "section": "Fixed-Effects Model",
    "text": "Fixed-Effects Model\nA fixed-effects model is a statistical model that uses waves of observations from the same individual, then calculates the changes for the variables that differ in different waves of observations.\nNote: The fixed-effects model also holds time constant.\n\nPanel data provides the most common use of varying intercepts to estimate causal effects. Here we use repeated observations within the same individuals to adjust for time-constant unobserved confounders. Here, issues of balance and overlap still exist, but they only apply for within-person comparisons.\nIf the purpose of matching/weighting is to get rid of all observable confounders, then the purpose of fixed-effects is to get rid of all time-constant unobserved confounders.\nNote. Fixed Effects are also Mixed Models.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Glossary</span>"
    ]
  },
  {
    "objectID": "glossary.html#difference-in-differences",
    "href": "glossary.html#difference-in-differences",
    "title": "Glossary",
    "section": "Difference-in-Differences",
    "text": "Difference-in-Differences\nA simple difference-in-differences test compares the amounts that the treated and control groups change at time 1 and time 2. The equation for calculating a difference-in-differences reads as follows:\n\nDiD = (Treated Time 2 - Treated Time 1) - (Untreated Time 2 - Untreated Time 1)\n\nNote: The DiD calculation relies on the parallel trends assumption.\n\nParallel Trends Assumption\nThe parallel trends assumption assumes that, during the measurement period, the treated and untreated groups would have changed in similar ways if the treated group did not receive treatment.\nNote: The parallel trends assumption cannot be directly tested, but looking at prior trends can be useful for determining if the assumption is likely to be true.\n\n\n\nCode\nlibrary(tidyverse)\ntheme_set(theme_light(base_family = \"Avenir Next Condensed\"))\n\ntibble(\n  t = c(\"Time 0\", \"Time 1\"),\n  y1 = 1:2,\n  y0 = 0:1,\n  y = c(1, 3)\n) |&gt; \n  pivot_longer(!c(t, y), names_to = \"g\") |&gt; \n  ggplot(aes(t, value, group = g)) + \n  geom_line(linetype = \"dashed\") + \n  geom_vline(xintercept = c(\"Time 0\", \"Time 1\")) +\n  geom_line(aes(t, y)) + \n  geom_segment(x = \"Time 1\", y = 2, yend = 3, xend = \"Time 1\", linewidth = 1.5, \n               color = \"steelblue1\") +\n  labs(y = NULL, x = NULL) + \n  theme(axis.text.y = element_blank(), axis.ticks.y = element_blank(), \n        panel.grid = element_blank()) + \n  annotate(\"text\", x = 2.2, y = 2.5, label = latex2exp::TeX(r\"($E[d^1 - d^0]$)\"), family = \"Crimson Text\")\n\n\n\n\n\n\n\n\nFigure 13.2: Parallel Trends\n\n\n\n\n\n\n\n\nTwo-Way Fixed Effects (TWFE)\nThe TWFE model is a way of isolating confounding effects in order to focus only on the within-subject treatment effect. Specifically, the TWFE model holds constant the effect of the individual and also holds constant the effect of time, therefore adjusting for both.\nTwo-Way Fixed Effects Difference-in-Difference Estimator\nThe TWFE difference-in-differences estimator refers to the coefficients learned by a regression model with fixed effects for treatment condition and time period, clustered for treatment condition.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Glossary</span>"
    ]
  },
  {
    "objectID": "glossary.html#ordinary-least-squares-ols",
    "href": "glossary.html#ordinary-least-squares-ols",
    "title": "Glossary",
    "section": "Ordinary Least Squares (OLS)",
    "text": "Ordinary Least Squares (OLS)\nA method of estimating the parameters in a linear regression model. OLS chooses the parameters that minimize the sum of the squared differences between the observed values and the values predicted by the model.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Glossary</span>"
    ]
  },
  {
    "objectID": "glossary.html#bootstrapping",
    "href": "glossary.html#bootstrapping",
    "title": "Glossary",
    "section": "Bootstrapping",
    "text": "Bootstrapping\nBootstrapping is a statistical technique in which a single data set is sampled and resampled in order to run simulations and pull samples to account for random error.\n\nWe use bootstrapping to calculate standard errors in some cases.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Glossary</span>"
    ]
  },
  {
    "objectID": "glossary.html#splines",
    "href": "glossary.html#splines",
    "title": "Glossary",
    "section": "Splines",
    "text": "Splines\nA spline is a flexible kind of model that combines different models [parameters] in order to create the best estimate for the equation [nonlinear regression curves expressed as the sum of many localized pieces].\nNote: The transition from one model into another (i.e., where the data splits) is referred to as a “knot.”\n\nYou don’t need to know this, but here is how this looks like:\n\n\nCode\nlibrary(tidyverse)\n\nN &lt;- 500\nd &lt;- tibble(\n  x = runif(N, 1, 200),\n  y = rnorm(N, mean = 5 + cos(0.05*x), sd = 0.5)\n)\n\nd |&gt; \n  ggplot(aes(x, y)) + \n  geom_point() + \n  geom_smooth(method = \"lm\", formula = y ~ splines::bs(x, df = 10))\n\n\n\n\n\n\n\n\n\nUnder the hood, the bs() function is partitioning the \\(x\\) variable into 10 separate variables, each of them with their own weight, so that we have something like this:\n\n\nCode\nB &lt;- splines::bs(d$x, df = 10)\nds &lt;- as_tibble(B)\nds$x &lt;- d$x\n\nds |&gt; \n  pivot_longer(!x, names_to = \"b\", values_to = \"w\") |&gt; \n  ggplot(aes(x, w, group = b)) + \n  geom_line()\n\n\n\n\n\n\n\n\n\nEach of these separate variables is then associated with its own coefficient, which we estimate using linear regression.\nThese parameters are very difficult to interpret directly.\n\n\nCode\nols &lt;- lm(y ~ splines::bs(x, df = 10), data = d)\nbroom::tidy(ols)\n\n\n# A tibble: 11 × 5\n   term                      estimate std.error statistic   p.value\n   &lt;chr&gt;                        &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n 1 (Intercept)                 5.89       0.162   36.3    8.85e-141\n 2 splines::bs(x, df = 10)1    0.0203     0.318    0.0637 9.49e-  1\n 3 splines::bs(x, df = 10)2   -0.308      0.240   -1.28   2.01e-  1\n 4 splines::bs(x, df = 10)3   -2.02       0.261   -7.74   5.83e- 14\n 5 splines::bs(x, df = 10)4   -1.91       0.211   -9.03   4.08e- 18\n 6 splines::bs(x, df = 10)5   -0.685      0.218   -3.14   1.78e-  3\n 7 splines::bs(x, df = 10)6    0.509      0.216    2.35   1.89e-  2\n 8 splines::bs(x, df = 10)7   -0.427      0.245   -1.74   8.21e-  2\n 9 splines::bs(x, df = 10)8   -1.58       0.267   -5.92   5.94e-  9\n10 splines::bs(x, df = 10)9   -2.10       0.254   -8.26   1.36e- 15\n11 splines::bs(x, df = 10)10  -1.60       0.249   -6.43   3.10e- 10\n\n\nWe then get the predictions by using the various columns of B instead of using x directly.\nNote. The overlap in the small “hills” shown in the previous figure means that each is in reality a weighted sum.\n\n\nCode\nalpha &lt;- ols$coefficients[1]\ntheta &lt;- ols$coefficients[2:11]\n\nd$pred &lt;- alpha + B %*% theta\n\nd |&gt; \n  ggplot(aes(x, y)) + \n  geom_point() + \n  geom_line(aes(y = pred), color = \"red\", linewidth = 1)\n\n\n\n\n\n\n\n\n\nIn a causal inference context, we would want to do this when we have a covariate that interacts with the treatment in ways that are simply not captured by adding a quadratic term. This is one situation in which marginal effects come in handy.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Glossary</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Angrist, Joshua D., and Jörn-Steffen Pischke. 2009. Mostly Harmless\nEconometrics. Princeton university press.\n\n\nAshworth, Scott, Christopher R. Berry, and Ethan Bueno de Mesquita.\n2021. Theory and Credibility: Integrating Theoretical and Empirical\nSocial Science. Princeton University Press.\n\n\nCallaway, Brantly, and Pedro H. C. Sant’Anna. 2021. “Difference-in-Differences\nwith Multiple Time Periods.” Journal of Econometrics\n225(2): 200–230.\n\n\nCinelli, Carlos, Andrew Forney, and Judea Pearl. 2022. “A Crash Course in Good\nand Bad Controls.” Sociological Methods &\nResearch 00491241221099552.\n\n\nCunningham, Scott. 2021. Causal Inference. Yale University\nPress.\n\n\nGelman, Andrew, Jennifer Hill, and Aki Vehtari. 2020. Regression and\nOther Stories. Cambridge University Press.\n\n\nGoodman-Bacon, Andrew. 2021. “Difference-in-Differences with\nVariation in Treatment Timing.” Journal of Econometrics\n225(2): 254277.\n\n\nHernan, Miquel A., and James M. Robins. 2023. Causal Inference: What\nIf. CRC Press.\n\n\nHolland, Paul W. 1986. “Statistics and Causal\nInference.” Journal of the American Statistical\nAssociation 81(396): 945–60.\n\n\nHünermund, Paul, and Beyers Louw. 2023. “On the Nuisance of\nControl Variables in Causal Regression Analysis.”\nOrganizational Research Methods 10944281231219274.\n\n\nHuntington-Klein, Nick. 2021. The Effect: An Introduction\nto Research Design and Causality. New York: Chapman; Hall/CRC.\n\n\nKeele, Luke, Randolph T. Stevenson, and Felix Elwert. 2020. “The\nCausal Interpretation of Estimated Associations in Regression\nModels.” Political Science Research and Methods 8(1):\n113.\n\n\nLundberg, Ian, Rebecca Johnson, and Brandon M. Stewart. 2021. “What Is Your Estimand?\nDefining the Target Quantity Connects Statistical Evidence to\nTheory.” American Sociological Review\n00031224211004187.\n\n\nMorgan, Stephen L., and Christopher Winship. 2014. Counterfactuals\nand Causal Inference: Methods and Principles for Social Research.\n2nd edition. 2nd edition. New York, NY: Cambridge University Press.\n\n\nPearl, Judea. 2009. Causality: Models, Reasoning and Inference.\n2nd edition. 2nd edition. Cambridge, U.K. ; New York: Cambridge\nUniversity Press.\n\n\nRohrer, Julia M. 2018. “Thinking Clearly about Correlations and\nCausation: Graphical Causal Models for Observational Data.”\nAdvances in Methods and Practices in Psychological Science\n1(1): 2742.\n\n\nRohrer, Julia M., and Kou Murayama. 2023. “These Are Not the\nEffects You Are Looking for: Causality and the Within-/Between-Persons\nDistinction in Longitudinal Data Analysis.” Advances in\nMethods and Practices in Psychological Science 6(1):\n25152459221140842.\n\n\nSen, Maya, and Omar Wasow. 2016. “Race as a\nBundle of Sticks: Designs That Estimate Effects of Seemingly Immutable\nCharacteristics.” Annual Review of Political Science\n19(1): 499–522.\n\n\nWestreich, Daniel, and Sander Greenland. 2013. “The Table 2 Fallacy:\nPresenting and Interpreting Confounder and Modifier\nCoefficients.” American Journal of Epidemiology\n177(4): 292–98.",
    "crumbs": [
      "References"
    ]
  }
]