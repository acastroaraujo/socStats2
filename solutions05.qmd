---
title: "Solutions 5"
editor_options: 
  chunk_output_type: console
---

```{r}
#| message: false
#| code-summary: "Set up"

library(tidyverse)
library(gssr)
library(marginaleffects)
library(broom)

gss2022 <- gss_get_yr(2022)
```

## Linear Regression

*We will use this data.*

```{r}
dols <- gss2022 |> 
  select(tvhours, degree, madeg, padeg) |> 
  mutate(pardeg = pmax(madeg, padeg, na.rm = TRUE),
         college = if_else(degree >= 3, 1L, 0L),
         parcol = if_else(pardeg >= 3, 1L, 0L)) |>
  select(tvhours, college, parcol) |> 
  drop_na()
```

### Exercise

**Additive link function, no interactions**

```{r}
mod1 <- lm(tvhours ~ college + parcol, data = dols)

# ATE estimate
avg_slopes(mod1, variables = "college") |> 
  tidy()
```

**Answer (using base R):**

```{r}
p0 <- predict(mod1, newdata = dols |> mutate(college = 0))
p1 <- predict(mod1, newdata = dols |> mutate(college = 1))

mean(p1 - p0)
```

ATT/ATU estimate

```{r}
avg_slopes(
  model = mod1, 
  variables = "college",
  by = "college" # separately by treatment group
) |> 
  tidy()
```

**Answer (using using base R):**

*Note. You should be able to know that the answer will be exactly the same because of the model being used.*

```{r}
dols$p0 <- predict(mod1, newdata = dols |> mutate(college = 0))
dols$p1 <- predict(mod1, newdata = dols |> mutate(college = 1))

dols |> 
  mutate(estimate = p1 - p0) |> 
  group_by(college) |> 
  summarize(estimate = mean(estimate))
```

### Exercise

**Additive link function, with interactions**

```{r}
mod2 <- lm(tvhours ~ college * parcol, data = dols)

# ATE estimate
avg_slopes(mod2, variables = "college") |> 
  tidy()
```

**Answer (using matrix algebra):**

```{r}
beta <- coefficients(mod2) |> as.matrix(ncol = 1) ## ensures this is a column vector

M0 <- model.matrix(tvhours ~ college * parcol, data = dols |> mutate(college = 0))
pred0 <- M0 %*% beta

M1 <- model.matrix(tvhours ~ college * parcol, data = dols |> mutate(college = 1))
pred1 <- M1 %*% beta

mean(pred1 - pred0)
```

ATT/ATU estimate

```{r}
avg_slopes(
  model = mod2, 
  variables = "college",
  by = "college" # separately by treatment group
) |> 
  tidy()
```

**Answer (using previous calculations)**

```{r}
dols$p1 <- pred1
dols$p0 <- pred0

dols |> 
  group_by(college) |> 
  summarize(estimate = mean(p1 - p0))
```

## Poisson Regression

*We will use this data.*

```{r}
dqp <- gss2022 |>
  filter(wrkstat == 1) |> # full time workers
  select(realrinc, degree, madeg, padeg, sex, age) |> 
  mutate(pardeg = pmax(madeg, padeg, na.rm = TRUE),
         college = if_else(degree >= 3, 1L, 0L),
         parcol = if_else(pardeg >= 3, 1L, 0L),
         female = if_else(sex == 2, 1L, 0L),
         realrinc = floor(realrinc)) |>             # integer
  select(realrinc, college, parcol, female, age) |> 
  drop_na()
```

### Exercise

**Using the log-counts, no interactions**

```{r}
qp1 <- glm(realrinc ~ college + (parcol + female + age + I(age^2)), 
           data = dqp,
           family = "quasipoisson")

avg_slopes(qp1,
           variables = "college",
           type = "link") |> 
  tidy()
```

**Answer:**

```{r}
pred0 <- predict(qp1, newdata = mutate(dqp, college = 0), type = "link")
pred1 <- predict(qp1, newdata = mutate(dqp, college = 1), type = "link")

mean(pred1 - pred0)
```

ATU/ATT

```{r}
avg_slopes(qp1,
           variables = "college",
           type = "link",
           by = "college") |> # separately by treatment group
  tidy()
```

**Answer**:

```{r}
dqp$pred0 <- pred0
dqp$pred1 <- pred1

dqp |> 
  group_by(college) |> 
  summarize(estimate = mean(pred1 - pred0))
```

### Exercise

**Non-linear response, no interactions**

```{r}
avg_slopes(qp1,
           variables = "college",
           type = "response") |> 
  tidy()
```

**Answer:**

```{r}
## use appropriate link function for Poisson
dqp$pred0 <- exp(pred0)
dqp$pred1 <- exp(pred1)

dqp |> 
  summarize(ate = mean(pred1 - pred0))
```

ATT/ ATU

```{r}
avg_slopes(qp1,
           variables = "college",
           type = "response",
           by = "college") |> # separately by treatment group
  tidy()
```

**Answer:**

```{r}
dqp |> 
  group_by(college) |> 
  summarize(ate = mean(pred1 - pred0))
```

### Exercise

**Using the log-counts, with interactions**

```{r}
qp2 <- glm(realrinc ~ college * (parcol + female + age + I(age^2)), 
           data = dqp,
           family = "quasipoisson")

avg_slopes(qp2,
           variables = "college",
           type = "link") |> 
  tidy()
```

**Answer**:

```{r}
dqp$pred0 <- predict(qp2, newdata = mutate(dqp, college = 0), type = "link")
dqp$pred1 <- predict(qp2, newdata = mutate(dqp, college = 1), type = "link")

dqp |> 
  summarize(estimate = mean(pred1 - pred0))
```

```{r}
avg_slopes(qp2,
           variables = "college",
           type = "link",
           by = "college") |> # separately by treatment group
  tidy()
```

```{r}
dqp |> 
  group_by(college) |> 
  summarize(estimate = mean(pred1 - pred0))
```

### Exercise

**Non-linear response, with interactions**

```{r}
avg_slopes(qp2,
           variables = "college",
           type = "response") |> 
  tidy()
```

**Answer:**

```{r}
dqp$pred0 <- predict(qp2, newdata = mutate(dqp, college = 0), type = "response")
dqp$pred1 <- predict(qp2, newdata = mutate(dqp, college = 1), type = "response")

dqp |> 
  summarize(estimate = mean(pred1 - pred0))
```

```{r}
avg_slopes(qp2,
           variables = "college",
           type = "response",
           by = "college") |> # separately by treatment group
  tidy()
```

Answer:

```{r}
dqp |> 
  group_by(college) |> 
  summarize(estimate = mean(pred1 - pred0))
```

## Logistic Regression

*We will use this data.*

```{r}
dlr <- gss2022 |>
  select(abany, degree, madeg, padeg, sex, age) |> 
  mutate(pardeg = pmax(madeg, padeg, na.rm = TRUE),
         college = if_else(degree >= 3, 1L, 0L),
         parcol = if_else(pardeg >= 3, 1L, 0L),
         female = if_else(sex == 2, 1L, 0L),
         abany = if_else(abany == 1, 1L, 0L)) |>
  select(abany, college, parcol, female, age) |> 
  drop_na()
```

### Exercise

**Using log-odds, no interactions**

```{r}
lr1 <- glm(abany ~ college + (parcol + female + age + I(age^2)),
          data = dlr,
          family = binomial)

# ATE estimate
avg_slopes(lr1,
           variables = "college",
           type = "link") |> 
  tidy()
```

Answer:

```{r}
dlr$pred0 <- predict(lr1, newdata = dlr |> mutate(college = 0), type = "link")
dlr$pred1 <- predict(lr1, newdata = dlr |> mutate(college = 1), type = "link")

dlr |> 
  summarize(estimate = mean(pred1 - pred0))
```

ATT/ATU

```{r}
avg_slopes(lr1,
           variables = "college",
           by = "college",
           type = "link") |> 
  tidy()
```

```{r}
dlr |> 
  group_by(college) |> 
  summarize(estimate = mean(pred1 - pred0))
```

### Exercise

**Using non-linear response (aka probabilities), no interactions**

```{r}
# ATE estimate
avg_slopes(lr1,
           variables = "college",
           type = "response") |> 
  tidy()
```

Answer:

```{r}
dlr |> 
  mutate(across(starts_with("pred"), plogis)) |> 
  summarize(estimate = mean(pred1 - pred0))
```

ATT/ATU

```{r}
avg_slopes(lr1,
           variables = "college",
           by = "college",
           type = "response") |> 
  tidy()
```

Answer:

```{r}
dlr |> 
  mutate(across(starts_with("pred"), plogis)) |> 
  group_by(college) |> 
  summarize(estimate = mean(pred1 - pred0))
```

### Exercise

**Using log-odds, with interactions**

```{r}
lr2 <- glm(abany ~ college * (parcol + female + age + I(age^2)),
          data = dlr,
          family = binomial)

# ATE estimate
avg_slopes(lr2,
           variables = "college",
           type = "link") |> 
  tidy()
```

Answer:

```{r}
dlr$pred0 <- predict(lr2, newdata = dlr |> mutate(college = 0), type = "link")
dlr$pred1 <- predict(lr2, newdata = dlr |> mutate(college = 1), type = "link")

dlr |> 
  summarize(estimate = mean(pred1 - pred0))
```

ATT/ATU:

```{r}
avg_slopes(lr2,
           variables = "college",
           by = "college",
           type = "link") |> 
  tidy()
```

Answer:

```{r}
dlr |> 
  group_by(college) |> 
  summarize(estimate = mean(pred1 - pred0))
```

### Exercise

**Using non-linear response (aka probabilities), with interactions**

```{r}
# ATE estimate
avg_slopes(lr2,
           variables = "college",
           type = "response") |> 
  tidy()
```

Answer:

```{r}
dlr$pred0 <- predict(lr2, newdata = dlr |> mutate(college = 0), type = "response")
dlr$pred1 <- predict(lr2, newdata = dlr |> mutate(college = 1), type = "response")

dlr |> 
  summarize(estimate = mean(pred1 - pred0))
```

ATT/ATU

```{r}
avg_slopes(lr2,
           variables = "college",
           by = "college",
           type = "response") |> 
  tidy()
```

Answer:

```{r}
dlr |> 
  group_by(college) |> 
  summarize(estimate = mean(pred1 - pred0))
```

## Bootstrap Standard Errors

So far, we haven't calculated the standard errors at all. And yet `marginaleffects` does this seamlessly.

Here is a potential bootstrap implementation.

```{r}
bootstrap_ate <- function(obj, treatment, type = c("response", "link"), S = 5e3) {
  ## Note. This code is inefficient and will only work with "glm" objects.  
  type <- match.arg(type)
  data <- obj$data
  
  ## will only work if "treatment" is binary (0, 1)
  data0 <- mutate(data, {{treatment}} := 0)
  data1 <- mutate(data, {{treatment}} := 1)
  
  out <- replicate(S, {
    i <- sample(nrow(data), replace = TRUE)
    nobj <- glm(obj$formula, data = data[i, ], family = obj$family)
    pred0 <- predict(nobj, newdata = data0[i, ], type = type)
    pred1 <- predict(nobj, newdata = data1[i, ], type = type)
    mean(pred1 - pred0)
  })
  
  return(out)
}
```

Logistic Regression using link function:

```{r}
avg_slopes(lr2, variables = "college", type = "link")
out <- bootstrap_ate(lr2, treatment = "college", type = "link")
c("estimate" = mean(out), "std. error" = sd(out))
```

Logistic Regression using response function:

```{r}
avg_slopes(lr2, variables = "college", type = "response")
out <- bootstrap_ate(lr2, college, type = "response")
c("estimate" = mean(out), "std. error" = sd(out))
```

Poisson Regression using link function:

```{r}
avg_slopes(qp1, variables = "college", type = "link")
out <- bootstrap_ate(qp1, treatment = "college", type = "link")
mean(out)
sd(out)
```

Poisson Regression using response function:

```{r}
avg_slopes(qp2, variables = "college", type = "response")
out <- bootstrap_ate(qp2, treatment = "college", type = "response")
c("estimate" = mean(out), "std. error" = sd(out))
```

### Linear Regression

Here is a function that designed to work with `lm` objects.

```{r}
bootstrap_ate_ols <- function(obj, data, treatment, S = 5e3) {
  
  ## will only work if "treatment" is binary (0, 1)
  data0 <- mutate(data, {{treatment}} := 0)
  data1 <- mutate(data, {{treatment}} := 1)
  
  out <- replicate(S, {
    i <- sample(nrow(data), replace = TRUE)
    nobj <- lm(obj$call$formula, data = data[i, ])
    pred0 <- predict(nobj, newdata = data0[i, ])
    pred1 <- predict(nobj, newdata = data1[i, ])
    mean(pred1 - pred0)
  })
  
  return(out)
}

avg_slopes(mod2, variables = "college")
out <- bootstrap_ate_ols(mod2, data = dols, treatment = "college")
mean(out)
sd(out)
```

*Wait a minute! Something is wrong...*

The bootstrap standard error is much smaller than the standard error calculated by `avg_slopes()`.

However, if you ask `marginaleffects` to re-calculate the standard errors using a bootstrap you'll see that the answers coincide.

```{r}
avg_slopes(mod2, variables = "college") |> 
  inferences(method = "boot")
```

But there's still something weird with just accepting this weird fact and moving on.

*What is happening?*

**Read on *if and only if* you want to go down a rabbit hole.**

The reason for this—and I will change this explanation if Steve tells me it's incorrect—is that the results were similar enough for Logistic and Poisson regression because they have no extra parameter for the *variance.*

Recall that we can write down a linear regression as follows:

$$
\begin{align}
y_i = \alpha + \beta x_i + \varepsilon_i, && \varepsilon \sim \text{Normal}(0, \sigma)
\end{align}
$$

This extra $\sigma$ parameter only exists in normal linear regression.

I think what is going on in the bootstrap is that the uncertainty about the variance isn't "propagating" when calculating the ATE. *This means that the standard error for the ATE is always going to be underestimated* unless we do something about this.

I am pretty sure the `marginaleffects` package estimates the standard error for the ATE in a way that takes into account this additional source of uncertainty.

So I modified the previous bootstrap function to incorporate some extra noise coming from $\sigma$. The results are not exactly the same, but they are closer.

```{r}
bootstrap_ate_ols2 <- function(obj, data, treatment, S = 5e3) {
  
  data0 <- mutate(data, {{treatment}} := 0)
  data1 <- mutate(data, {{treatment}} := 1)
  
  out <- replicate(S, {
    i <- sample(nrow(data), replace = TRUE)
    nobj <- lm(obj$call$formula, data = data[i, ])
    
    pred0 <- predict(nobj, newdata = data0[i, ])
    pred1 <- predict(nobj, newdata = data1[i, ])
    
    ## extra noise:
    n <- nrow(data)
    sigma <- sd(nobj$residuals)
    mean(rnorm(n, pred1, sigma) - rnorm(n, pred0, sigma))
    
  })
  
  return(out)
}

out <- bootstrap_ate_ols2(mod2, data = dols, treatment = "college")
mean(out)
sd(out)
```

This result is *better*.
