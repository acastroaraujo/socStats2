{
  "hash": "bc75a94b0a098ac376cde36d4c4ae571",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Solutions 6\"\nexecute: \n  eval: false\n---\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(broom)\n```\n:::\n\n\n## Exercise\n\n\n::: {.cell}\n\n```{.r .cell-code}\nd <- haven::read_dta(\"data/cattaneo2.dta\")\n\nd <- d |>  \n  haven::zap_labels() |>             \n  select(bweight, lbweight, mbsmoke, mmarried, \n         mage, medu, fbaby, alcohol, mrace, nprenatal)\n\nglimpse(d)\n\nd |> \n  select(!matches(\"weight\")) |> \n  pivot_longer(!mbsmoke, names_to = \"covariate\") |> \n  group_by(mbsmoke, covariate) |> \n  summarize(avg = mean(value), sd = sd(value)) |> \n  pivot_wider(names_from = mbsmoke, values_from = c(avg, sd)) |> \n  mutate(diff = (avg_1 - avg_0) / sd_1) |> \n  mutate(sign = factor(sign(diff), labels = c(\"negative\", \"positive\"))) |> \n  mutate(covariate = fct_reorder(covariate, abs(diff))) |> \n  ggplot(aes(x = diff, y = covariate)) +\n  geom_segment(aes(xend = diff, yend = covariate), x = 0) +\n  geom_point(fill = \"white\", shape = 21) +\n  geom_vline(xintercept = 0, linetype = \"dashed\") +\n  coord_cartesian(xlim = c(-0.6, 0.6)) +\n  labs(y = NULL, x = \"Standardized Differences in Means\", \n       title = \"Imbalance in averages of confounding covariates across treatment groups\")\n\n# ggsave(\n#   plot = last_plot(),\n#   filename = \"images/imbalance_plot_example.png\", \n#   device = \"png\", \n#   dpi = \"print\", \n#   height = 5, \n#   width = 7\n# )\n```\n:::\n\n\n## Exercise \n\nNHK\n\n## A Lesson in Simulation\n\nLast week I was going to have you re-do something I found in @gelman2020 [pp. 385] that's supposed to build intuition about omitted variable bias and then compare it with results from simulations.\n\n![](images/clipboard-874638397.png){fig-align=\"center\" width=\"90%\"}\n\nIt seems simple at first, but it's also misleading.\n\nSo, I changed things up a bit and added a simple toy DAG:\n\n::: grid\n::: g-col-4\n![](images/clipboard-133306100.png){fig-align=\"center\" width=\"100%\"}\n:::\n\n::: g-col-8\n$$\n\\begin{align}\n&(1) &y_i &= \\beta_0 + \\beta_1 x_i + \\beta_2 w_i + \\varepsilon_i, \n&& \\varepsilon_i \\sim \\text{Normal}(0, \\sigma_y^2) \\\\\\\\\n&(2) &x_i &= \\alpha_0 + \\alpha_1 w_i + u_i, && u_i \\sim \\text{Normal}(0, \\sigma_x^2)\n\\\\\\\\ &&&&&w_i \\sim \\text{Normal}(0, \\sigma_w^2)\n\\end{align}\n$$\n:::\n:::\n\n::: callout-tip\nI expressed Equation (2) so that it would make sense to *generate* data from a simulation.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfork_simulation <- function(N = 1e5, a0 = 1, a1 = 1, b0 = 1, b1 = 1, b2 = 1) {\n  \n  tibble(\n    w = rnorm(N, 0, 20),\n    x = a0 + a1*w + rnorm(N, 0, 2),\n    y = b0 + b1*x + b2*w + rnorm(N, 0, 2)\n  )\n  \n}\n```\n:::\n\n:::\n\nNow suppose that we want to estimate a regression but refuse to adjust for $w$. This means that we will estimate something like the following equation:\n\n$$\n\\begin{align}\n(3) &&y_i = \\beta_0^* + \\beta_1^* x_i + \\varepsilon_i^*\n\\end{align}\n$$\n\nTake out some pen and paper.\n\n-   Take Equation (2) and solve for $w$.\n\n-   Plug in the resulting equation into Equation (1)\n\n    $y_i = \\beta_0 + \\beta_1 x_i + \\beta_2 \\overset{\\overset{\\text{HERE}}{\\downarrow}}{w_i} + \\varepsilon_i$\n\n-   Re-arrange the terms and express them so that the result resembles Equation (3).\n\n    This is the result:\n\n$$\ny_i = \\underbrace{\\bigg(\\beta_0 - \\frac{\\beta_2 \\alpha_0}{\\alpha_1} \\bigg)}_{\\beta_0^*} + \\underbrace{\\bigg(\\beta_1 + \\frac{\\beta_2}{\\alpha_1}\\bigg)}_{\\beta_1^*} \\ x_i + \\underbrace{\\bigg(\\varepsilon_i - \\frac{\\beta_2 u_i}{\\alpha_1}\\bigg)}_{\\varepsilon^*}\n$$\n\nNow we can compare with a simulation. If I set $\\beta_1 = 1$, $\\beta_2 = 3$, and $\\alpha_1 = 2$, then $\\beta_1^* \\approx 2.5$\n\n\n::: {.cell}\n\n```{.r .cell-code}\nd <- fork_simulation(b1 = 1, b2 = 3, a1 = 2)\n\nlm(y ~ x, data = d) |> tidy()\n```\n:::\n\n\nThat's great, but unfortunately things are not that simple...\n\nThe results change if we fiddle around with the with data generating process a bit.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfork_simulation <- function(\n    N = 1e5, a0 = 1, a1 = 1, b0 = 1, b1 = 1, b2 = 1,\n    w_expr = rnorm(N, 0, 20)\n    ) {\n  \n  w_expr <- substitute(w_expr)\n  \n  tibble(\n    w = eval(w_expr),\n    x = a0 + a1*w + rnorm(N, 0, 2),\n    y = b0 + b1*x + b2*w + rnorm(N, 0, 2)\n  )\n  \n}\n\nd <- fork_simulation(b1 = 1, b2 = 3, a1 = 2, w_expr = rnorm(N, 0, 1))\nlm(y ~ x, data = d) |> tidy()\n\nd <- fork_simulation(b1 = 1, b2 = 3, a1 = 2, w_expr = runif(N, -1, 1))\nlm(y ~ x, data = d) |> tidy()\n\ncov(d$x, d$y) / var(d$x)\n```\n:::\n\n\n### \n\n::: callout-tip\n### Exercise\n\nNow you are ready to answer some easy questions.\n\n-   If $\\beta_1 = 10$, what needs to be true about the other coefficients so that $\\beta_1^* = 0$.\n\n-   Check your result by generating new data with the `fork_simulation()`.\n\n-   If $\\beta_1 = 1$, what needs to true about the other coefficients so that $\\beta_1^* < 0$?\n\n-   Check your result by generating new data with the `fork_simulation()`.\n\n-   If $\\beta_1 = -3$, what needs to true about the other coefficients sot that $\\beta_1^* > 0$?\n\n-   Check your result by generating new data with the `fork_simulation()`.\n:::\n",
    "supporting": [
      "solutions06_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}