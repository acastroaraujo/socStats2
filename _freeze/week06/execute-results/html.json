{
  "hash": "766097352c475332a3e1f0893e22d538",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Week 6\"\ncallout-appearance: simple\ncallout-icon: false\neditor_options: \n  chunk_output_type: console\n---\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(broom)\n\ntheme_set(theme_linedraw(base_family = \"Avenir Next Condensed\"))\n```\n:::\n\n\n## Regression\n\nBefore we leave regression for a couple of weeks, I will ask you to do tw.\n\n### Algebraic\n\nSuppose our data generating process is as follows:\n\n::: grid\n::: g-col-4\n![](images/clipboard-133306100.png){fig-align=\"center\" width=\"100%\"}\n:::\n\n::: g-col-8\n$$\n\\begin{align}\n&(1) &y_i &= \\beta_0 + \\beta_1 x_i + \\beta_2 w_i + \\varepsilon_i, \n&& \\varepsilon_i \\sim \\text{Normal}(0, \\sigma_y^2) \\\\\\\\\n&(2) &x_i &= \\alpha_0 + \\alpha_1 w_i + u_i, && u_i \\sim \\text{Normal}(0, \\sigma_x^2)\n\\\\\\\\ &&&&&w_i \\sim \\text{Normal}(0, \\sigma_w^2)\n\\end{align}\n$$\n:::\n:::\n\n**Simulation**\n\nThis is what a little simulation looks like:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfork_simulation <- function(N = 1e5, a0 = 1, a1 = 1, b0 = 1, b1 = 1, b2 = 1) {\n  \n  tibble(\n    w = rnorm(N, 0, 2),\n    x = a0 + a1*w + rnorm(N, 0, 2),\n    y = b0 + b1*x + b2*w + rnorm(N, 0, 2)\n  )\n  \n}\n\nd <- fork_simulation()\n```\n:::\n\n\n**Omitted Variable Bias**\n\nNow suppose that we want to estimate a regression but refuse to adjust for $w$. This means that we will estimate something like the following equation:\n\n$$\n\\begin{align}\n(3) &&y_i = \\beta_0^* + \\beta_1^* x_i + \\varepsilon_i^*\n\\end{align}\n$$\n\nThe estimated effect of $\\beta_1^*$ is going to be biased in some way, but it's hard to tell exactly *how much.*\n\nThe `d` dataset was created from `fork_simulation()` so that all $\\alpha$'s and $\\beta$'s are equal to one. Intuitively, we should be able to tell that $\\beta_1^*$ is gonna be greater than $\\beta_1$.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlm(y ~ x, data = d) |> tidy()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 5\n  term        estimate std.error statistic p.value\n  <chr>          <dbl>     <dbl>     <dbl>   <dbl>\n1 (Intercept)    0.501   0.00821      61.1       0\n2 x              1.50    0.00274     548.        0\n```\n\n\n:::\n:::\n\n\n*Can some high school algebra help us be a little more precise about this?*\n\n### Exercise\n\n::: callout-note\nTake out some pen and paper.\n\n-   Take Equation (2) and solve for $w$.\n\n-   Plug in the resulting equation into Equation (1)\n\n    $y_i = \\beta_0 + \\beta_1 x_i + \\beta_2 \\overset{\\overset{\\text{HERE}}{\\downarrow}}{w_i} + \\varepsilon_i$\n\n-   Re-arrange the terms and express them so that the result resembles Equation (3).\n:::\n\n::: callout-tip\nThe result should look something like this:\n\n$$\ny_i = \\underbrace{\\bigg(\\beta_0 + ...\\bigg)}_{\\beta_0^*} + \\underbrace{\\bigg(\\beta_1 + ...\\bigg)}_{\\beta_1^*} \\ x_i + \\underbrace{\\bigg(\\varepsilon_i - \\frac{\\beta_2 u_i}{\\alpha_1}\\bigg)}_{\\varepsilon^*}\n$$\n:::\n\n### Exercise\n\n::: callout-note\nNow you are ready to answer some easy questions.\n\n-   If $\\beta_1 = 10$, what needs to be true about the other coefficients so that $\\beta_1^* = 0$.\n\n-   Check your result by generating new data with the `fork_simulation()`.\n\n-   If $\\beta_1 = 1$, what needs to true about the other coefficients so that $\\beta_1^* < 0$?\n\n-   Check your result by generating new data with the `fork_simulation()`.\n\n-   If $\\beta_1 = -3$, what needs to true about the other coefficients sot that $\\beta_1^* > 0$?\n\n-   Check your result by generating new data with the `fork_simulation()`.\n:::\n\n## Balance and Overlap\n\n**Balance**\n\nThe sort of bias that we get from confounding can be interpreted more precisely as **imbalance** in the potential outcomes across treatment groups. This is the sort of imbalance is unlikely with *randomization*, but it’s almost guaranteed in observational studies.\n\nIn other words, imbalance occurs if the distributions of confounders differ for the treatment and control groups.\n\n**Overlap**\n\n@fig-overlap shows what lack of complete overlap (with respect to $x$) might look like:\n\n\n::: {#fig-overlap .cell layout-ncol=\"3\"}\n::: {.cell-output-display}\n![Two distributions with no overlap](week06_files/figure-html/fig-overlap-1.png){#fig-overlap-1 width=240}\n:::\n\n::: {.cell-output-display}\n![Two distributions with partial overlap](week06_files/figure-html/fig-overlap-2.png){#fig-overlap-2 width=240}\n:::\n\n::: {.cell-output-display}\n![The _range_ of one distribution is a subset of the range of the other.](week06_files/figure-html/fig-overlap-3.png){#fig-overlap-3 width=240}\n:::\n\nLack of complete overlap in distributions across treatment and control groups. Dashed lines indicate distributions for the control group; solid lines indicate distributions for the treatment group.\n:::\n\n\nLack of complete overlap or \"common support\" creates problems because in this setting we have treatment or control observations for which we have *no empirical counterfactuals*. Thus, knowledge about treatment effects is inherently limited in regions of non-overlap. Any causal inference in @fig-overlap-1 would rely on modeling assumptions instead of having direct support from the data. In @fig-overlap-3 causal inference is possible for the full treatment group but only for a subset of the control group.\n\n*Note. This is the exact same thing we talked about when thinking about the potential outcomes for a cervical cancer vaccine in a population for men and women.*\n\n### Exercise\n\n**Looking for imbalance.**\n\nLoad the `cattaneo2.dta` data that Steve showed us in class.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nd <- haven::read_dta(\"data/cattaneo2.dta\")\n\nd <- d |>  \n  haven::zap_labels() |>             \n  select(bweight, lbweight, mbsmoke, mmarried, mage, medu, fbaby, alcohol, mrace, nprenatal)\n\nglimpse(d)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 4,642\nColumns: 10\n$ bweight   <dbl> 3459, 3260, 3572, 2948, 2410, 3147, 3799, 3629, 2835, 3880, …\n$ lbweight  <dbl> 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, …\n$ mbsmoke   <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, …\n$ mmarried  <dbl> 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, …\n$ mage      <dbl> 24, 20, 22, 26, 20, 27, 27, 24, 21, 30, 26, 20, 34, 21, 23, …\n$ medu      <dbl> 14, 10, 9, 12, 12, 12, 12, 12, 12, 15, 12, 12, 14, 8, 12, 12…\n$ fbaby     <dbl> 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, …\n$ alcohol   <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ mrace     <dbl> 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, …\n$ nprenatal <dbl> 10, 6, 10, 10, 12, 9, 16, 11, 20, 9, 14, 5, 13, 8, 4, 10, 13…\n```\n\n\n:::\n:::\n\n\nWe can start checking for imbalance for several covariates by examining their absolute standardized difference in means—i.e., a **balance plot**. I've included a graph that shows the *absolute standardized difference in means values* for a set of confounding covariates that might predict both `mbsmoke` and birth weight.\n\n![](images/imbalance_plot_example.png){fig-align=\"center\" width=\"100%\"}\n\n::: callout-note\nYou're job is to reproduce something close to this figure.\n\nWhat do you think are the most important covariates you need to adjust for in terms of the potential biases in the treatment effect?\n:::\n\n::: callout-tip\nI used `geom_segment()`, but you can just use `geom_point()`.\n\nAn open question is exactly *which* \"standard deviation\" to use. I used the standard deviations of the treated group, but you are not required to use that one. This means that your final plot doesn't have to reproduce\n:::\n\n## Matching\n\nThis question is copied from NHK's exercises. To answer this question you need to read sections 14.1 and 14.2 of [*The Effect*](https://theeffectbook.net/)*.*\n\n### Exercise\n\n> You want to know whether practicing cursive improves your penmanship (on a 1-10 scale). You find that, among people who don’t practice cursive, average penmanship is 5, 10 people are left-handed, 2 are ambidextrous, and 88 are right-handed. Among people who do practice cursive, 6 are left-handed with average penmanship 7, 4 are ambidextrous with average penmanship 4, and 90 are right-handed with average penmanship 6.\n>\n> a.  You want to create a set of weights that will *make the treated group match the control group on handedness*. Follow the process in [section 14.2](https://theeffectbook.net/ch-Matching.html#weighted-averages), paying attention to *why* certain numbers are going in certain positions. What weights will be given to the left, ambidextrous, and right-handed people *in the control group*?\n>\n> b.  What weights will be given to the left, ambidextrous, and right-handed people *in the treated group*?\n>\n> c.  Use the weights from part b to calculate the *proportion of left-handed people in the treated group*, as well as the proportion of ambidextrous people and the proportion of right-handed people. If you don’t get 10%, 2%, and 88% (or very close with some rounding error), your weights are wrong, try again.\n>\n> d.  What is the weighted average penmanship score in the treated group?\n>\n> e.  What is the effect of practicing cursive that we would estimate using this data?\n\nCHANGE TO RNORM()\n",
    "supporting": [
      "week06_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}