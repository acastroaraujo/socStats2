{
  "hash": "7fec39a246f42cefef58e6678f2fd7b3",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Week 7\"\n---\n\n::: {.cell}\n\n```{.r .cell-code  code-summary=\"Packages and Helper Functions\"}\n# Packages ---\n\nlibrary(tidyverse)\nlibrary(broom)\nlibrary(cobalt)\nlibrary(MatchIt)\nlibrary(WeightIt)\n\n# Helper Functions ---\n\nlove_plot <- function(x) {\n  cobalt::love.plot(x, \n    binary = \"std\" ,\n    stats = c(\"m\", \"ks\") ,\n    thresholds = c(.1, .05),\n    var.order = \"adjusted\",\n    abs = TRUE\n  )\n}\n```\n:::\n\n\n## Matching and Weighting\n\n*Note. The following exercises where adapted from last year's class.*\n\nFor these exercises, we are going to use one of the versions of the \"Lalonde data,\" which is used in almost every paper on matching.[^week07-1] This is data on a job training program (the treatment) that was intended to raise future earnings (the outcome).\n\n[^week07-1]: Lalonde, R. (1986). \"Evaluating the econometric evaluations of training programs with experimental data.\" *American Economic Review* 76: 604-620.\n\nYou can load the data by typing the following:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nload(\"data/exercise_data.Rdata\")\n```\n:::\n\n\nThis will bring two objects into the global environment: `d_exper`, which is the experimental subset of the data and `d`, which comprises the treated cases and a sample of observational controls from the PSID. The treatment is `treat` and the outcome is `re78`, which is income in \\$1000s. We are going to use the experimental subset to set an experimental benchmark and then see how close we can get to this benchmark using various matching and weighting methods.\n\nThe rest of the variables are as follows:\n\n| Variable  | Description                    |\n|:----------|:-------------------------------|\n| `age`     | Age in years                   |\n| `educ`    | Years of education             |\n| `black`   | 1 = Black; 0 otherwise         |\n| `hisp`    | 1 = Hispanic; 0 otherwise      |\n| `married` | 1 = married; 0 otherwise       |\n| `nodegr`  | 1 = no degree; 0 otherwise     |\n| `re74`    | 1974 income in \\$1000s         |\n| `re75`    | 1975 income in \\$1000s         |\n| `u74`     | 1 = no '74 income; 0 otherwise |\n| `u75`     | 1 = no '75 income; 0 otherwise |\n\nBefore starting the exercises, you may want to consider a few things that will make your life easier:\n\n-   add a factor version of the treatment to the data frame for easy plotting\n-   create formula objects that contain the propensity score (or matching) models with and without quadratic terms\n\nYou can get by without doing these steps but they avoid extra typing down the road.\n\nYou will begin by looking at the experimental data (`d_exper`). After that, you will conduct various forms of matching and weighting on the observational data (`d`). For each exercise *after the first four*, your basic workflow will be:\n\n1.  Match or weight, as directed\n2.  Check balance (overall, if applicable and by covariate) using graphical and numeric means\n3.  Estimate the ATT\n\n### Exercise\n\n*Use the experimental data to estimate the effect of the job training treatment. How much does it appear to affect 1978 income? Now look at the observational data (for all exercises from now on). How large is the raw difference in 1978 income between the treatment group and the PSID comparison group?*\n\n### Exercise\n\n*Try to estimate the effect of the treatment using regression. What does regression say the effect of the program is?*\n\n### Exercise\n\n*Begin by exact matching on all the dummy variables. How many treated cases cannot be matched? What is the (FS)ATT estimate?*\n\n### Exercise\n\n*Use the observational data to estimate each case's propensity to receive treatment using `glm()`. Use a logistic regression with quadratic terms for age, education, 1974 income, and 1975 income. Spend a few moments thinking about what this model says. Look at the density plots of the p-score for treated and untreated groups.*\n\n### Exercise\n\n*Estimate propensity scores and ATT weights using `weightit()`. Ignore the warning you get. We'll discuss that more in class. Estimate the ATT. Check for covariate balance.*\n\n### Exercise\n\n*Now do the same as above using \"entropy balancing.\"* *Confirm that you've achieved balance on the means and the variances of the covariates. Estimate the ATT.*\n\n::: callout-tip\nDon't worry (for now) if you don't understand what this is; simply change the method argument to `\"cbps\"`, we will cover this in class.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nOUTPUT <- weightit(FORMULA, \n                   data = d,\n                   method = \"ebal\",\n                   moments = 3,\n                   estimand = \"ATT\")\n```\n:::\n\n:::\n\n### Bonus\n\n*Implement a bootstrap of your preferred estimate. What is the bootstrapped standard error?*\n",
    "supporting": [
      "week07_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}